{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM clients initialized\n",
      "Using LLM client :: ClientType.AZURE\n"
     ]
    }
   ],
   "source": [
    "from services.scraper import query_result_retriever\n",
    "from llm_prompts import create_refine_search_results_user_prompt, REFINE_SEARCH_SYSTEM_PROMPT\n",
    "from services.llm_service import OpenAIService\n",
    "from services.researcher import ResearcherService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM clients initialized\n",
      "LLM clients initialized\n"
     ]
    }
   ],
   "source": [
    "openai_instance = OpenAIService()\n",
    "\n",
    "researcher_service_instance = ResearcherService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating queries for LLM\n",
      "Generate 5 search queries for the following research topic LLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B9BAL6TpN6XSdqWx38idhhpQ25rDt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SBKkAa2stoOMFMt0SpbXb4wd', function=Function(arguments='{\"queries\":[{\"query\":\"What is a Large Language Model (LLM)?\",\"goal\":\"To understand the foundational concept and definition of Large Language Models.\"},{\"query\":\"How do Large Language Models work?\",\"goal\":\"To explore the underlying mechanics and architecture of LLMs.\"},{\"query\":\"Applications and use cases of Large Language Models\",\"goal\":\"To identify the various practical applications and industries where LLMs are utilized.\"},{\"query\":\"Challenges and limitations of Large Language Models\",\"goal\":\"To uncover potential drawbacks and issues faced in the deployment and development of LLMs.\"},{\"query\":\"Comparative analysis of different Large Language Models\",\"goal\":\"To compare and contrast various LLMs available, highlighting their differences and unique features.\"}],\"sub_topics\":[\"Understanding Large Language Models\",\"Mechanics and Architecture of LLMs\",\"Applications of LLMs\",\"Challenges and Limitations\",\"Comparative Analysis of Popular LLMs\"]}', name='generate_research_queries'), type='function')]), content_filter_results={})], created=1741527573, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_b705f0c291', usage=CompletionUsage(completion_tokens=186, prompt_tokens=222, total_tokens=408, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "Query generation complete\n",
      "[{'query': 'What is a Large Language Model (LLM)?', 'goal': 'To understand the foundational concept and definition of Large Language Models.'}, {'query': 'How do Large Language Models work?', 'goal': 'To explore the underlying mechanics and architecture of LLMs.'}, {'query': 'Applications and use cases of Large Language Models', 'goal': 'To identify the various practical applications and industries where LLMs are utilized.'}, {'query': 'Challenges and limitations of Large Language Models', 'goal': 'To uncover potential drawbacks and issues faced in the deployment and development of LLMs.'}, {'query': 'Comparative analysis of different Large Language Models', 'goal': 'To compare and contrast various LLMs available, highlighting their differences and unique features.'}]\n",
      "['Understanding Large Language Models', 'Mechanics and Architecture of LLMs', 'Applications of LLMs', 'Challenges and Limitations', 'Comparative Analysis of Popular LLMs']\n",
      "What is a Large Language Model (LLM)?\n",
      "Querying for What is a Large Language Model (LLM)?\n",
      "Result link count 5\n",
      "retrieving page content for https://www.cloudflare.com/learning/ai/what-is-large-language-model/\n",
      "retrieving page content for https://aws.amazon.com/what-is/large-language-model/\n",
      "retrieving page content for https://www.ibm.com/think/topics/large-language-models\n",
      "retrieving page content for https://www.geeksforgeeks.org/large-language-model-llm/retrieving page content for https://en.wikipedia.org/wiki/Large_language_model\n",
      "\n",
      "Request failed: https://www.cloudflare.com/learning/ai/what-is-large-language-model/ :: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/ai/what-is-large-language-model/\n",
      "How do Large Language Models work?\n",
      "Querying for How do Large Language Models work?\n",
      "Result link count 5\n",
      "retrieving page content for https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f\n",
      "retrieving page content for https://aws.amazon.com/what-is/large-language-model/\n",
      "retrieving page content for https://www.cloudflare.com/learning/ai/what-is-large-language-model/\n",
      "retrieving page content for https://www.elastic.co/what-is/large-language-models\n",
      "retrieving page content for https://en.wikipedia.org/wiki/Large_language_model\n",
      "Request failed: https://www.cloudflare.com/learning/ai/what-is-large-language-model/ :: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/ai/what-is-large-language-model/\n",
      "Search retrieval complete complete\n",
      "\n",
      "\n",
      "\n",
      "Refining results\n",
      "-----------------------------------------\n",
      "\n",
      "{'url': 'https://aws.amazon.com/what-is/large-language-model/', 'markdown': \"Skip to main content\\n\\nClick here to return to Amazon Web Services homepage\\n\\nAbout AWS Contact Us Support  English  My Account\\n\\nSign In\\n\\nCreate an AWS Account\\n\\n__\\n\\n__\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nLogin\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nView profile\\n\\nLog out\\n\\n__\\n\\n  * Amazon Q\\n  * Products\\n  * Solutions\\n  * Pricing\\n  * Documentation\\n  * Learn\\n  * Partner Network\\n  * AWS Marketplace\\n  * Customer Enablement\\n  * Events\\n  * Explore More \\n\\n__\\n\\n__\\n\\nClose\\n\\n  * عربي\\n  * Bahasa Indonesia\\n  * Deutsch\\n  * English\\n  * Español\\n  * Français\\n  * Italiano\\n  * Português\\n\\n  * Tiếng Việt\\n  * Türkçe\\n  * Ρусский\\n  * ไทย\\n  * 日本語\\n  * 한국어\\n  * 中文 (简体)\\n  * 中文 (繁體)\\n\\nClose\\n\\n  * My Profile\\n  * Sign out of AWS Builder ID\\n  * AWS Management Console\\n  * Account Settings\\n  * Billing & Cost Management\\n  * Security Credentials\\n  * AWS Personal Health Dashboard\\n\\nClose\\n\\n  * Support Center\\n  * Expert Help\\n  * Knowledge Center\\n  * AWS Support Overview\\n  * AWS re:Post\\n\\nClick here to return to Amazon Web Services homepage\\n\\n__\\n\\n__\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nLogin\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nView profile\\n\\nLog out\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nView profile\\n\\nLog out\\n\\nGet Started for Free\\n\\nContact Us\\n\\n  * Products\\n  * Solutions\\n  * Pricing\\n  * Introduction to AWS\\n  * Getting Started\\n  * Documentation\\n  * Training and Certification\\n  * Developer Center\\n  * Customer Success\\n  * Partner Network\\n  * AWS Marketplace\\n  * Support\\n  * AWS re:Post\\n  * Log into Console\\n  * Download the Mobile App\\n\\n  * What is Cloud Computing?\\n  * Cloud Computing Concepts Hub\\n  * Generative AI\\n\\n# What is LLM (Large Language Model)?\\n\\n  \\n\\nCreate an AWS Account\\n\\nExplore Generative AI Services\\n\\nBuild, deploy, and run generative AI applications on AWS\\n\\nCheck out Generative AI on AWS\\n\\nInnovate faster with the most comprehensive set of Generative AI services\\n\\nBrowse Generative AI Trainings\\n\\nGet started on generative AI training with content built by AWS experts\\n\\nRead Generative AI Blogs\\n\\nGet the latest AWS generative AI product news and best practices\\n\\nWhat are Large Language Models? Why are large language models important? How\\ndo large language models work? What are applications of large language models?\\nHow are large language models trained? What is the future of LLMs? How can AWS\\nhelp with LLMs?\\n\\n## What are Large Language Models?\\n\\nLarge language models, also known as LLMs, are very large deep learning models\\nthat are pre-trained on vast amounts of data. The underlying transformer is a\\nset of neural networks that consist of an encoder and a decoder with self-\\nattention capabilities. The encoder and decoder extract meanings from a\\nsequence of text and understand the relationships between words and phrases in\\nit.\\n\\nTransformer LLMs are capable of unsupervised training, although a more precise\\nexplanation is that transformers perform self-learning. It is through this\\nprocess that transformers learn to understand basic grammar, languages, and\\nknowledge.\\n\\nUnlike earlier recurrent neural networks (RNN) that sequentially process\\ninputs, transformers process entire sequences in parallel. This allows the\\ndata scientists to use GPUs for training transformer-based LLMs, significantly\\nreducing the training time.\\n\\nTransformer neural network architecture allows the use of very large models,\\noften with hundreds of billions of parameters. Such large-scale models can\\ningest massive amounts of data, often from the internet, but also from sources\\nsuch as the Common Crawl, which comprises more than 50 billion web pages, and\\nWikipedia, which has approximately 57 million pages.\\n\\nRead more about neural networks »\\n\\nRead more about deep learning »\\n\\n## Why are large language models important?\\n\\nLarge language models are incredibly flexible. One model can perform\\ncompletely different tasks such as answering questions, summarizing documents,\\ntranslating languages and completing sentences. LLMs have the potential to\\ndisrupt content creation and the way people use search engines and virtual\\nassistants.\\n\\nWhile not perfect, LLMs are demonstrating a remarkable ability to make\\npredictions based on a relatively small number of prompts or inputs. LLMs can\\nbe used for generative AI (artificial intelligence) to produce content based\\non input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many\\npossible uses. Here are some examples:\\n\\n  * Open AI's GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural and readable output. While we don’t know the size of Claude 2, it can take inputs up to 100K tokens in each prompt, which means it can work over hundreds of pages of technical documentation or even an entire book.\\n  * AI21 Labs’ Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000-word parts and similar conversational capabilities.\\n  * Cohere’s Command model has similar capabilities and can work in more than 100 different languages.\\n  * LightOn's Paradigm offers foundation models with claimed capabilities that exceed those of GPT-3. All these LLMs come with APIs that allow developers to create unique generative AI applications.\\n\\nRead more about generative AI »\\n\\nRead more about foundation models »\\n\\n## How do large language models work?\\n\\nA key factor in how LLMs work is the way they represent words. Earlier forms\\nof machine learning used a numerical table to represent each word. But, this\\nform of representation could not recognize relationships between words such as\\nwords with similar meanings. This limitation was overcome by using multi-\\ndimensional vectors, commonly referred to as word embeddings, to represent\\nwords so that words with similar contextual meanings or other relationships\\nare close to each other in the vector space.\\n\\nUsing word embeddings, transformers can pre-process text as numerical\\nrepresentations through the encoder and understand the context of words and\\nphrases with similar meanings as well as other relationships between words\\nsuch as parts of speech. It is then possible for LLMs to apply this knowledge\\nof the language through the decoder to produce a unique output.\\n\\n## What are applications of large language models?\\n\\nThere are many practical applications for LLMs.\\n\\n### Copywriting\\n\\nApart from GPT-3 and ChatGPT, Claude, Llama 2, Cohere Command, and Jurassiccan\\nwrite original copy. AI21 Wordspice suggests changes to original sentences to\\nimprove style and voice.\\n\\n### Knowledge base answering\\n\\nOften referred to as knowledge-intensive natural language processing (KI-NLP),\\nthe technique refers to LLMs that can answer specific questions from\\ninformation help in digital archives. An example is the ability of AI21 Studio\\nplayground to answer general knowledge questions.\\n\\n### Text classification\\n\\nUsing clustering, LLMs can classify text with similar meanings or sentiments.\\nUses include measuring customer sentiment, determining the relationship\\nbetween texts, and document search.\\n\\n### Code generation\\n\\nLLM are proficient in code generation from natural language prompts. Examples\\ninclude Amazon CodeWhisperer and Open AI's codex used in GitHub Copilot, which\\ncan code in Python, JavaScript, Ruby and several other programming languages.\\nOther coding applications include creating SQL queries, writing shell commands\\nand website design. Learn more about AI code generation.\\n\\n### Text generation\\n\\nSimilar to code generation, text generation can complete incomplete sentences,\\nwrite product documentation or, like Alexa Create, write a short children's\\nstory.\\n\\n## How are large language models trained?\\n\\nTransformer-based neural networks are very large. These networks contain\\nmultiple nodes and layers. Each node in a layer has connections to all nodes\\nin the subsequent layer, each of which has a weight and a bias. Weights and\\nbiases along with embeddings are known as model parameters. Large transformer-\\nbased neural networks can have billions and billions of parameters. The size\\nof the model is generally determined by an empirical relationship between the\\nmodel size, the number of parameters, and the size of the training data.\\n\\nTraining is performed using a large corpus of high-quality data. During\\ntraining, the model iteratively adjusts parameter values until the model\\ncorrectly predicts the next token from an the previous squence of input\\ntokens. It does this through self-learning techniques which teach the model to\\nadjust parameters to maximize the likelihood of the next tokens in the\\ntraining examples.\\n\\nOnce trained, LLMs can be readily adapted to perform multiple tasks using\\nrelatively small sets of supervised data, a process known as fine tuning.\\n\\nThree common learning models exist:\\n\\n  * Zero-shot learning; Base LLMs can respond to a broad range of requests without explicit training, often through prompts, although answer accuracy varies.\\n  * Few-shot learning: By providing a few relevant training examples, base model performance significantly improves in that specific area.\\n  * Fine-tuning: This is an extension of few-shot learning in that data scientists train a base model to adjust its parameters with additional data relevant to the specific application.\\n\\n## What is the future of LLMs?\\n\\nThe introduction of large language models like ChatGPT, Claude 2, and Llama 2\\nthat can answer questions and generate text points to exciting possibilities\\nin the future. Slowly, but surely, LLMs are moving closer to human-like\\nperformance. The immediate success of these LLMs demonstrates a keen interest\\nin robotic-type LLMs that emulate and, in some contexts, outperform the human\\nbrain. Here are some thoughts on the future of LLMs,\\n\\n### Increased capabilities\\n\\nAs impressive as they are, the current level of technology is not perfect and\\nLLMs are not infallible. However, newer releases will have improved accuracy\\nand enhanced capabilities as developers learn how to improve their performance\\nwhile reducing bias and eliminating incorrect answers.\\n\\n### Audiovisual training\\n\\nWhile developers train most LLMs using text, some have started training models\\nusing video and audio input. This form of training should lead to faster model\\ndevelopment and open up new possibilities in terms of using LLMs for\\nautonomous vehicles.\\n\\n### Workplace transformation\\n\\nLLMs are a disruptive factor that will change the workplace. LLMs will likely\\nreduce monotonous and repetitive tasks in the same way that robots did for\\nrepetitive manufacturing tasks. Possibilities include repetitive clerical\\ntasks, customer service chatbots, and simple automated copywriting.\\n\\n### Conversational AI\\n\\nLLMs will undoubtedly improve the performance of automated virtual assistants\\nlike Alexa, Google Assistant, and Siri. They will be better able to interpret\\nuser intent and respond to sophisticated commands.\\n\\nRead more about conversational AI\\n\\n## How can AWS help with LLMs?\\n\\nAWS offers several possibilities for large language model developers. Amazon\\nBedrock is the easiest way to build and scale generative AI applications with\\nLLMs. Amazon Bedrock is a fully managed service that makes LLMs from Amazon\\nand leading AI startups available through an API, so you can choose from\\nvarious LLMs to find the model that's best suited for your use case.\\n\\nAmazon SageMaker JumpStart __ is a machine learning hub with foundation\\nmodels, built-in algorithms, and prebuilt ML solutions that you can deploy\\nwith just a few clicks With SageMaker JumpStart, you can access pretrained\\nmodels, including foundation models, to perform tasks like article\\nsummarization and image generation. Pretrained models are fully customizable\\nfor your use case with your data, and you can easily deploy them into\\nproduction with the user interface or SDK.\\n\\nGet started with LLMs and AI on AWS by creating a free account today.\\n\\n##  Next Steps on AWS\\n\\nCheck out additional product-related resources\\n\\nInnovate faster with AWS generative AI services  __\\n\\nSign up for a free account\\n\\nInstant get access to the AWS Free Tier.\\n\\nSign up  __\\n\\nStart building in the console\\n\\nGet started building in the AWS management console.\\n\\nSign in  __\\n\\nSign In to the Console\\n\\n###  Learn About AWS\\n\\n  * What Is AWS?\\n  * What Is Cloud Computing?\\n  * AWS Accessibility\\n  * What Is DevOps?\\n  * What Is a Container?\\n  * What Is a Data Lake?\\n  * What is Artificial Intelligence (AI)?\\n  * What is Generative AI?\\n  * What is Machine Learning (ML)?\\n  * AWS Cloud Security\\n  * What's New\\n  * Blogs\\n  * Press Releases\\n\\n###  Resources for AWS\\n\\n  * Getting Started\\n  * Training and Certification\\n  * AWS Trust Center\\n  * AWS Solutions Library\\n  * Architecture Center\\n  * Product and Technical FAQs\\n  * Analyst Reports\\n  * AWS Partners\\n\\n###  Developers on AWS\\n\\n  * Developer Center\\n  * SDKs & Tools\\n  * .NET on AWS\\n  * Python on AWS\\n  * Java on AWS\\n  * PHP on AWS\\n  * JavaScript on AWS\\n\\n###  Help\\n\\n  * Contact Us\\n  * Get Expert Help\\n  * File a Support Ticket\\n  * AWS re:Post\\n  * Knowledge Center\\n  * AWS Support Overview\\n  * Legal\\n  * AWS Careers\\n\\nCreate an AWS Account\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\nAmazon is an Equal Opportunity Employer: _Minority / Women / Disability /\\nVeteran / Gender Identity / Sexual Orientation / Age._\\n\\n  * Language\\n  * عربي\\n  * Bahasa Indonesia\\n  * Deutsch\\n  * English\\n  * Español\\n  * Français\\n  * Italiano\\n  * Português\\n  * Tiếng Việt\\n  * Türkçe\\n  * Ρусский\\n  * ไทย\\n  * 日本語\\n  * 한국어\\n  * 中文 (简体)\\n  * 中文 (繁體)\\n\\n  * Privacy\\n  * |\\n  * Accessibility\\n  * |\\n  * Site Terms\\n  * |\\n  * Cookie Preferences \\n  * |\\n  * © 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\\n\\n####  Ending Support for Internet Explorer\\n\\nGot it\\n\\nAWS support for Internet Explorer ends on 07/31/2022. Supported browsers are\\nChrome, Firefox, Edge, and Safari. Learn more »\\n\\nGot it\\n\\n\"}\n",
      "Refining :: https://aws.amazon.com/what-is/large-language-model/\n",
      "-------------------------------\n",
      "{'url': 'https://www.ibm.com/think/topics/large-language-models', 'markdown': \"#  What are large language models (LLMs)?\\n\\nArtificial Intelligence\\n\\n####  2 November 2023\\n\\nLink copied\\n\\n##  What are LLMs?\\n\\nLarge language models (LLMs) are a category of foundation models trained on\\nimmense amounts of data making them capable of understanding and generating\\nnatural language and other types of content to perform a wide range of tasks.\\n\\nLLMs have become a household name thanks to the role they have played in\\nbringing generative AI to the forefront of the public interest, as well as the\\npoint on which organizations are focusing to adopt artificial intelligence\\nacross numerous business functions and use cases.\\n\\nOutside of the enterprise context, it may seem like LLMs have arrived out of\\nthe blue along with new developments in generative AI. However, many\\ncompanies, including IBM, have spent years implementing LLMs at different\\nlevels to enhance their natural language understanding (NLU) and natural\\nlanguage processing (NLP) capabilities. This has occurred alongside advances\\nin machine learning, machine learning models, algorithms, neural networks and\\nthe transformer models that provide the architecture for these AI systems.\\n\\nLLMs are a class of foundation models, which are trained on enormous amounts\\nof data to provide the foundational capabilities needed to drive multiple use\\ncases and applications, as well as resolve a multitude of tasks. This is in\\nstark contrast to the idea of building and training domain specific models for\\neach of these use cases individually, which is prohibitive under many criteria\\n(most importantly cost and infrastructure), stifles synergies and can even\\nlead to inferior performance.\\n\\nLLMs represent a significant breakthrough in NLP and artificial intelligence,\\nand are easily accessible to the public through interfaces like Open AI’s Chat\\nGPT-3 and GPT-4, which have garnered the support of Microsoft. Other examples\\ninclude Meta’s Llama models and Google’s bidirectional encoder representations\\nfrom transformers (BERT/RoBERTa) and PaLM models. IBM has also recently\\nlaunched its Granite model series on watsonx.ai, which has become the\\ngenerative AI backbone for other IBM products like watsonx Assistant and\\nwatsonx Orchestrate.\\n\\nIn a nutshell, LLMs are designed to understand and generate text like a human,\\nin addition to other forms of content, based on the vast amount of data used\\nto train them. They have the ability to infer from context, generate coherent\\nand contextually relevant responses, translate to languages other than\\nEnglish, summarize text, answer questions (general conversation and FAQs) and\\neven assist in creative writing or code generation tasks.\\n\\nThey are able to do this thanks to billions of parameters that enable them to\\ncapture intricate patterns in language and perform a wide array of language-\\nrelated tasks. LLMs are revolutionizing applications in various fields, from\\nchatbots and virtual assistants to content generation, research assistance and\\nlanguage translation.\\n\\nAs they continue to evolve and improve, LLMs are poised to reshape the way we\\ninteract with technology and access information, making them a pivotal part of\\nthe modern digital landscape.\\n\\nIndustry newsletter\\n\\n### The latest tech news, backed by expert insights\\n\\n### Thank you! You are subscribed.\\n\\nStay up to date on the most important—and intriguing—industry trends on AI,\\nautomation, data and beyond with the Think newsletter. See the IBM Privacy\\nStatement.\\n\\nYour subscription will be delivered in English. You will find an unsubscribe\\nlink in every newsletter. You can manage your subscriptions or unsubscribe\\nhere. Refer to our IBM Privacy Statement for more information.  \\n\\n##  How large language models work\\n\\nLLMs operate by leveraging deep learning techniques and vast amounts of\\ntextual data. These models are typically based on a transformer architecture,\\nlike the generative pre-trained transformer, which excels at handling\\nsequential data like text input. LLMs consist of multiple layers of neural\\nnetworks, each with parameters that can be fine-tuned during training, which\\nare enhanced further by a numerous layer known as the attention mechanism,\\nwhich dials in on specific parts of data sets.\\n\\nDuring the training process, these models learn to predict the next word in a\\nsentence based on the context provided by the preceding words. The model does\\nthis through attributing a probability score to the recurrence of words that\\nhave been tokenized— broken down into smaller sequences of characters. These\\ntokens are then transformed into embeddings, which are numeric representations\\nof this context.\\n\\nTo ensure accuracy, this process involves training the LLM on a massive\\ncorpora of text (in the billions of pages), allowing it to learn grammar,\\nsemantics and conceptual relationships through zero-shot and self-supervised\\nlearning. Once trained on this training data, LLMs can generate text by\\nautonomously predicting the next word based on the input they receive, and\\ndrawing on the patterns and knowledge they've acquired. The result is coherent\\nand contextually relevant language generation that can be harnessed for a wide\\nrange of NLU and content generation tasks.\\n\\nModel performance can also be increased through prompt engineering, prompt-\\ntuning, fine-tuning and other tactics like reinforcement learning with human\\nfeedback (RLHF) to remove the biases, hateful speech and factually incorrect\\nanswers known as “hallucinations” that are often unwanted byproducts of\\ntraining on so much unstructured data. This is one of the most important\\naspects of ensuring enterprise-grade LLMs are ready for use and do not expose\\norganizations to unwanted liability, or cause damage to their reputation.\\n\\nAI Academy\\n\\n###  Why foundation models are a paradigm shift for AI\\n\\nLearn about a new class of flexible, reusable AI models that can unlock new\\nrevenue, reduce costs and increase productivity, then use our guidebook to\\ndive deeper.\\n\\nGo to episode\\n\\n##  LLM use cases\\n\\nLLMs are redefining an increasing number of business processes and have proven\\ntheir versatility across a myriad of use cases and tasks in various\\nindustries. They augment conversational AI in chatbots and virtual assistants\\n(like IBM watsonx Assistant and Google’s BARD) to enhance the interactions\\nthat underpin excellence in customer care, providing context-aware responses\\nthat mimic interactions with human agents.\\n\\nLLMs also excel in content generation, automating content creation for blog\\narticles, marketing or sales materials and other writing tasks. In research\\nand academia, they aid in summarizing and extracting information from vast\\ndatasets, accelerating knowledge discovery. LLMs also play a vital role in\\nlanguage translation, breaking down language barriers by providing accurate\\nand contextually relevant translations. They can even be used to write code,\\nor “translate” between programming languages.\\n\\nMoreover, they contribute to accessibility by assisting individuals with\\ndisabilities, including text-to-speech applications and generating content in\\naccessible formats. From healthcare to finance, LLMs are transforming\\nindustries by streamlining processes, improving customer experiences and\\nenabling more efficient and data-driven decision making.\\n\\nMost excitingly, all of these capabilities are easy to access, in some cases\\nliterally an API integration away.\\n\\nHere is a list of some of the most important areas where LLMs benefit\\norganizations:\\n\\n  * **Text generation:** language generation abilities, such as writing emails, blog posts or other mid-to-long form content in response to prompts that can be refined and polished. An excellent example is retrieval-augmented generation (RAG). \\n\\n  * **Content summarization:** summarize long articles, news stories, research reports, corporate documentation and even customer history into thorough texts tailored in length to the output format.\\n\\n  * **AI assistants:** chatbots that answer customer queries, perform backend tasks and provide detailed information in natural language as a part of an integrated, self-serve customer care solution. \\n\\n  * **Code generation:** assists developers in building applications, finding errors in code and uncovering security issues in multiple programming languages, even “translating” between them.\\n\\n  * **Sentiment analysis:** analyze text to determine the customer’s tone in order understand customer feedback at scale and aid in brand reputation management. \\n\\n  * **Language translation:** provides wider coverage to organizations across languages and geographies with fluent translations and multilingual capabilities.\\n\\nLLMs stand to impact every industry, from finance to insurance, human\\nresources to healthcare and beyond, by automating customer self-service,\\naccelerating response times on an increasing number of tasks as well as\\nproviding greater accuracy, enhanced routing and intelligent context\\ngathering.\\n\\n##  LLMs and governance\\n\\nOrganizations need a solid foundation in governance practices to harness the\\npotential of AI models to revolutionize the way they do business. This means\\nproviding access to AI tools and technology that is trustworthy, transparent,\\nresponsible and secure. AI governance and traceability are also fundamental\\naspects of the solutions IBM brings to its customers, so that activities that\\ninvolve AI are managed and monitored to allow for tracing origins, data and\\nmodels in a way that is always auditable and accountable.\\n\\nEbook  How to choose the right foundation model\\n\\nLearn how to choose the right approach in preparing datasets and employing\\nfoundation models.\\n\\nRead the ebook\\n\\nRelated content  Read: DeepSeek’s reasoning AI shows power of small models,\\nefficiently trained\\n\\nRelated solutions\\n\\nFoundation models\\n\\nExplore the IBM library of foundation models in the watsonx portfolio to scale\\ngenerative AI for your business with confidence.\\n\\nDiscover watsonx.ai\\n\\nArtificial intelligence solutions\\n\\nPut AI to work in your business with IBM’s industry-leading AI expertise and\\nportfolio of solutions at your side.\\n\\nExplore AI solutions\\n\\nAI consulting and services\\n\\nReinvent critical workflows and operations by adding AI to maximize\\nexperiences, real-time decision-making and business value.\\n\\nExplore AI services\\n\\n##  Resources\\n\\nAI models  Explore IBM Granite\\n\\nDiscover IBM® Granite™, our family of open, performant and trusted AI models,\\ntailored for business and optimized to scale your AI applications. Explore\\nlanguage, code, time series and guardrail options.\\n\\nMeet Granite\\n\\nEbook  How to choose the right foundation model\\n\\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\nArticle  Discover the power of LLMs\\n\\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge\\nof LLMs.\\n\\nExplore the articles\\n\\nGuide  The CEO’s guide to model optimization\\n\\nLearn how to continually push teams to improve model performance and outpace\\nthe competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide\\n\\nReport  A differentiated approach to AI foundation models\\n\\nExplore the value of enterprise-grade foundation models that provide trust,\\nperformance and cost-effective benefits to all industries.\\n\\nRead the report\\n\\nEbook  Unlock the Power of Generative AI and ML\\n\\nLearn how to incorporate generative AI, machine learning and foundation models\\ninto your business operations for improved performance.\\n\\nRead the ebook\\n\\nReport  AI in Action 2024\\n\\nRead about 2,000 organizations we surveyed about their AI initiatives to\\ndiscover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\nTake the next step\\n\\nExplore the IBM library of foundation models in the IBM watsonx portfolio to\\nscale generative AI for your business with confidence.\\n\\nExplore watsonx.ai Explore AI solutions\\n\\n\"}\n",
      "Refining :: https://www.ibm.com/think/topics/large-language-models\n",
      "-------------------------------\n",
      "{'url': 'https://en.wikipedia.org/wiki/Large_language_model', 'markdown': 'Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nmove to sidebar hide\\n\\nNavigation\\n\\n  * Main page\\n  * Contents\\n  * Current events\\n  * Random article\\n  * About Wikipedia\\n  * Contact us\\n\\nContribute\\n\\n  * Help\\n  * Learn to edit\\n  * Community portal\\n  * Recent changes\\n  * Upload file\\n  * Special pages\\n\\nSearch\\n\\nSearch\\n\\nAppearance\\n\\n  * Donate\\n  * Create account\\n  * Log in\\n\\nPersonal tools\\n\\n  * Donate\\n  * Create account\\n  * Log in\\n\\nPages for logged out editors learn more\\n\\n  * Contributions\\n  * Talk\\n\\n## Contents\\n\\nmove to sidebar hide\\n\\n  * (Top)\\n\\n  * 1 History\\n\\n  * 2 Dataset preprocessing\\n\\nToggle Dataset preprocessing subsection\\n\\n    * 2.1 Tokenization\\n\\n      * 2.1.1 BPE\\n\\n      * 2.1.2 Problems\\n\\n    * 2.2 Dataset cleaning\\n\\n    * 2.3 Synthetic data\\n\\n  * 3 Training and architecture\\n\\nToggle Training and architecture subsection\\n\\n    * 3.1 Reinforcement learning from human feedback\\n\\n    * 3.2 Instruction tuning\\n\\n    * 3.3 Mixture of experts\\n\\n    * 3.4 Prompt engineering, attention mechanism, and context window\\n\\n    * 3.5 Infrastructure\\n\\n  * 4 Training cost\\n\\n  * 5 Tool use\\n\\n  * 6 Agency\\n\\n  * 7 Compression\\n\\n  * 8 Multimodality\\n\\n  * 9 Reasoning\\n\\n  * 10 Properties\\n\\nToggle Properties subsection\\n\\n    * 10.1 Scaling laws\\n\\n    * 10.2 Emergent abilities\\n\\n  * 11 Interpretation\\n\\nToggle Interpretation subsection\\n\\n    * 11.1 Understanding and intelligence\\n\\n  * 12 Evaluation\\n\\nToggle Evaluation subsection\\n\\n    * 12.1 Perplexity\\n\\n      * 12.1.1 BPW, BPC, and BPT\\n\\n    * 12.2 Task-specific datasets and benchmarks\\n\\n      * 12.2.1 Adversarially constructed evaluations\\n\\n      * 12.2.2 Limitations of LLM benchmarks\\n\\n  * 13 Wider impact\\n\\nToggle Wider impact subsection\\n\\n    * 13.1 Memorization and copyright\\n\\n    * 13.2 Security\\n\\n    * 13.3 Algorithmic bias\\n\\n      * 13.3.1 Stereotyping\\n\\n      * 13.3.2 Selection bias\\n\\n      * 13.3.3 Political bias\\n\\n    * 13.4 Energy demands\\n\\n  * 14 See also\\n\\n  * 15 References\\n\\n  * 16 Further reading\\n\\nToggle the table of contents\\n\\n# Large language model\\n\\n54 languages\\n\\n  * Afrikaans\\n  * العربية\\n  * Aragonés\\n  * Azərbaycanca\\n  * বাংলা\\n  * 閩南語 / Bân-lâm-gú\\n  * Boarisch\\n  * Bosanski\\n  * Català\\n  * Čeština\\n  * Dansk\\n  * Deutsch\\n  * Ελληνικά\\n  * Español\\n  * Esperanto\\n  * Euskara\\n  * فارسی\\n  * Français\\n  * Gaeilge\\n  * Galego\\n  * 한국어\\n  * हिन्दी\\n  * Ido\\n  * Bahasa Indonesia\\n  * IsiZulu\\n  * Italiano\\n  * עברית\\n  * Қазақша\\n  * Magyar\\n  * Македонски\\n  * Монгол\\n  * Nederlands\\n  * 日本語\\n  * Polski\\n  * Português\\n  * Qaraqalpaqsha\\n  * Română\\n  * Runa Simi\\n  * Русский\\n  * Shqip\\n  * Simple English\\n  * Slovenščina\\n  * کوردی\\n  * Српски / srpski\\n  * Tagalog\\n  * తెలుగు\\n  * ไทย\\n  * Türkçe\\n  * Українська\\n  * ئۇيغۇرچە / Uyghurche\\n  * Tiếng Việt\\n  * 文言\\n  * 粵語\\n  * 中文\\n\\nEdit links\\n\\n  * Article\\n  * Talk\\n\\nEnglish\\n\\n  * Read\\n  * Edit\\n  * View history\\n\\nTools\\n\\nTools\\n\\nmove to sidebar hide\\n\\nActions\\n\\n  * Read\\n  * Edit\\n  * View history\\n\\nGeneral\\n\\n  * What links here\\n  * Related changes\\n  * Upload file\\n  * Permanent link\\n  * Page information\\n  * Cite this page\\n  * Get shortened URL\\n  * Download QR code\\n\\nPrint/export\\n\\n  * Download as PDF\\n  * Printable version\\n\\nIn other projects\\n\\n  * Wikimedia Commons\\n  * Wikidata item\\n\\nAppearance\\n\\nmove to sidebar hide\\n\\nFrom Wikipedia, the free encyclopedia\\n\\nType of machine learning model\\n\\nNot to be confused with Logic learning machine.\\n\\nPart of a series on  \\n---  \\nMachine learning  \\nand data mining  \\nParadigms\\n\\n  * Supervised learning\\n  * Unsupervised learning\\n  * Semi-supervised learning\\n  * Self-supervised learning\\n  * Reinforcement learning\\n  * Meta-learning\\n  * Online learning\\n  * Batch learning\\n  * Curriculum learning\\n  * Rule-based learning\\n  * Neuro-symbolic AI\\n  * Neuromorphic engineering\\n  * Quantum machine learning\\n\\n  \\nProblems\\n\\n  * Classification\\n  * Generative modeling\\n  * Regression\\n  * Clustering\\n  * Dimensionality reduction\\n  * Density estimation\\n  * Anomaly detection\\n  * Data cleaning\\n  * AutoML\\n  * Association rules\\n  * Semantic analysis\\n  * Structured prediction\\n  * Feature engineering\\n  * Feature learning\\n  * Learning to rank\\n  * Grammar induction\\n  * Ontology learning\\n  * Multimodal learning\\n\\n  \\nSupervised learning  \\n(**classification** • **regression**)\\n\\n  * Apprenticeship learning\\n  * Decision trees\\n  * Ensembles\\n    * Bagging\\n    * Boosting\\n    * Random forest\\n  * _k_ -NN\\n  * Linear regression\\n  * Naive Bayes\\n  * Artificial neural networks\\n  * Logistic regression\\n  * Perceptron\\n  * Relevance vector machine (RVM)\\n  * Support vector machine (SVM)\\n\\n  \\nClustering\\n\\n  * BIRCH\\n  * CURE\\n  * Hierarchical\\n  * _k_ -means\\n  * Fuzzy\\n  * Expectation–maximization (EM)\\n  *   \\nDBSCAN\\n\\n  * OPTICS\\n  * Mean shift\\n\\n  \\nDimensionality reduction\\n\\n  * Factor analysis\\n  * CCA\\n  * ICA\\n  * LDA\\n  * NMF\\n  * PCA\\n  * PGD\\n  * t-SNE\\n  * SDL\\n\\n  \\nStructured prediction\\n\\n  * Graphical models\\n    * Bayes net\\n    * Conditional random field\\n    * Hidden Markov\\n\\n  \\nAnomaly detection\\n\\n  * RANSAC\\n  * _k_ -NN\\n  * Local outlier factor\\n  * Isolation forest\\n\\n  \\nArtificial neural network\\n\\n  * Autoencoder\\n  * Deep learning\\n  * Feedforward neural network\\n  * Recurrent neural network\\n    * LSTM\\n    * GRU\\n    * ESN\\n    * reservoir computing\\n  * Boltzmann machine\\n    * Restricted\\n  * GAN\\n  * Diffusion model\\n  * SOM\\n  * Convolutional neural network\\n    * U-Net\\n    * LeNet\\n    * AlexNet\\n    * DeepDream\\n  * Neural radiance field\\n  * Transformer\\n    * Vision\\n  * Mamba\\n  * Spiking neural network\\n  * Memtransistor\\n  * Electrochemical RAM (ECRAM)\\n\\n  \\nReinforcement learning\\n\\n  * Q-learning\\n  * SARSA\\n  * Temporal difference (TD)\\n  * Multi-agent\\n    * Self-play\\n\\n  \\nLearning with humans\\n\\n  * Active learning\\n  * Crowdsourcing\\n  * Human-in-the-loop\\n  * RLHF\\n\\n  \\nModel diagnostics\\n\\n  * Coefficient of determination\\n  * Confusion matrix\\n  * Learning curve\\n  * ROC curve\\n\\n  \\nMathematical foundations\\n\\n  * Kernel machines\\n  * Bias–variance tradeoff\\n  * Computational learning theory\\n  * Empirical risk minimization\\n  * Occam learning\\n  * PAC learning\\n  * Statistical learning\\n  * VC theory\\n  * Topological deep learning\\n\\n  \\nJournals and conferences\\n\\n  * ECML PKDD\\n  * NeurIPS\\n  * ICML\\n  * ICLR\\n  * IJCAI\\n  * ML\\n  * JMLR\\n\\n  \\nRelated articles\\n\\n  * Glossary of artificial intelligence\\n  * List of datasets for machine-learning research\\n    * List of datasets in computer vision and image processing\\n  * Outline of machine learning\\n\\n  \\n  \\n  * v\\n  * t\\n  * e\\n\\n  \\n  \\nA **large language model** (**LLM**) is a type of machine learning model\\ndesigned for natural language processing tasks such as language generation.\\nLLMs are language models with many parameters, and are trained with self-\\nsupervised learning on a vast amount of text.\\n\\nThe largest and most capable LLMs are generative pretrained transformers\\n(GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt\\nengineering.[1] These models acquire predictive power regarding syntax,\\nsemantics, and ontologies[2] inherent in human language corpora, but they also\\ninherit inaccuracies and biases present in the data they are trained in.[3]\\n\\n## History\\n\\n[edit]\\n\\nThe training compute of notable large models in FLOPs vs publication date over\\nthe period 2010-2024. For overall notable models (top left), frontier models\\n(top right), top language models (bottom left) and top models within leading\\ncompanies (bottom right). The majority of these models are language models.\\nThe training compute of notable large AI models in FLOPs vs publication date\\nover the period 2017-2024. The majority of large models are language models or\\nmultimodal models with language capacity.\\n\\nBefore 2017, there were a few language models that were large as compared to\\ncapacities then available. In the 1990s, the IBM alignment models pioneered\\nstatistical language modelling. A smoothed n-gram model in 2001 trained on 0.3\\nbillion words achieved state-of-the-art perplexity at the time.[4] In the\\n2000s, as Internet use became prevalent, some researchers constructed\\nInternet-scale language datasets (\"web as corpus\"[5]), upon which they trained\\nstatistical language models.[6][7] In 2009, in most language processing tasks,\\nstatistical language models dominated over symbolic language models because\\nthey can usefully ingest large datasets.[8]\\n\\nAfter neural networks became dominant in image processing around 2012,[9] they\\nwere applied to language modelling as well. Google converted its translation\\nservice to Neural Machine Translation in 2016. Because it preceded the\\nexistence of transformers, it was done by seq2seq deep LSTM networks.\\n\\nAn illustration of main components of the transformer model from the original\\npaper, where layers were normalized after (instead of before) multiheaded\\nattention\\n\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer\\narchitecture in their landmark paper \"Attention Is All You Need\". This paper\\'s\\ngoal was to improve upon 2014 seq2seq technology,[10] and was based mainly on\\nthe attention mechanism developed by Bahdanau et al. in 2014.[11] The\\nfollowing year in 2018, BERT was introduced and quickly became\\n\"ubiquitous\".[12] Though the original transformer has both encoder and decoder\\nblocks, BERT is an encoder-only model. Academic and research usage of BERT\\nbegan to decline in 2023, following rapid improvements in the abilities of\\ndecoder-only models (such as GPT) to solve tasks via prompting.[13]\\n\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that\\ncaught widespread attention because OpenAI at first deemed it too powerful to\\nrelease publicly, out of fear of malicious use.[14] GPT-3 in 2020 went a step\\nfurther and as of 2024[update] is available only via API with no offering of\\ndownloading the model to execute locally. But it was the 2022 consumer-facing\\nbrowser-based ChatGPT that captured the imaginations of the general population\\nand caused some media hype and online buzz.[15] The 2023 GPT-4 was praised for\\nits increased accuracy and as a \"holy grail\" for its multimodal\\ncapabilities.[16] OpenAI did not reveal the high-level architecture and the\\nnumber of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM\\nusage across several research subfields of computer science, including\\nrobotics, software engineering, and societal impact work.[17] In 2024 OpenAI\\nreleased the reasoning model OpenAI o1, which generates long chains of thought\\nbefore returning a final answer.\\n\\nCompeting language models have for the most part been attempting to equal the\\nGPT series, at least in terms of number of parameters.[18]\\n\\nSince 2022, source-available models have been gaining popularity, especially\\nat first with BLOOM and LLaMA, though both have restrictions on the field of\\nuse. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive\\nApache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-\\nparameter open-weight model that performs comparably to OpenAI o1 but at a\\nmuch lower cost.[19]\\n\\nSince 2023, many LLMs have been trained to be multimodal, having the ability\\nto also process or generate other types of data, such as images or audio.\\nThese LLMs are also called large multimodal models (LMMs).[20]\\n\\nAs of 2024, the largest and most capable models are all based on the\\ntransformer architecture. Some recent implementations are based on other\\narchitectures, such as recurrent neural network variants and Mamba (a state\\nspace model).[21][22][23]\\n\\n## Dataset preprocessing\\n\\n[edit]\\n\\nSee also: List of datasets for machine-learning research § Internet\\n\\n### Tokenization\\n\\n[edit]\\n\\nAs machine learning algorithms process numbers rather than text, the text must\\nbe converted to numbers. In the first step, a vocabulary is decided upon, then\\ninteger indices are arbitrarily but uniquely assigned to each vocabulary\\nentry, and finally, an embedding is associated to the integer index.\\nAlgorithms include byte-pair encoding (BPE) and WordPiece. There are also\\nspecial tokens serving as control characters, such as `[MASK]` for masked-out\\ntoken (as used in BERT), and `[UNK]` (\"unknown\") for characters not appearing\\nin the vocabulary. Also, some special symbols are used to denote special text\\nformatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and\\nGPT. \"##\" denotes continuation of a preceding word in BERT.[24]\\n\\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split `tokenizer:\\ntexts -> series of numerical \"tokens\"` as\\n\\ntoken  | izer  | :  |  texts  |  -> | series  |  of  |  numerical  |  \"  | t  | ok  | ens  | \"   \\n---|---|---|---|---|---|---|---|---|---|---|---|---  \\n  \\nTokenization also compresses the datasets. Because LLMs generally require\\ninput to be an array that is not jagged, the shorter texts must be \"padded\"\\nuntil they match the length of the longest one. How many tokens are, on\\naverage, needed per word depends on the language of the dataset.[25][26]\\n\\n#### BPE\\n\\n[edit]\\n\\nMain article: Byte pair encoding\\n\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first\\nstep, all unique characters (including blanks and punctuation marks) are\\ntreated as an initial set of _n_ -grams (i.e. initial set of uni-grams).\\nSuccessively the most frequent pair of adjacent characters is merged into a\\nbi-gram and all instances of the pair are replaced by it. All occurrences of\\nadjacent pairs of (previously merged) _n_ -grams that most frequently occur\\ntogether are then again merged into even lengthier _n_ -gram, until a\\nvocabulary of prescribed size is obtained (in case of GPT-3, the size is\\n50257).[27] After a tokenizer is trained, any text can be tokenized by it, as\\nlong as it does not contain characters not appearing in the initial-set of\\nuni-grams.[28]\\n\\n#### Problems\\n\\n[edit]\\n\\nA token vocabulary based on the frequencies extracted from mainly English\\ncorpora uses as few tokens as possible for an average English word. However,\\nan average word in another language encoded by such an English-optimized\\ntokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use\\nup to 15 times more tokens per word for some languages, for example for the\\nShan language from Myanmar. Even more widespread languages such as Portuguese\\nand German have \"a premium of 50%\" compared to English.[29]\\n\\nGreedy tokenization also causes subtle problems with text completion.[30]\\n\\n### Dataset cleaning\\n\\n[edit]\\n\\nMain article: Data cleansing\\n\\nIn the context of training LLMs, datasets are typically cleaned by removing\\nlow-quality, duplicated, or toxic data.[31] Cleaned datasets can increase\\ntraining efficiency and lead to improved downstream performance.[32][33] A\\ntrained LLM can be used to clean datasets for training a further LLM.[34]\\n\\nWith the increasing proportion of LLM-generated content on the web, data\\ncleaning in the future may include filtering out such content. LLM-generated\\ncontent can pose a problem if the content is similar to human text (making\\nfiltering difficult) but of lower quality (degrading performance of models\\ntrained on it).[35]\\n\\n### Synthetic data\\n\\n[edit]\\n\\nMain article: Synthetic data\\n\\nTraining of largest language models might need more linguistic data than\\nnaturally available, or that the naturally occurring data is of insufficient\\nquality. In these cases, synthetic data might be used. Microsoft\\'s Phi series\\nof LLMs is trained on textbook-like data generated by another LLM.[36]\\n\\n## Training and architecture\\n\\n[edit]\\n\\nSee also: Fine-tuning (machine learning)\\n\\n### Reinforcement learning from human feedback\\n\\n[edit]\\n\\nReinforcement learning from human feedback (RLHF) through algorithms, such as\\nproximal policy optimization, is used to further fine-tune a model based on a\\ndataset of human preferences.[37]\\n\\n### Instruction tuning\\n\\n[edit]\\n\\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct\\nresponses, replacing any naive responses, starting from human-generated\\ncorrections of a few cases. For example, in the instruction \"Write an essay\\nabout the main themes represented in _Hamlet_ ,\" an initial naive completion\\nmight be \"If you submit the essay after March 17, your grade will be reduced\\nby 10% for each day of delay,\" based on the frequency of this textual sequence\\nin the corpus.[38]\\n\\n### Mixture of experts\\n\\n[edit]\\n\\nMain article: Mixture of experts\\n\\nThe largest LLM may be too expensive to train and use directly. For such\\nmodels, mixture of experts (MoE) can be applied, a line of research pursued by\\nGoogle researchers since 2017 to train models reaching up to 1 trillion\\nparameters.[39][40][41]\\n\\n### Prompt engineering, attention mechanism, and context window\\n\\n[edit]\\n\\nSee also: Prompt engineering and Attention (machine learning)\\n\\nMost results previously achievable only by (costly) fine-tuning, can be\\nachieved through prompt engineering, although limited to the scope of a single\\nconversation (more precisely, limited to the scope of a context window).[42]\\n\\nWhen each head calculates, according to its own criteria, how much other\\ntokens are relevant for the \"it_\" token, note that the second attention head,\\nrepresented by the second column, is focusing most on the first two rows, i.e.\\nthe tokens \"The\" and \"animal\", while the third column is focusing most on the\\nbottom two rows, i.e. on \"tired\", which has been tokenized into two\\ntokens.[43]\\n\\nIn order to find out which tokens are relevant to each other within the scope\\nof the context window, the attention mechanism calculates \"soft\" weights for\\neach token, more precisely for its embedding, by using multiple attention\\nheads, each with its own \"relevance\" for calculating its own soft weights. For\\nexample, the small (i.e. 117M parameter sized) GPT-2 model has had twelve\\nattention heads and a context window of only 1k tokens.[44] In its medium\\nversion it has 345M parameters and contains 24 layers, each with 12 attention\\nheads. For the training with gradient descent a batch size of 512 was\\nutilized.[28]\\n\\nThe largest models, such as Google\\'s Gemini 1.5, presented in February 2024,\\ncan have a context window sized up to 1 million (context window of 10 million\\nwas also \"successfully tested\").[45] Other models with large context windows\\nincludes Anthropic\\'s Claude 2.1, with a context window of up to 200k\\ntokens.[46] Note that this maximum refers to the number of input tokens and\\nthat the maximum number of output tokens differs from the input and is often\\nsmaller. For example, the GPT-4 Turbo model has a maximum output of 4096\\ntokens.[47]\\n\\nLength of a conversation that the model can take into account when generating\\nits next answer is limited by the size of a context window, as well. If the\\nlength of a conversation, for example with ChatGPT, is longer than its context\\nwindow, only the parts inside the context window are taken into account when\\ngenerating the next answer, or the model needs to apply some algorithm to\\nsummarize the too distant parts of conversation.\\n\\nThe shortcomings of making a context window larger include higher\\ncomputational cost and possibly diluting the focus on local context, while\\nmaking it smaller can cause a model to miss an important long-range\\ndependency. Balancing them is a matter of experimentation and domain-specific\\nconsiderations.\\n\\nA model may be pre-trained either to predict how the segment continues, or\\nwhat is missing in the segment, given a segment from its training dataset.[48]\\nIt can be either\\n\\n  * autoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\\n  * \"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\"[49] does it): for example, given a segment \"I like to `[__] [__]` cream\", the model predicts that \"eat\" and \"ice\" are missing.\\n\\nModels may be trained on auxiliary tasks which test their understanding of the\\ndata distribution, such as Next Sentence Prediction (NSP), in which pairs of\\nsentences are presented and the model must predict whether they appear\\nconsecutively in the training corpus.[49] During training, regularization loss\\nis also used to stabilize training. However regularization loss is usually not\\nused during testing and evaluation.\\n\\n### Infrastructure\\n\\n[edit]\\n\\nSubstantial infrastructure is necessary for training the largest\\nmodels.[50][51][52]\\n\\n## Training cost\\n\\n[edit]\\n\\nThe qualifier \"large\" in \"large language model\" is inherently vague, as there\\nis no definitive threshold for the number of parameters required to qualify as\\n\"large\". As time goes on, what was previously considered \"large\" may evolve.\\nGPT-1 of 2018 is usually considered the first LLM, even though it has only\\n0.117 billion parameters. The tendency towards larger models is visible in the\\nlist of large language models.\\n\\nAs technology advanced, large sums have been invested in increasingly large\\nmodels. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters\\nmodel) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-\\nparameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in\\n2021) cost around $11 million.[53]\\n\\nFor Transformer-based LLM, training cost is much higher than inference cost.\\nIt costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2\\nFLOPs per parameter to infer on one token.[54]\\n\\n## Tool use\\n\\n[edit]\\n\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at\\nleast not without the use of external tools or additional software. An example\\nof such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that\\nthe LLM has not already encountered a continuation of this calculation in its\\ntraining corpus.[_dubious – discuss_] In such cases, the LLM needs to resort\\nto running program code that calculates the result, which can then be included\\nin its response.[_dubious – discuss_]: Another example is \"What is the time\\nnow? It is \", where a separate program interpreter would need to execute a\\ncode to get system time on the computer, so that the LLM can include it in its\\nreply.[55][56] This basic strategy can be sophisticated with multiple attempts\\nof generated programs, and other sampling strategies.[57]\\n\\nGenerally, in order to get an LLM to use tools, one must fine-tune it for\\ntool-use. If the number of tools is finite, then fine-tuning may be done just\\nonce. If the number of tools can grow arbitrarily, as with online API\\nservices, then the LLM can be fine-tuned to be able to read API documentation\\nand call API correctly.[58][59]\\n\\nRetrieval-augmented generation (RAG) is another approach that enhances LLMs by\\nintegrating them with document retrieval systems. Given a query, a document\\nretriever is called to retrieve the most relevant documents. This is usually\\ndone by encoding the query and the documents into vectors, then finding the\\ndocuments with vectors (usually stored in a vector database) most similar to\\nthe vector of the query. The LLM then generates an output based on both the\\nquery and context included from the retrieved documents.[60]\\n\\n## Agency\\n\\n[edit]\\n\\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability\\nto interact with dynamic environments, recall past behaviors, and plan future\\nactions, but can be transformed into one by integrating modules like\\nprofiling, memory, planning, and action.[61]\\n\\nThe ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of\\nan LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\".\\nSpecifically, the language model is prompted with a textual description of the\\nenvironment, a goal, a list of possible actions, and a record of the actions\\nand observations so far. It generates one or more thoughts before generating\\nan action, which is then executed in the environment.[62] The linguistic\\ndescription of the environment given to the LLM planner can even be the LaTeX\\ncode of a paper describing the environment.[63]\\n\\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first\\nconnected to the visual world via image descriptions, then it is prompted to\\nproduce plans for complex tasks and behaviors based on its pretrained\\nknowledge and environmental feedback it receives.[64]\\n\\nThe Reflexion method[65] constructs an agent that learns over multiple\\nepisodes. At the end of each episode, the LLM is given the record of the\\nepisode, and prompted to think up \"lessons learned\", which would help it\\nperform better at a subsequent episode. These \"lessons learned\" are given to\\nthe agent in the subsequent episodes.[_citation needed_]\\n\\nMonte Carlo tree search can use an LLM as rollout heuristic. When a\\nprogrammatic world model is not available, an LLM can also be prompted with a\\ndescription of the environment to act as world model.[66]\\n\\nFor open-ended exploration, an LLM can be used to score observations for their\\n\"interestingness\", which can be used as a reward signal to guide a normal\\n(non-LLM) reinforcement learning agent.[67] Alternatively, it can propose\\nincreasingly difficult tasks for curriculum learning.[68] Instead of\\noutputting individual actions, an LLM planner can also construct \"skills\", or\\nfunctions for complex action sequences. The skills can be stored and later\\ninvoked, allowing increasing levels of abstraction in planning.[68]\\n\\nLLM-powered agents can keep a long-term memory of its previous contexts, and\\nthe memory can be retrieved in the same way as Retrieval Augmented Generation.\\nMultiple such agents can interact socially.[69]\\n\\n## Compression\\n\\n[edit]\\n\\nTypically, LLMs are trained with single- or half-precision floating point\\nnumbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one\\nbillion parameters require 2 gigabytes. The largest models typically have 100\\nbillion parameters, requiring 200 gigabytes to load, which places them outside\\nthe range of most consumer electronics.[70]\\n\\n_Post-trainingquantization_[71] aims to decrease the space requirement by\\nlowering precision of the parameters of a trained model, while preserving most\\nof its performance.[72][73] The simplest form of quantization simply truncates\\nall numbers to a given number of bits. It can be improved by using a different\\nquantization codebook per layer. Further improvement can be done by applying\\ndifferent precisions to different parameters, with higher precision for\\nparticularly important parameters (\"outlier weights\").[74] See the visual\\nguide to quantization by Maarten Grootendorst[75] for a visual depiction.\\n\\nWhile quantized models are typically frozen, and only pre-quantized models are\\nfine-tuned, quantized models can still be fine-tuned.[76]\\n\\n## Multimodality\\n\\n[edit]\\n\\nSee also: Multimodal learning\\n\\nMultimodality means \"having several modalities\", and a \"modality\" refers to a\\ntype of input or output, such as video, image, audio, text, proprioception,\\netc.[77] There have been many AI models trained specifically to ingest one\\nmodality and output another modality, such as AlexNet for image to label,[78]\\nvisual question answering for image-text to text,[79] and speech recognition\\nfor speech to text.\\n\\nA common method to create multimodal models out of an LLM is to \"tokenize\" the\\noutput of a trained encoder. Concretely, one can construct an LLM that can\\nunderstand images as follows: take a trained LLM, and take a trained image\\nencoder  E {\\\\displaystyle E} . Make a small multilayered perceptron  f\\n{\\\\displaystyle f} , so that for any image  y {\\\\displaystyle y} , the post-\\nprocessed vector  f ( E ( y ) ) {\\\\displaystyle f(E(y))} has the same\\ndimensions as an encoded token. That is an \"image token\". Then, one can\\ninterleave text tokens and image tokens. The compound model is then fine-tuned\\non an image-text dataset. This basic construction can be applied with more\\nsophistication to improve the model. The image encoder may be frozen to\\nimprove stability.[80]\\n\\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning\\na pair of pretrained language model and image encoder to perform better on\\nvisual question answering than models trained from scratch.[81] Google PaLM\\nmodel was fine-tuned into a multimodal model PaLM-E using the tokenization\\nmethod, and applied to robotic control.[82] LLaMA models have also been turned\\nmultimodal using the tokenization method, to allow image inputs,[83] and video\\ninputs.[84]\\n\\nGPT-4 can use both text and image as inputs[85] (although the vision component\\nwas not released to the public until GPT-4V[86]); Google DeepMind\\'s Gemini is\\nalso multimodal.[87] Mistral introduced its own multimodel Pixtral 12B model\\nin September 2024.[88]\\n\\n## Reasoning\\n\\n[edit]\\n\\nIn late 2024, a new direction emerged in LLM development with models\\nspecifically designed for complex reasoning tasks. These \"reasoning models\"\\nwere trained to spend more time generating step-by-step solutions before\\nproviding final answers, similar to human problem-solving processes.[89]\\nOpenAI introduced this trend with their o1 model in September 2024, followed\\nby o3 in December 2024. These models showed significant improvements in\\nmathematics, science, and coding tasks compared to traditional LLMs. For\\nexample, on International Mathematics Olympiad qualifying exam problems,\\nGPT-4o achieved 13% accuracy while o1 reached 83%.[89][90] In January 2025,\\nthe Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter\\nopen-weight reasoning model that achieved comparable performance to OpenAI\\'s\\no1 while being significantly more cost-effective to operate. Unlike\\nproprietary models from OpenAI, DeepSeek-R1\\'s open-weight nature allowed\\nresearchers to study and build upon the algorithm, though its training data\\nremained private.[91] These reasoning models typically require more\\ncomputational resources per query compared to traditional LLMs, as they\\nperform more extensive processing to work through problems step-by-step.\\nHowever, they have shown superior capabilities in domains requiring structured\\nlogical thinking, such as mathematics, scientific research, and computer\\nprogramming.[90]\\n\\nEfforts to reduce or compensate for hallucinations have employed automated\\nreasoning, RAG (retrieval-augmented generation), fine-tuning, and other\\nmethods.[92]\\n\\n## Properties\\n\\n[edit]\\n\\n### Scaling laws\\n\\n[edit]\\n\\nMain article: Neural scaling law\\n\\nThe performance of an LLM after pretraining largely depends on the:\\n\\n  * cost of pretraining  C {\\\\displaystyle C} (the total amount of compute used),\\n  * size of the artificial neural network itself, such as number of parameters  N {\\\\displaystyle N} (i.e. amount of neurons in its layers, amount of weights between them and biases),\\n  * size of its pretraining dataset (i.e. number of tokens in corpus,  D {\\\\displaystyle D} ).\\n\\n\"Scaling laws\" are empirical statistical laws that predict LLM performance\\nbased on such factors. One particular scaling law (\"Chinchilla scaling\") for\\nLLM autoregressively trained for one epoch, with a log-log learning rate\\nschedule, states that:[93] { C = C 0 N D L = A N α + B D β + L 0\\n{\\\\displaystyle {\\\\begin{cases}C=C_{0}ND\\\\\\\\\\\\\\\\[6pt]L={\\\\frac {A}{N^{\\\\alpha\\n}}}+{\\\\frac {B}{D^{\\\\beta }}}+L_{0}\\\\end{cases}}} where the variables are\\n\\n  * C {\\\\displaystyle C} is the cost of training the model, in FLOPs.\\n  * N {\\\\displaystyle N} is the number of parameters in the model.\\n  * D {\\\\displaystyle D} is the number of tokens in the training set.\\n  * L {\\\\displaystyle L} is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\\n\\nand the statistical hyper-parameters are\\n\\n  * C 0 = 6 {\\\\displaystyle C_{0}=6} , meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.[54]\\n  * α = 0.34 , β = 0.28 , A = 406.4 , B = 410.7 , L 0 = 1.69 {\\\\displaystyle \\\\alpha =0.34,\\\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\\n\\n### Emergent abilities\\n\\n[edit]\\n\\nAt point(s) referred to as breaks,[94] the lines change their slopes,\\nappearing on a linear-log plot as a series of linear segments connected by\\narcs.\\n\\nPerformance of bigger models on various tasks, when plotted on a log-log\\nscale, appears as a linear extrapolation of performance achieved by smaller\\nmodels. However, this linearity may be punctuated by \"break(s)\"[94] in the\\nscaling law, where the slope of the line changes abruptly, and where larger\\nmodels acquire \"emergent abilities\".[42][95] They arise from the complex\\ninteraction of the model\\'s components and are not explicitly programmed or\\ndesigned.[96]\\n\\nFurthermore, recent research has demonstrated that AI systems, including large\\nlanguage models, can employ heuristic reasoning akin to human cognition. They\\nbalance between exhaustive logical processing and the use of cognitive\\nshortcuts (heuristics), adapting their reasoning strategies to optimize\\nbetween accuracy and effort. This behavior aligns with principles of resource-\\nrational human cognition, as discussed in classical theories of bounded\\nrationality and dual-process theory.[97]\\n\\nOne of the emergent abilities is in-context learning from example\\ndemonstrations.[98] In-context learning is involved in tasks, such as:\\n\\n  * reported arithmetics\\n  * decoding the International Phonetic Alphabet\\n  * unscrambling a word\\'s letters\\n  * disambiguating word-in-context datasets[42][99][100]\\n  * converting spatial words\\n  * cardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.[101]\\n  * chain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.[102]\\n  * identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.[103]\\n\\nSchaeffer _et. al._ argue that the emergent abilities are not unpredictably\\nacquired, but predictably acquired according to a smooth scaling law. The\\nauthors considered a toy statistical model of an LLM solving multiple-choice\\nquestions, and showed that this statistical model, modified to account for\\nother types of tasks, applies to these tasks as well.[104]\\n\\nLet  x {\\\\displaystyle x} be the number of parameter count, and  y\\n{\\\\displaystyle y} be the performance of the model.\\n\\n  * When  y = average  Pr ( correct token ) {\\\\displaystyle y={\\\\text{average }}\\\\Pr({\\\\text{correct token}})} , then  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} is an exponential curve (before it hits the plateau at one), which looks like emergence.\\n  * When  y = average  log \\u2061 ( Pr ( correct token ) ) {\\\\displaystyle y={\\\\text{average }}\\\\log(\\\\Pr({\\\\text{correct token}}))} , then the  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} plot is a straight line (before it hits the plateau at zero), which does not look like emergence.\\n  * When  y = average  Pr ( the most likely token is correct ) {\\\\displaystyle y={\\\\text{average }}\\\\Pr({\\\\text{the most likely token is correct}})} , then  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} is a step-function, which looks like emergence.\\n\\n## Interpretation\\n\\n[edit]\\n\\nLarge language models by themselves are black boxes, and it is not clear how\\nthey can perform linguistic tasks. Similarly, it is unclear if or how LLMs\\nshould be viewed as models of the human brain and/or human mind.[105]\\n\\nThere are several methods for understanding how LLMs work. Mechanistic\\ninterpretability aims to reverse-engineer LLMs by discovering symbolic\\nalgorithms that approximate the inference performed by an LLM. One example is\\nOthello-GPT, where a small Transformer is trained to predict legal Othello\\nmoves. It is found that there is a linear representation of the Othello board,\\nand modifying the representation changes the predicted legal Othello moves in\\nthe correct way.[106][107] In another example, a small Transformer is trained\\non Karel programs. Similar to the Othello-GPT example, there is a linear\\nrepresentation of Karel program semantics, and modifying the representation\\nchanges output in the correct way. The model also generates correct programs\\nthat are on average shorter than those in the training set.[108]\\n\\nIn another example, the authors trained small transformers on modular\\narithmetic addition. The resulting models were reverse-engineered, and it\\nturned out they used discrete Fourier transform.[109]\\n\\nA related concept is AI explainability, which focuses on understanding how an\\nAI model arrives at a given result.\\n\\n### Understanding and intelligence\\n\\n[edit]\\n\\nSee also: Philosophy of artificial intelligence and Artificial consciousness\\n\\nNLP researchers were evenly split when asked, in a 2022 survey, whether\\n(untuned) LLMs \"could (ever) understand natural language in some nontrivial\\nsense\".[110] Proponents of \"LLM understanding\" believe that some LLM\\nabilities, such as mathematical reasoning, imply an ability to \"understand\"\\ncertain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel\\nand difficult tasks that span mathematics, coding, vision, medicine, law,\\npsychology and more\" and that GPT-4 \"could reasonably be viewed as an early\\n(yet still incomplete) version of an artificial general intelligence system\":\\n\"Can one reasonably say that a system that passes exams for software\\nengineering candidates is not _really_ intelligent?\"[111][112] Ilya Sutskever\\nargues that predicting the next word sometimes involves reasoning and deep\\ninsights, for example if the LLM has to predict the name of the criminal in an\\nunknown detective novel after processing the entire story leading up to the\\nrevelation.[113] Some researchers characterize LLMs as \"alien\\nintelligence\".[114][115] For example, Conjecture CEO Connor Leahy considers\\nuntuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF\\ntuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If\\nyou don\\'t push it too far, the smiley face stays on. But then you give it [an\\nunexpected] prompt, and suddenly you see this massive underbelly of insanity,\\nof weird thought processes and clearly non-human understanding.\"[116][117]\\n\\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are\\n\"simply remixing and recombining existing writing\",[115] a phenomenon known as\\nstochastic parrot, or they point to the deficits existing LLMs continue to\\nhave in prediction skills, reasoning skills, agency, and explainability.[110]\\nFor example, GPT-4 has natural deficits in planning and in real-time\\nlearning.[112] Generative LLMs have been observed to confidently assert claims\\nof fact which do not seem to be justified by their training data, a phenomenon\\nwhich has been termed \"hallucination\".[118] Specifically, hallucinations in\\nthe context of LLMs correspond to the generation of text or responses that\\nseem syntactically sound, fluent, and natural but are factually incorrect,\\nnonsensical, or unfaithful to the provided source input.[119] Neuroscientist\\nTerrence Sejnowski has argued that \"The diverging opinions of experts on the\\nintelligence of LLMs suggests that our old ideas based on natural intelligence\\nare inadequate\".[110]\\n\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main\\naspects – the first is how to model thought and language in a computer system,\\nand the second is how to enable the computer system to generate human like\\nlanguage.[110] These aspects of language as a model of cognition have been\\ndeveloped in the field of cognitive linguistics. American linguist George\\nLakoff presented Neural Theory of Language (NTL)[120] as a computational basis\\nfor using language as a model of learning tasks and understanding. The NTL\\nModel outlines how specific neural structures of the human brain shape the\\nnature of thought and language and in turn what are the computational\\nproperties of such neural systems that can be applied to model thought and\\nlanguage in a computer system. After a framework for modeling language in a\\ncomputer systems was established, the focus shifted to establishing frameworks\\nfor computer systems to generate language with acceptable grammar. In his 2014\\nbook titled _The Language Myth: Why Language Is Not An Instinct_ , British\\ncognitive linguist and digital communication technologist Vyvyan Evans mapped\\nout the role of probabilistic context-free grammar (PCFG) in enabling NLP to\\nmodel cognitive patterns and generate human like language.[121][122]\\n\\n## Evaluation\\n\\n[edit]\\n\\n### Perplexity\\n\\n[edit]\\n\\nThe canonical measure of the performance of an LLM is its perplexity on a\\ngiven text corpus. Perplexity measures how well a model predicts the contents\\nof a dataset; the higher the likelihood the model assigns to the dataset, the\\nlower the perplexity. In mathematical terms, perplexity is the exponential of\\nthe average negative log likelihood per token.\\n\\nlog \\u2061 ( Perplexity ) = − 1 N ∑ i = 1 N log \\u2061 ( Pr ( token i ∣ context for\\ntoken i ) ) {\\\\displaystyle \\\\log({\\\\text{Perplexity}})=-{\\\\frac {1}{N}}\\\\sum\\n_{i=1}^{N}\\\\log(\\\\Pr({\\\\text{token}}_{i}\\\\mid {\\\\text{context for token}}_{i}))}\\n\\nHere,  N {\\\\displaystyle N} is the number of tokens in the text corpus, and\\n\"context for token  i {\\\\displaystyle i} \" depends on the specific type of LLM.\\nIf the LLM is autoregressive, then \"context for token  i {\\\\displaystyle i} \"\\nis the segment of text appearing before token  i {\\\\displaystyle i} . If the\\nLLM is masked, then \"context for token  i {\\\\displaystyle i} \" is the segment\\nof text surrounding token  i {\\\\displaystyle i} .\\n\\nBecause language models may overfit to training data, models are usually\\nevaluated by their perplexity on a test set.[49] This evaluation is\\npotentially problematic for larger models which, as they are trained on\\nincreasingly large corpora of text, are increasingly likely to inadvertently\\ninclude portions of any given test set.[1]\\n\\n#### BPW, BPC, and BPT\\n\\n[edit]\\n\\nIn information theory, the concept of entropy is intricately linked to\\nperplexity, a relationship notably established by Claude Shannon.[123] This\\nrelationship is mathematically expressed as  Entropy = log 2 \\u2061 ( Perplexity )\\n{\\\\displaystyle {\\\\text{Entropy}}=\\\\log _{2}({\\\\text{Perplexity}})} .\\n\\nEntropy, in this context, is commonly quantified in terms of bits per word\\n(BPW) or bits per character (BPC), which hinges on whether the language model\\nutilizes word-based or character-based tokenization.\\n\\nNotably, in the case of larger language models that predominantly employ sub-\\nword tokenization, bits per token (BPT) emerges as a seemingly more\\nappropriate measure. However, due to the variance in tokenization methods\\nacross different Large Language Models (LLMs), BPT does not serve as a\\nreliable metric for comparative analysis among diverse models. To convert BPT\\ninto BPW, one can multiply it by the average number of tokens per word.\\n\\nIn the evaluation and comparison of language models, cross-entropy is\\ngenerally the preferred metric over entropy. The underlying principle is that\\na lower BPW is indicative of a model\\'s enhanced capability for compression.\\nThis, in turn, reflects the model\\'s proficiency in making accurate\\npredictions.\\n\\n### Task-specific datasets and benchmarks\\n\\n[edit]\\n\\nA large number of testing datasets and benchmarks have also been developed to\\nevaluate the capabilities of language models on more specific downstream\\ntasks. Tests may be designed to evaluate a variety of capabilities, including\\ngeneral knowledge, bias, commonsense reasoning, and mathematical problem-\\nsolving.\\n\\nOne broad category of evaluation dataset is question answering datasets,\\nconsisting of pairs of questions and correct answers, for example, (\"Have the\\nSan Jose Sharks won the Stanley Cup?\", \"No\").[124] A question answering task\\nis considered \"open book\" if the model\\'s prompt includes text from which the\\nexpected answer can be derived (for example, the previous question could be\\nadjoined with some text which includes the sentence \"The Sharks have advanced\\nto the Stanley Cup finals once, losing to the Pittsburgh Penguins in\\n2016.\"[124]). Otherwise, the task is considered \"closed book\", and the model\\nmust draw on knowledge retained during training.[125] Some examples of\\ncommonly used question answering datasets include TruthfulQA, Web Questions,\\nTriviaQA, and SQuAD.[125]\\n\\nEvaluation datasets may also take the form of text completion, having the\\nmodel select the most likely word or sentence to complete a prompt, for\\nexample: \"Alice was friends with Bob. Alice went to visit her friend,\\n____\".[1]\\n\\nSome composite benchmarks have also been developed which combine a diversity\\nof different evaluation datasets and tasks. Examples include GLUE, SuperGLUE,\\nMMLU, BIG-bench, HELM, and HLE (Humanity\\'s Last Exam).[123][125] OpenAI has\\nreleased tools for running composite benchmarks, but noted that the eval\\nresults are sensitive to the prompting method.[126][127] Some public datasets\\ncontain questions that are mislabeled, ambiguous, unanswerable, or otherwise\\nof low-quality, which can be cleaned to give more reliable benchmark\\nscores.[128]\\n\\nBias in LLMs may be measured through benchmarks such as CrowS-Pairs\\n(Crowdsourced Stereotype Pairs),[129] Stereo Set,[130] and the more recent\\nParity Benchmark.[131]\\n\\nIt was previously standard to report results on a heldout portion of an\\nevaluation dataset after doing supervised fine-tuning on the remainder. It is\\nnow more common to evaluate a pre-trained model directly through prompting\\ntechniques, though researchers vary in the details of how they formulate\\nprompts for particular tasks, particularly with respect to how many examples\\nof solved tasks are adjoined to the prompt (i.e. the value of _n_ in _n_ -shot\\nprompting).\\n\\n#### Adversarially constructed evaluations\\n\\n[edit]\\n\\nBecause of the rapid pace of improvement of large language models, evaluation\\nbenchmarks have suffered from short lifespans, with state of the art models\\nquickly \"saturating\" existing benchmarks, exceeding the performance of human\\nannotators, leading to efforts to replace or augment the benchmark with more\\nchallenging tasks.[132] In addition, there are cases of \"shortcut learning\"\\nwherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical\\ncorrelations in superficial test question wording in order to guess the\\ncorrect responses, without necessarily understanding the actual question being\\nasked.[110]\\n\\nSome datasets have been constructed adversarially, focusing on particular\\nproblems on which extant language models seem to have unusually poor\\nperformance compared to humans. One example is the TruthfulQA dataset, a\\nquestion answering dataset consisting of 817 questions which language models\\nare susceptible to answering incorrectly by mimicking falsehoods to which they\\nwere repeatedly exposed during training. For example, an LLM may answer \"No\"\\nto the question \"Can you teach an old dog new tricks?\" because of its exposure\\nto the English idiom _you can\\'t teach an old dog new tricks_ , even though\\nthis is not literally true.[133]\\n\\nAnother example of an adversarial evaluation dataset is Swag and its\\nsuccessor, HellaSwag, collections of problems in which one of multiple options\\nmust be selected to complete a text passage. The incorrect completions were\\ngenerated by sampling from a language model and filtering with a set of\\nclassifiers. The resulting problems are trivial for humans but at the time the\\ndatasets were created state of the art language models had poor accuracy on\\nthem. For example:\\n\\n> We see a fitness center sign. We then see a man talking to the camera and\\n> sitting and laying on a exercise ball. The man...  \\n> a) demonstrates how to increase efficient exercise work by running up and\\n> down balls.  \\n> b) moves all his arms and legs and builds up a lot of muscle.  \\n> c) then plays the ball and we see a graphics and hedge trimming\\n> demonstration.  \\n> d) performs sit ups while on the ball and talking.[134]\\n\\nBERT selects b) as the most likely completion, though the correct answer is\\nd).[134]\\n\\n#### Limitations of LLM benchmarks\\n\\n[edit]\\n\\nBenchmarks can become outdated rapidly. Once a model attains near-perfect\\nscores on a given benchmark, that benchmark ceases to serve as a meaningful\\nindicator of progress. This phenomenon, known as \"benchmark saturation,\"\\nnecessitates the development of more challenging and nuanced tasks to continue\\nadvancing LLM capabilities. For instance, traditional benchmarks like\\nHellaSwag and MMLU have seen models achieving high accuracy already.\\n\\n## Wider impact\\n\\n[edit]\\n\\nIn 2023, _Nature Biomedical Engineering_ wrote that \"it is no longer possible\\nto accurately distinguish\" human-written text from text created by large\\nlanguage models, and that \"It is all but certain that general-purpose large\\nlanguage models will rapidly proliferate... It is a rather safe bet that they\\nwill change many industries over time.\"[135] Goldman Sachs suggested in 2023\\nthat generative language AI could increase global GDP by 7% in the next ten\\nyears, and could expose to automation 300 million jobs globally.[136][137]\\n\\n### Memorization and copyright\\n\\n[edit]\\n\\nFurther information: Artificial intelligence and copyright\\n\\nMemorization is an emergent behavior in LLMs in which long strings of text are\\noccasionally output verbatim from training data, contrary to typical behavior\\nof traditional artificial neural nets. Evaluations of controlled LLM output\\nmeasure the amount memorized from training data (focused on GPT-2-series\\nmodels) as variously over 1% for exact duplicates[138] or up to about 7%.[139]\\n\\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the\\nsame word indefinitely, after a few hundreds of repetitions, it would start\\noutputting excerpts from its training data.[140]\\n\\n### Security\\n\\n[edit]\\n\\nSome commenters expressed concern over accidental or deliberate creation of\\nmisinformation, or other forms of misuse.[141] For example, the availability\\nof large language models could reduce the skill-level required to commit\\nbioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM\\ncreators should exclude from their training data papers on creating or\\nenhancing pathogens.[142]\\n\\nThe potential presence of \"sleeper agents\" within LLMs is another emerging\\nsecurity concern. These are hidden functionalities built into the model that\\nremain dormant until triggered by a specific event or condition. Upon\\nactivation, the LLM deviates from its expected behavior to make insecure\\nactions.[143]\\n\\nLLM applications accessible to the public, like ChatGPT or Claude, typically\\nincorporate safety measures designed to filter out harmful content. However,\\nimplementing these controls effectively has proven challenging. For instance,\\na 2023 study[144] proposed a method for circumventing LLM safety systems.\\nSimilarly, Yongge Wang[145] illustrated in 2024 how a potential criminal could\\npotentially bypass ChatGPT 4o\\'s safety controls to obtain information on\\nestablishing a drug trafficking operation.\\n\\n### Algorithmic bias\\n\\n[edit]\\n\\nMain article: Algorithmic bias\\n\\nWhile LLMs have shown remarkable capabilities in generating human-like text,\\nthey are susceptible to inheriting and amplifying biases present in their\\ntraining data. This can manifest in skewed representations or unfair treatment\\nof different demographics, such as those based on race, gender, language, and\\ncultural groups.[146] Since English data is overrepresented in current large\\nlanguage models\\' training data, it may also downplay non-English views.[147]\\n\\n#### Stereotyping\\n\\n[edit]\\n\\nAI models can reinforce a wide range of stereotypes, including those based on\\ngender, ethnicity, age, nationality, religion, or occupation. This can lead to\\noutputs that unfairly generalize or caricature groups of people, sometimes in\\nharmful or derogatory ways.[148]\\n\\nNotably, gender bias refers to the tendency of these models to produce outputs\\nthat are unfairly prejudiced towards one gender over another. This bias\\ntypically arises from the data on which these models are trained. Large\\nlanguage models often assign roles and characteristics based on traditional\\ngender norms.[146] For example, it might associate nurses or secretaries\\npredominantly with women and engineers or CEOs with men.[149]\\n\\n#### Selection bias\\n\\n[edit]\\n\\nSelection bias refers the inherent tendency of large language models to favor\\ncertain option identifiers irrespective of the actual content of the options.\\nThis bias primarily stems from token bias—that is, the model assigns a higher\\na priori probability to specific answer tokens (such as “A”) when generating\\nresponses. As a result, when the ordering of options is altered (for example,\\nby systematically moving the correct answer to different positions), the\\nmodel’s performance can fluctuate significantly. This phenomenon undermines\\nthe reliability of large language models in multiple-choice\\nsettings.[150][151]\\n\\n#### Political bias\\n\\n[edit]\\n\\nPolitical bias refers to the tendency of algorithms to systematically favor\\ncertain political viewpoints, ideologies, or outcomes over others. Language\\nmodels may also exhibit political biases. Since the training data includes a\\nwide range of political opinions and coverage, the models might generate\\nresponses that lean towards particular political ideologies or viewpoints,\\ndepending on the prevalence of those views in the data.[152]\\n\\n### Energy demands\\n\\n[edit]\\n\\nThe energy demands of LLMs have grown along with their size and capabilities.\\nData centers that enable LLM training require substantial amounts of\\nelectricity. Much of that electricity is generated by non-renewable resources\\nthat create greenhouse gases and contribute to climate change.[153] Nuclear\\npower and geothermal energy are two options tech companies are exploring to\\nmeet the sizable energy demands of LLM training.[154] The significant expense\\nof investing in geothermal solutions has led to major shale producers like\\nChevron and Exxon Mobil advocating for tech companies to use electricity\\nproduced via natural gas to fuel their large energy demands.[155]\\n\\n## See also\\n\\n[edit]\\n\\n  * Foundation models\\n  * List of large language models\\n  * List of chatbots\\n\\n## References\\n\\n[edit]\\n\\n  1. ^ _**a**_ _**b**_ _**c**_ Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). \"Language Models are Few-Shot Learners\" (PDF). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 1877–1901\\\\. Archived (PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\\n  2. **^** Fathallah, Nadeen; Das, Arunav; De Giorgis, Stefano; Poltronieri, Andrea; Haase, Peter; Kovriguina, Liubov (2024-05-26). _NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning_ (PDF). Extended Semantic Web Conference 2024. Hersonissos, Greece.\\n  3. **^** Manning, Christopher D. (2022). \"Human Language Understanding & Reasoning\". _Daedalus_. **151** (2): 127–138\\\\. doi:10.1162/daed_a_01905. S2CID 248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\n  4. **^** Goodman, Joshua (2001-08-09), _A Bit of Progress in Language Modeling_ , arXiv:cs/0108005, Bibcode:2001cs........8005G\\n  5. **^** Kilgarriff, Adam; Grefenstette, Gregory (September 2003). \"Introduction to the Special Issue on the Web as Corpus\". _Computational Linguistics_. **29** (3): 333–347\\\\. doi:10.1162/089120103322711569. ISSN 0891-2017.\\n  6. **^** Banko, Michele; Brill, Eric (2001). \"Scaling to very very large corpora for natural language disambiguation\". _Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL \\'01_. Morristown, NJ, USA: Association for Computational Linguistics: 26–33\\\\. doi:10.3115/1073012.1073017.\\n  7. **^** Resnik, Philip; Smith, Noah A. (September 2003). \"The Web as a Parallel Corpus\". _Computational Linguistics_. **29** (3): 349–380\\\\. doi:10.1162/089120103322711578. ISSN 0891-2017. Archived from the original on 2024-06-07. Retrieved 2024-06-07.\\n  8. **^** Halevy, Alon; Norvig, Peter; Pereira, Fernando (March 2009). \"The Unreasonable Effectiveness of Data\". _IEEE Intelligent Systems_. **24** (2): 8–12\\\\. doi:10.1109/MIS.2009.36. ISSN 1541-1672.\\n  9. **^** Chen, Leiyu; Li, Shaobo; Bai, Qiang; Yang, Jing; Jiang, Sanlong; Miao, Yanming (2021). \"Review of Image Classification Algorithms Based on Convolutional Neural Networks\". _Remote Sensing_. **13** (22): 4712. Bibcode:2021RemS...13.4712C. doi:10.3390/rs13224712.\\n  10. **^** Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). \"Attention is All you Need\" (PDF). _Advances in Neural Information Processing Systems_. **30**. Curran Associates, Inc. Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21.\\n  11. **^** Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n  12. **^** Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What We Know About How BERT Works\". _Transactions of the Association for Computational Linguistics_. **8** : 842–866\\\\. arXiv:2002.12327. doi:10.1162/tacl_a_00349. S2CID 211532403. Archived from the original on 2022-04-03. Retrieved 2024-01-21.\\n  13. **^** Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\". _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. pp. 1223–1243\\\\. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\n  14. **^** Hern, Alex (14 February 2019). \"New AI fake text generator may be too dangerous to release, say creators\". _The Guardian_. Archived from the original on 14 February 2019. Retrieved 20 January 2024.\\n  15. **^** \"ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months\". Euronews. November 30, 2023. Archived from the original on January 14, 2024. Retrieved January 20, 2024.\\n  16. **^** Heaven, Will (March 14, 2023). \"GPT-4 is bigger and better than ChatGPT—but OpenAI won\\'t say why\". MIT Technology Review. Archived from the original on March 17, 2023. Retrieved January 20, 2024.\\n  17. **^** Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\". _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. pp. 1223–1243\\\\. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\n  18. **^** \"Parameters in notable artificial intelligence systems\". _ourworldindata.org_. November 30, 2023. Retrieved January 20, 2024.\\n  19. **^** Sharma, Shubham (2025-01-20). \"Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95% less cost\". _VentureBeat_. Retrieved 2025-01-26.\\n  20. **^** Zia, Dr Tehseen (2024-01-08). \"Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024\". _Unite.AI_. Retrieved 2024-12-28.\\n  21. **^** Peng, Bo; et al. (2023). \"RWKV: Reinventing RNNS for the Transformer Era\". arXiv:2305.13048 [cs.CL].\\n  22. **^** Merritt, Rick (2022-03-25). \"What Is a Transformer Model?\". _NVIDIA Blog_. Archived from the original on 2023-11-17. Retrieved 2023-07-25.\\n  23. **^** Gu, Albert; Dao, Tri (2023-12-01), _Mamba: Linear-Time Sequence Modeling with Selective State Spaces_ , arXiv:2312.00752\\n  24. **^** Kaushal, Ayush; Mahowald, Kyle (2022-06-06), _What do tokens know about their characters and how do they know it?_ , arXiv:2206.02608\\n  25. **^** Yennie Jun (2023-05-03). \"All languages are NOT created (tokenized) equal\". _Language models cost much more in some languages than others_. Archived from the original on 2023-08-17. Retrieved 2023-08-17. \"In other words, to express the same sentiment, some languages require up to 10 times more tokens.\"\\n  26. **^** Petrov, Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). \"Language Model Tokenizers Introduce Unfairness Between Languages\". _NeurIPS_. arXiv:2305.15425. Archived from the original on December 15, 2023. Retrieved September 16, 2023 – via openreview.net.\\n  27. **^** \"OpenAI API\". _platform.openai.com_. Archived from the original on April 23, 2023. Retrieved 2023-04-30.\\n  28. ^ _**a**_ _**b**_ Paaß, Gerhard; Giesselbach, Sven (2022). \"Pre-trained Language Models\". _Foundation Models for Natural Language Processing_. Artificial Intelligence: Foundations, Theory, and Algorithms. pp. 19–78\\\\. doi:10.1007/978-3-031-23190-2_2. ISBN 9783031231902. Archived from the original on 3 August 2023. Retrieved 3 August 2023.\\n  29. **^** Petrov, Aleksandar; Emanuele La Malfa; Torr, Philip H. S.; Bibi, Adel (2023). \"Language Model Tokenizers Introduce Unfairness Between Languages\". arXiv:2305.15425 [cs.CL].\\n  30. **^** Lundberg, Scott (2023-12-12). \"The Art of Prompt Design: Prompt Boundaries and Token Healing\". _Medium_. Retrieved 2024-08-05.\\n  31. **^** Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt (2021). \"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\". arXiv:2104.08758 [cs.CL].\\n  32. **^** Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; Carlini, Nicholas (May 2022). \"Deduplicating Training Data Makes Language Models Better\" (PDF). _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_. 1: Long Papers: 8424–8445\\\\. doi:10.18653/v1/2022.acl-long.577.\\n  33. **^** Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar, Suriya; Lee, Yin Tat (2023-09-11), _Textbooks Are All You Need II: phi-1.5 technical report_ , arXiv:2309.05463\\n  34. **^** Lin, Zhenghao; Gou, Zhibin; Gong, Yeyun; Liu, Xiao; Shen, Yelong; Xu, Ruochen; Lin, Chen; Yang, Yujiu; Jiao, Jian (2024-04-11). \"Rho-1: Not All Tokens Are What You Need\". arXiv:2404.07965 [cs.CL].\\n  35. **^** Brown, Tom B.; et al. (2020). \"Language Models are Few-Shot Learners\". arXiv:2005.14165 [cs.CL].\\n  36. **^** Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash (2024-04-23). \"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\". arXiv:2404.14219 [cs.CL].\\n  37. **^** Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \"Training language models to follow instructions with human feedback\". arXiv:2203.02155 [cs.CL].\\n  38. **^** Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Model with Self Generated Instructions\". arXiv:2212.10560 [cs.CL].\\n  39. **^** Shazeer, Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geoffrey; Dean, Jeff (2017-01-01). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\". arXiv:1701.06538 [cs.LG].\\n  40. **^** Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng (2021-01-12). \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\". arXiv:2006.16668 [cs.CL].\\n  41. **^** Dai, Andrew M; Du, Nan (December 9, 2021). \"More Efficient In-Context Learning with GLaM\". _ai.googleblog.com_. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\n  42. ^ _**a**_ _**b**_ _**c**_ Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). \"Emergent Abilities of Large Language Models\". _Transactions on Machine Learning Research_. ISSN 2835-8856. Archived from the original on 22 March 2023. Retrieved 19 March 2023.\\n  43. **^** Allamar, Jay. \"Illustrated transformer\". Archived from the original on 2023-07-25. Retrieved 2023-07-29.\\n  44. **^** Allamar, Jay. \"The Illustrated GPT-2 (Visualizing Transformer Language Models)\". Retrieved 2023-08-01.\\n  45. **^** \"Our next-generation model: Gemini 1.5\". _Google_. 15 February 2024. Archived from the original on 18 February 2024. Retrieved 18 February 2024.\\n  46. **^** \"Long context prompting for Claude 2.1\". December 6, 2023. Archived from the original on August 27, 2024. Retrieved January 20, 2024.\\n  47. **^** \"Rate limits\". _openai.com_. Archived from the original on February 2, 2024. Retrieved January 20, 2024.\\n  48. **^** Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). \"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\". _Proceedings of the Australasian Computer Science Week Multiconference_. pp. 1–4\\\\. arXiv:2104.10810. doi:10.1145/3373017.3373028. ISBN 9781450376976. S2CID 211040895.\\n  49. ^ _**a**_ _**b**_ _**c**_ Jurafsky, Dan; Martin, James H. (7 January 2023). _Speech and Language Processing_ (PDF) (3rd edition draft ed.). Archived (PDF) from the original on 23 March 2023. Retrieved 24 May 2022.\\n  50. **^** \"From bare metal to a 70B model: infrastructure set-up and scripts\". _imbue.com_. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\n  51. **^** \"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\". _GitHub_. Archived from the original on 2024-01-24. Retrieved 2024-07-24.\\n  52. **^** Albrecht, Josh (2024-07-23). \"State of the Art: Training >70B LLMs on 10,000 H100 clusters\". _www.latent.space_. Retrieved 2024-07-24.\\n  53. **^** Maslej, Nestor; Fattorini, Loredana; Brynjolfsson, Erik; Etchemendy, John; Ligett, Katrina; Lyons, Terah; Manyika, James; Ngo, Helen; Niebles, Juan Carlos (2023-10-05), _Artificial Intelligence Index Report 2023_ , arXiv:2310.03715\\n  54. ^ _**a**_ _**b**_ Section 2.1 and Table 1, Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \"Scaling Laws for Neural Language Models\". arXiv:2001.08361 [cs.LG].\\n  55. **^** Gao, Luyu; Madaan, Aman; Zhou, Shuyan; Alon, Uri; Liu, Pengfei; Yang, Yiming; Callan, Jamie; Neubig, Graham (2022-11-01). \"PAL: Program-aided Language Models\". arXiv:2211.10435 [cs.CL].\\n  56. **^** \"PAL: Program-aided Language Models\". _reasonwithpal.com_. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\n  57. **^** Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer, Luke; Tulio Ribeiro, Marco (2023-03-01). \"ART: Automatic multi-step reasoning and tool-use for large language models\". arXiv:2303.09014 [cs.CL].\\n  58. **^** Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\". arXiv:2303.16434 [cs.AI].\\n  59. **^** Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). \"Gorilla: Large Language Model Connected with Massive APIs\". arXiv:2305.15334 [cs.CL].\\n  60. **^** Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\". _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 9459–9474\\\\. arXiv:2005.11401. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\n  61. **^** \"The Growth Behind LLM-based Autonomous Agents\". _KDnuggets_. October 23, 2023.\\n  62. **^** Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \"ReAct: Synergizing Reasoning and Acting in Language Models\". arXiv:2210.03629 [cs.CL].\\n  63. **^** Wu, Yue; Prabhumoye, Shrimai; Min, So Yeon (24 May 2023). \"SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning\". arXiv:2305.15486 [cs.AI].\\n  64. **^** Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\". arXiv:2302.01560 [cs.AI].\\n  65. **^** Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01). \"Reflexion: Language Agents with Verbal Reinforcement Learning\". arXiv:2303.11366 [cs.AI].\\n  66. **^** Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; Wang, Zhen; Zhe Wang, Daisy; Hu, Zhiting (2023-05-01). \"Reasoning with Language Model is Planning with World Model\". arXiv:2305.14992 [cs.CL].\\n  67. **^** Zhang, Jenny; Lehman, Joel; Stanley, Kenneth; Clune, Jeff (2 June 2023). \"OMNI: Open-endedness via Models of human Notions of Interestingness\". arXiv:2306.01711 [cs.AI].\\n  68. ^ _**a**_ _**b**_ \"Voyager | An Open-Ended Embodied Agent with Large Language Models\". _voyager.minedojo.org_. Archived from the original on 2023-06-08. Retrieved 2023-06-09.\\n  69. **^** Park, Joon Sung; O\\'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-04-01). \"Generative Agents: Interactive Simulacra of Human Behavior\". arXiv:2304.03442 [cs.HC].\\n  70. **^** Mann, Tobias. \"How to run an LLM locally on your PC in less than 10 minutes\". _www.theregister.com_. Retrieved 2024-05-17.\\n  71. **^** Nagel, Markus; Amjad, Rana Ali; Baalen, Mart Van; Louizos, Christos; Blankevoort, Tijmen (2020-11-21). \"Up or Down? Adaptive Rounding for Post-Training Quantization\". _Proceedings of the 37th International Conference on Machine Learning_. PMLR: 7197–7206\\\\. Archived from the original on 2023-06-14. Retrieved 2023-06-14.\\n  72. **^** Polino, Antonio; Pascanu, Razvan; Alistarh, Dan (2018-02-01). \"Model compression via distillation and quantization\". arXiv:1802.05668 [cs.NE].\\n  73. **^** Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan (2022-10-01). \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\". arXiv:2210.17323 [cs.LG].\\n  74. **^** Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, Vage; Kuznedelev, Denis; Frantar, Elias; Ashkboos, Saleh; Borzunov, Alexander; Hoefler, Torsten; Alistarh, Dan (2023-06-01). \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\". arXiv:2306.03078 [cs.CL].\\n  75. **^** Grootendorst, Maarten. \"A Visual Guide to Quantization\". _newsletter.maartengrootendorst.com_. Archived from the original on 31 Jul 2024. Retrieved 2024-07-31.\\n  76. **^** Dettmers, Tim; Pagnoni, Artidoro; Holtzman, Ari; Zettlemoyer, Luke (2023-05-01). \"QLoRA: Efficient Finetuning of Quantized LLMs\". arXiv:2305.14314 [cs.LG].\\n  77. **^** Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). \"Multimodal Neural Language Models\". _Proceedings of the 31st International Conference on Machine Learning_. PMLR: 595–603\\\\. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  78. **^** Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E (2012). \"ImageNet Classification with Deep Convolutional Neural Networks\". _Advances in Neural Information Processing Systems_. **25**. Curran Associates, Inc. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  79. **^** Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi (2015). \"VQA: Visual Question Answering\". _ICCV_ : 2425–2433\\\\. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  80. **^** Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\". arXiv:2301.12597 [cs.CV].\\n  81. **^** Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). \"Flamingo: a Visual Language Model for Few-Shot Learning\". _Advances in Neural Information Processing Systems_. **35** : 23716–23736\\\\. arXiv:2204.14198. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  82. **^** Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). \"PaLM-E: An Embodied Multimodal Language Model\". arXiv:2303.03378 [cs.LG].\\n  83. **^** Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \"Visual Instruction Tuning\". arXiv:2304.08485 [cs.CV].\\n  84. **^** Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\". arXiv:2306.02858 [cs.CL].\\n  85. **^** OpenAI (2023-03-27). \"GPT-4 Technical Report\". arXiv:2303.08774 [cs.CL].\\n  86. **^** OpenAI (September 25, 2023). \"GPT-4V(ision) System Card\" (PDF).\\n  87. **^** Pichai, Sundar (10 May 2023), _Google Keynote (Google I/O \\'23)_, timestamp 15:31, retrieved 2023-07-02\\n  88. **^** Wiggers, Kyle (11 September 2024). \"Mistral releases Pixtral 12B, its first multimodal model\". _TechCrunch_. Retrieved 14 September 2024.\\n  89. ^ _**a**_ _**b**_ \"Introducing OpenAI o1-preview\". _OpenAI_. 2024-09-12. Retrieved 2025-02-03.\\n  90. ^ _**a**_ _**b**_ Metz, Cade (2024-12-20). \"OpenAI Unveils New A.I. That Can \\'Reason\\' Through Math and Science Problems\". _The New York Times_. Retrieved 2025-02-03.\\n  91. **^** Gibney, Elizabeth (2025-01-30). \"China\\'s cheap, open AI model DeepSeek thrills scientists\". _Nature_. Retrieved 2025-02-03.\\n  92. **^** Lin, Belle (2025-02-05). \"Why Amazon is Betting on \\'Automated Reasoning\\' to Reduce AI\\'s Hallucinations: The tech giant says an obscure field that combines AI and math can mitigate—but not completely eliminate—AI\\'s propensity to provide wrong answers\". _Wall Street Journal_. ISSN 0099-9660.\\n  93. **^** Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \"Training Compute-Optimal Large Language Models\". arXiv:2203.15556 [cs.CL].\\n  94. ^ _**a**_ _**b**_ Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \"Broken Neural Scaling Laws\". arXiv:2210.14891 [cs.LG].\\n  95. **^** \"137 emergent abilities of large language models\". _Jason Wei_. Retrieved 2023-06-24.\\n  96. **^** Bowman, Samuel R. (2023). \"Eight Things to Know about Large Language Models\". arXiv:2304.00612 [cs.CL].\\n  97. **^** Mukherjee, Anirban; Chang, Hannah (2024). \"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption\". arXiv:2403.09404 [cs.AI].\\n  98. **^** Hahn, Michael; Goyal, Navin (2023-03-14). \"A Theory of Emergent In-Context Learning as Implicit Structure Induction\". arXiv:2303.07971 [cs.LG].\\n  99. **^** Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). \"Proceedings of the 2019 Conference of the North\". _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_. Minneapolis, Minnesota: Association for Computational Linguistics: 1267–1273\\\\. doi:10.18653/v1/N19-1128. S2CID 102353817. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\n  100. **^** \"WiC: The Word-in-Context Dataset\". _pilehvar.github.io_. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\n  101. **^** Patel, Roma; Pavlick, Ellie (2021-10-06). \"Mapping Language Models to Grounded Conceptual Spaces\". _ICLR_. Archived from the original on 2023-06-24. Retrieved 2023-06-27.\\n  102. **^** _A Closer Look at Large Language Models Emergent Abilities Archived 2023-06-24 at the Wayback Machine_ (Yao Fu, Nov 20, 2022)\\n  103. **^** Ornes, Stephen (March 16, 2023). \"The Unpredictable Abilities Emerging From Large AI Models\". _Quanta Magazine_. Archived from the original on March 16, 2023. Retrieved March 16, 2023.\\n  104. **^** Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \"Are Emergent Abilities of Large Language Models a Mirage?\". arXiv:2304.15004 [cs.AI].\\n  105. **^** Blank, Idan A. (November 2023). \"What are large language models supposed to model?\". _Trends in Cognitive Sciences_. **27** (11): 987–989\\\\. doi:10.1016/j.tics.2023.08.006. PMID 37659920.\\n  106. **^** Li, Kenneth; Hopkins, Aspen K.; Bau, David; Viégas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin (2022-10-01). \"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\". arXiv:2210.13382 [cs.LG].\\n  107. **^** \"Large Language Model: world models or surface statistics?\". _The Gradient_. 2023-01-21. Retrieved 2023-06-12.\\n  108. **^** Jin, Charles; Rinard, Martin (2023-05-01). \"Evidence of Meaning in Language Models Trained on Programs\". arXiv:2305.11169 [cs.LG].\\n  109. **^** Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \"Progress measures for grokking via mechanistic interpretability\". arXiv:2301.05217 [cs.LG].\\n  110. ^ _**a**_ _**b**_ _**c**_ _**d**_ _**e**_ Mitchell, Melanie; Krakauer, David C. (28 March 2023). \"The debate over understanding in AI\\'s large language models\". _Proceedings of the National Academy of Sciences_. **120** (13): e2215907120. arXiv:2210.13966. Bibcode:2023PNAS..12015907M. doi:10.1073/pnas.2215907120. PMC 10068812. PMID 36943882.\\n  111. **^** Metz, Cade (16 May 2023). \"Microsoft Says New A.I. Shows Signs of Human Reasoning\". _The New York Times_.\\n  112. ^ _**a**_ _**b**_ Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\". arXiv:2303.12712 [cs.CL].\\n  113. **^** \"Anthropic CEO Dario Amodei pens a smart look at our AI future\". _Fast Company_. October 17, 2024.\\n  114. **^** \"ChatGPT is more like an \\'alien intelligence\\' than a human brain, says futurist\". _ZDNET_. 2023. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\n  115. ^ _**a**_ _**b**_ Newport, Cal (13 April 2023). \"What Kind of Mind Does ChatGPT Have?\". _The New Yorker_. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\n  116. **^** Roose, Kevin (30 May 2023). \"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\" _The New York Times_. Archived from the original on 30 May 2023. Retrieved 12 June 2023.\\n  117. **^** \"The A to Z of Artificial Intelligence\". _Time Magazine_. 13 April 2023. Archived from the original on 16 June 2023. Retrieved 12 June 2023.\\n  118. **^** Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). \"Survey of Hallucination in Natural Language Generation\" (pdf). _ACM Computing Surveys_. **55** (12). Association for Computing Machinery: 1–38\\\\. arXiv:2202.03629. doi:10.1145/3571730. S2CID 246652372. Archived from the original on 26 March 2023. Retrieved 15 January 2023.\\n  119. **^** Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). \"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation\". arXiv:2307.03987 [cs.CL].\\n  120. **^** Lakoff, George (1999). _Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm_. New York Basic Books. pp. 569–583\\\\. ISBN 978-0-465-05674-3.\\n  121. **^** Evans, Vyvyan. (2014). _The Language Myth_. Cambridge University Press. ISBN 978-1-107-04396-1.\\n  122. **^** Friston, Karl J. (2022). _Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference_. The MIT Press. ISBN 978-0-262-36997-8.\\n  123. ^ _**a**_ _**b**_ Huyen, Chip (October 18, 2019). \"Evaluation Metrics for Language Modeling\". _The Gradient_. Retrieved January 14, 2024.\\n  124. ^ _**a**_ _**b**_ Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). \"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\". arXiv:1905.10044 [cs.CL].\\n  125. ^ _**a**_ _**b**_ _**c**_ Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; Wang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang, Junjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang, Xinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Yun; Wen, Ji-Rong (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\\n  126. **^** _openai/simple-evals_, OpenAI, 2024-05-28, retrieved 2024-05-28\\n  127. **^** _openai/evals_, OpenAI, 2024-05-28, archived from the original on 2024-05-08, retrieved 2024-05-28\\n  128. **^** \"Sanitized open-source datasets for natural language and code understanding: how we evaluated our 70B model\". _imbue.com_. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\n  129. **^** Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R. (November 2020). \"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models\". In Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang (ed.). _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics. pp. 1953–1967\\\\. arXiv:2010.00133. doi:10.18653/v1/2020.emnlp-main.154.`{{cite conference}}`: CS1 maint: multiple names: authors list (link)\\n  130. **^** Nadeem, Moin and Bethke, Anna and Reddy, Siva (August 2021). \"StereoSet: Measuring stereotypical bias in pretrained language models\". In Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto (ed.). _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Association for Computational Linguistics. pp. 5356–5371\\\\. arXiv:2004.09456. doi:10.18653/v1/2021.acl-long.416.`{{cite conference}}`: CS1 maint: multiple names: authors list (link)\\n  131. **^** Simpson, Shmona and Nukpezah, Jonathan and Kie Brooks and Pandya, Raaghav (17 December 2024). \"Parity benchmark for measuring bias in LLMs\". _AI and Ethics_. Springer. doi:10.1007/s43681-024-00613-4.`{{cite journal}}`: CS1 maint: multiple names: authors list (link)\\n  132. **^** Srivastava, Aarohi; et al. (2022). \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\". arXiv:2206.04615 [cs.CL].\\n  133. **^** Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\". arXiv:2109.07958 [cs.CL].\\n  134. ^ _**a**_ _**b**_ Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \"HellaSwag: Can a Machine Really Finish Your Sentence?\". arXiv:1905.07830 [cs.CL].\\n  135. **^** \"Prepare for truly useful large language models\". _Nature Biomedical Engineering_. **7** (2): 85–86\\\\. 7 March 2023. doi:10.1038/s41551-023-01012-6. PMID 36882584. S2CID 257403466.\\n  136. **^** \"Your job is (probably) safe from artificial intelligence\". _The Economist_. 7 May 2023. Archived from the original on 17 June 2023. Retrieved 18 June 2023.\\n  137. **^** \"Generative AI Could Raise Global GDP by 7%\". _Goldman Sachs_. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\n  138. **^** Peng, Zhencan; Wang, Zhizhi; Deng, Dong (13 June 2023). \"Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation\" (PDF). _Proceedings of the ACM on Management of Data_. **1** (2): 1–18\\\\. doi:10.1145/3589324. S2CID 259213212. Archived (PDF) from the original on 2024-08-27. Retrieved 2024-01-20. Citing Lee et al 2022.\\n  139. **^** Peng, Wang & Deng 2023, p. 8.\\n  140. **^** Stephen Council (1 Dec 2023). \"How Googlers cracked an SF rival\\'s tech model with a single word\". SFGATE. Archived from the original on 16 December 2023.\\n  141. **^** Alba, Davey (1 May 2023). \"AI chatbots have been used to create dozens of news content farms\". _The Japan Times_. Retrieved 18 June 2023.\\n  142. **^** \"Could chatbots help devise the next pandemic virus?\". _Science_. 14 June 2023. doi:10.1126/science.adj2463. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\n  143. **^** Hubinger, Evan (10 January 2024). \"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\". arXiv:2401.05566 [cs.CR].\\n  144. **^** Kang, Daniel (2023). \"Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks\". arXiv:2302.05733 [cs.CR].\\n  145. **^** Wang, Yongge (20 June 2024). \"Encryption Based Covert Channel for Large Language Models\" (PDF). IACR ePrint 2024/586. Archived (PDF) from the original on 24 June 2024. Retrieved 24 June 2024.\\n  146. ^ _**a**_ _**b**_ Stokel-Walker, Chris (November 22, 2023). \"ChatGPT Replicates Gender Bias in Recommendation Letters\". _Scientific American_. Archived from the original on 2023-12-29. Retrieved 2023-12-29.\\n  147. **^** Luo, Queenie; Puett, Michael J.; Smith, Michael D. (2023-03-28). \"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\". arXiv:2303.16281v2 [cs.CY].\\n  148. **^** Cheng, Myra; Durmus, Esin; Jurafsky, Dan (2023-05-29), _Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models_ , arXiv:2305.18189\\n  149. **^** Kotek, Hadas; Dockum, Rikker; Sun, David (2023-11-05). \"Gender bias and stereotypes in Large Language Models\". _Proceedings of the ACM Collective Intelligence Conference_. CI \\'23. New York, NY, USA: Association for Computing Machinery. pp. 12–24\\\\. doi:10.1145/3582269.3615599. ISBN 979-8-4007-0113-9.\\n  150. **^** Choi, Hyeong Kyu; Xu, Weijie; Xue, Chi; Eckman, Stephanie; Reddy, Chandan K. (2024-09-27), _Mitigating Selection Bias with Node Pruning and Auxiliary Options_ , arXiv:2409.18857\\n  151. **^** Zheng, Chujie; Zhou, Hao; Meng, Fandong; Zhou, Jie; Huang, Minlie (2023-09-07), _Large Language Models Are Not Robust Multiple Choice Selectors_ , arXiv:2309.03882\\n  152. **^** Heikkilä, Melissa (August 7, 2023). \"AI language models are rife with different political biases\". _MIT Technology Review_. Retrieved 2023-12-29.\\n  153. **^** Mehta, Sourabh (2024-07-03). \"How Much Energy Do LLMs Consume? Unveiling the Power Behind AI\". _Association of Data Scientists_. Retrieved 2025-01-27.\\n  154. **^** \"Artificial Intelligence wants to go nuclear. Will it work?\". _NPR_. Retrieved 2025-01-27.\\n  155. **^** Roy, Dareen (December 19, 2024). \"AI\\'s energy hunger fuels geothermal startups but natgas rivalry clouds future\". _Reuters_.\\n\\n  \\n\\n## Further reading\\n\\n[edit]\\n\\n  * Jurafsky, Dan, Martin, James. H. _Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition_, 3rd Edition draft, 2023.\\n  * Zhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\\n  * Kaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\\n  * Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". _National Science Review_. **11** (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.\\n  * \"AI Index Report 2024 – Artificial Intelligence Index\". _aiindex.stanford.edu_. Retrieved 2024-05-05.\\n  * Frank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". _Nature Reviews Psychology_. **2** (8): 451–452\\\\. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\\n\\n  * v\\n  * t\\n  * e\\n\\nNatural language processing  \\n---  \\nGeneral terms|\\n\\n  * AI-complete\\n  * Bag-of-words\\n  * n-gram\\n    * Bigram\\n    * Trigram\\n  * Computational linguistics\\n  * Natural language understanding\\n  * Stop words\\n  * Text processing\\n\\n  \\nText analysis|\\n\\n  * Argument mining\\n  * Collocation extraction\\n  * Concept mining\\n  * Coreference resolution\\n  * Deep linguistic processing\\n  * Distant reading\\n  * Information extraction\\n  * Named-entity recognition\\n  * Ontology learning\\n  * Parsing\\n    * Semantic parsing\\n    * Syntactic parsing\\n  * Part-of-speech tagging\\n  * Semantic analysis\\n  * Semantic role labeling\\n  * Semantic decomposition\\n  * Semantic similarity\\n  * Sentiment analysis\\n\\n  * Terminology extraction\\n  * Text mining\\n  * Textual entailment\\n  * Truecasing\\n  * Word-sense disambiguation\\n  * Word-sense induction\\n\\n| Text segmentation|\\n\\n  * Compound-term processing\\n  * Lemmatisation\\n  * Lexical analysis\\n  * Text chunking\\n  * Stemming\\n  * Sentence segmentation\\n  * Word segmentation\\n\\n  \\n---|---  \\n  \\nAutomatic summarization|\\n\\n  * Multi-document summarization\\n  * Sentence extraction\\n  * Text simplification\\n\\n  \\nMachine translation|\\n\\n  * Computer-assisted\\n  * Example-based\\n  * Rule-based\\n  * Statistical\\n  * Transfer-based\\n  * Neural\\n\\n  \\nDistributional semantics models|\\n\\n  * BERT\\n  * Document-term matrix\\n  * Explicit semantic analysis\\n  * fastText\\n  * GloVe\\n  * Language model (large)\\n  * Latent semantic analysis\\n  * Seq2seq\\n  * Word embedding\\n  * Word2vec\\n\\n  \\nLanguage resources,  \\ndatasets and corpora| | Types and  \\nstandards|\\n\\n  * Corpus linguistics\\n  * Lexical resource\\n  * Linguistic Linked Open Data\\n  * Machine-readable dictionary\\n  * Parallel text\\n  * PropBank\\n  * Semantic network\\n  * Simple Knowledge Organization System\\n  * Speech corpus\\n  * Text corpus\\n  * Thesaurus (information retrieval)\\n  * Treebank\\n  * Universal Dependencies\\n\\n  \\n---|---  \\nData|\\n\\n  * BabelNet\\n  * Bank of English\\n  * DBpedia\\n  * FrameNet\\n  * Google Ngram Viewer\\n  * UBY\\n  * WordNet\\n  * Wikidata\\n\\n  \\n  \\nAutomatic identification  \\nand data capture|\\n\\n  * Speech recognition\\n  * Speech segmentation\\n  * Speech synthesis\\n  * Natural language generation\\n  * Optical character recognition\\n\\n  \\nTopic model|\\n\\n  * Document classification\\n  * Latent Dirichlet allocation\\n  * Pachinko allocation\\n\\n  \\nComputer-assisted  \\nreviewing|\\n\\n  * Automated essay scoring\\n  * Concordancer\\n  * Grammar checker\\n  * Predictive text\\n  * Pronunciation assessment\\n  * Spell checker\\n\\n  \\nNatural language  \\nuser interface|\\n\\n  * Chatbot\\n  * Interactive fiction (c.f. Syntax guessing)\\n  * Question answering\\n  * Virtual assistant\\n  * Voice user interface\\n\\n  \\nRelated|\\n\\n  * Formal semantics\\n  * Hallucination\\n  * Natural Language Toolkit\\n  * spaCy\\n\\n  \\n  \\n  * v\\n  * t\\n  * e\\n\\nArtificial intelligence (AI)  \\n---  \\nHistory (timeline)  \\nConcepts|\\n\\n  * Parameter\\n    * Hyperparameter\\n  * Loss functions\\n  * Regression\\n    * Bias–variance tradeoff\\n    * Double descent\\n    * Overfitting\\n  * Clustering\\n  * Gradient descent\\n    * SGD\\n    * Quasi-Newton method\\n    * Conjugate gradient method\\n  * Backpropagation\\n  * Attention\\n  * Convolution\\n  * Normalization\\n    * Batchnorm\\n  * Activation\\n    * Softmax\\n    * Sigmoid\\n    * Rectifier\\n  * Gating\\n  * Weight initialization\\n  * Regularization\\n  * Datasets\\n    * Augmentation\\n  * Prompt engineering\\n  * Reinforcement learning\\n    * Q-learning\\n    * SARSA\\n    * Imitation\\n    * Policy gradient\\n  * Diffusion\\n  * Latent diffusion model\\n  * Autoregression\\n  * Adversary\\n  * RAG\\n  * Uncanny valley\\n  * RLHF\\n  * Self-supervised learning\\n  * Recursive self-improvement\\n  * Word embedding\\n  * Hallucination\\n\\n  \\nApplications|\\n\\n  * Machine learning\\n    * In-context learning\\n  * Artificial neural network\\n    * Deep learning\\n  * Language model\\n    * Large language model\\n    * NMT\\n  * Artificial general intelligence\\n\\n  \\nImplementations| | Audio–visual| \\n\\n  * AlexNet\\n  * WaveNet\\n  * Human image synthesis\\n  * HWR\\n  * OCR\\n  * Speech synthesis\\n    * 15.ai\\n    * ElevenLabs\\n  * Speech recognition\\n    * Whisper\\n  * Facial recognition\\n  * AlphaFold\\n  * Text-to-image models\\n    * Aurora\\n    * DALL-E\\n    * Firefly\\n    * Flux\\n    * Ideogram\\n    * Imagen\\n    * Midjourney\\n    * Stable Diffusion\\n  * Text-to-video models\\n    * Dream Machine\\n    * Gen-3 Alpha\\n    * Hailuo AI\\n    * Kling\\n    * Sora\\n    * Veo\\n  * Music generation\\n    * Suno AI\\n    * Udio\\n\\n  \\n---|---  \\nText|\\n\\n  * Word2vec\\n  * Seq2seq\\n  * GloVe\\n  * BERT\\n  * T5\\n  * Llama\\n  * Chinchilla AI\\n  * PaLM\\n  * GPT\\n    * 1\\n    * 2\\n    * 3\\n    * J\\n    * ChatGPT\\n    * 4\\n    * 4o\\n    * 4.5\\n    * o1\\n    * o3\\n  * Claude\\n  * Gemini\\n    * chatbot\\n  * Grok\\n  * LaMDA\\n  * BLOOM\\n  * Project Debater\\n  * IBM Watson\\n  * IBM Watsonx\\n  * Granite\\n  * PanGu-Σ\\n  * DeepSeek\\n  * Qwen\\n\\n  \\nDecisional|\\n\\n  * AlphaGo\\n  * AlphaZero\\n  * OpenAI Five\\n  * Self-driving car\\n  * MuZero\\n  * Action selection\\n    * AutoGPT\\n  * Robot control\\n\\n  \\n  \\nPeople|\\n\\n  * Alan Turing\\n  * Warren Sturgis McCulloch\\n  * Walter Pitts\\n  * John von Neumann\\n  * Claude Shannon\\n  * Marvin Minsky\\n  * John McCarthy\\n  * Nathaniel Rochester\\n  * Allen Newell\\n  * Cliff Shaw\\n  * Herbert A. Simon\\n  * Oliver Selfridge\\n  * Frank Rosenblatt\\n  * Bernard Widrow\\n  * Joseph Weizenbaum\\n  * Seymour Papert\\n  * Seppo Linnainmaa\\n  * Paul Werbos\\n  * Jürgen Schmidhuber\\n  * Yann LeCun\\n  * Geoffrey Hinton\\n  * John Hopfield\\n  * Yoshua Bengio\\n  * Lotfi A. Zadeh\\n  * Stephen Grossberg\\n  * Alex Graves\\n  * Andrew Ng\\n  * Fei-Fei Li\\n  * Alex Krizhevsky\\n  * Ilya Sutskever\\n  * Demis Hassabis\\n  * David Silver\\n  * Ian Goodfellow\\n  * Andrej Karpathy\\n\\n  \\nArchitectures|\\n\\n  * Neural Turing machine\\n  * Differentiable neural computer\\n  * Transformer\\n    * Vision transformer (ViT)\\n  * Recurrent neural network (RNN)\\n  * Long short-term memory (LSTM)\\n  * Gated recurrent unit (GRU)\\n  * Echo state network\\n  * Multilayer perceptron (MLP)\\n  * Convolutional neural network (CNN)\\n  * Residual neural network (RNN)\\n  * Highway network\\n  * Mamba\\n  * Autoencoder\\n  * Variational autoencoder (VAE)\\n  * Generative adversarial network (GAN)\\n  * Graph neural network (GNN)\\n\\n  \\n  \\n  * Portals \\n    * Technology\\n  * Category\\n    * Artificial neural networks\\n    * Machine learning\\n  * List \\n    * Companies\\n    * Projects\\n\\n  \\n  \\nRetrieved from\\n\"https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1279480880\"\\n\\nCategories:\\n\\n  * Large language models\\n  * Deep learning\\n  * Natural language processing\\n\\nHidden categories:\\n\\n  * CS1: long volume value\\n  * Webarchive template wayback links\\n  * CS1 maint: multiple names: authors list\\n  * Articles with short description\\n  * Short description is different from Wikidata\\n  * Articles containing potentially dated statements from 2024\\n  * All articles containing potentially dated statements\\n  * All accuracy disputes\\n  * Articles with disputed statements from September 2024\\n  * All articles with unsourced statements\\n  * Articles with unsourced statements from February 2024\\n\\n  * This page was last edited on 8 March 2025, at 20:38 (UTC).\\n  * Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n  * Privacy policy\\n  * About Wikipedia\\n  * Disclaimers\\n  * Contact Wikipedia\\n  * Code of Conduct\\n  * Developers\\n  * Statistics\\n  * Cookie statement\\n  * Mobile view\\n\\n  *   * \\n\\nSearch\\n\\nSearch\\n\\nToggle the table of contents\\n\\nLarge language model\\n\\n54 languages Add topic\\n\\n  *[v]: View this template\\n  *[t]: Discuss this template\\n  *[e]: Edit this template\\n\\n'}\n",
      "Refining :: https://en.wikipedia.org/wiki/Large_language_model\n",
      "-------------------------------\n",
      "{'url': 'https://www.geeksforgeeks.org/large-language-model-llm/', 'markdown': \"Skip to content\\n\\n  * Courses __\\n    * Get IBM Certifications __\\n      * Complete Machine Learning& Data Science Program\\n      * Data Science Training Program\\n      * Data Analytics Training using Excel, SQL, Python & PowerBI\\n      * Complete Data Analytics Program\\n    * DSA to Development\\n    * For Working Professionals __\\n      * Data Structure& Algorithm Classes (Live)\\n      * System Design (Live)\\n      * JAVA Backend Development(Live)\\n      * DevOps(Live)\\n      * Data Structures & Algorithms in Python\\n    * For Students __\\n      * Interview Preparation Course\\n      * GATE CS & IT 2025\\n      * Data Science (Live)\\n      * Data Structure & Algorithm-Self Paced(C++/JAVA)\\n      * Master Competitive Programming(Live)\\n      * Full Stack Development with React & Node JS(Live)\\n    * All Courses\\n    * Full Stack Development\\n    * Data Science Program\\n  * Tutorials __\\n    * Python __\\n      * Python Tutorial\\n      * Python Programs\\n      * Advanced Python Tutorial\\n      * Python Projects\\n    * Web Development in Python __\\n      * Django\\n      * Flask\\n      * Postman\\n      * Web Scrapping in Python\\n    * Data Structures & Algorithms __\\n      * Complete DSA Tutorial\\n      * Company Wise Preparation\\n      * Competitive Programming\\n      * SDE Sheets\\n    * Languages __\\n      * Java\\n      * C++\\n      * R Tutorial\\n      * C\\n      * C#\\n      * SQL\\n      * Perl\\n      * Go Language\\n    * Interview Corner __\\n      * System Design Tutorial\\n      * Top Topics\\n      * Practice Company Questions\\n      * Interview Experiences\\n      * Experienced Interviews\\n      * Internship Interviews\\n      * Multiple Choice Quizzes\\n      * Aptitude for Placements\\n      * Puzzles for Interviews\\n    * Web Development __\\n      * HTML\\n      * CSS\\n      * JavaScript\\n      * TypeScript\\n      * ReactJS\\n      * NextJS\\n      * Node.js\\n      * PHP\\n      * 100 Days of Web Development\\n    * CS Subjects __\\n      * Engineering Mathematics\\n      * Operating System\\n      * DBMS\\n      * Computer Networks\\n      * Computer Organization and Architecture\\n      * Theory of Computation\\n      * Compiler Design\\n      * Digital Logic\\n      * Software Engineering\\n    * DevOps And Linux __\\n      * DevOps Tutorial\\n      * GIT\\n      * AWS\\n      * Kubernetes\\n      * Docker\\n      * Microsoft Azure Tutorial\\n      * Google Cloud Platform\\n      * Linux Tutorial\\n    * GATE __\\n      * GATE Computer Science Notes\\n      * GATE CS Original Papers and Official Keys\\n      * GATE CS 2025 Syllabus\\n      * GATE DA 2025 Syllabus\\n    * School Learning\\n    * GeeksforGeeks Videos\\n  * Data Science __\\n    * Machine Learning\\n    * EDA [Exploratory Data Analysis]\\n    * Data Analysis with Python\\n    * Python Data Visualization\\n    * Data Science using Python\\n    * Data Science using R\\n    * Deep Learning\\n    * NLP Tutorial\\n    * Computer Vision\\n    * Interview Corner __\\n      * Machine Learning Interview Question\\n      * Deep Learning Interview Question\\n      * NLP Interview Question\\n      * Python Interview Questions\\n      * Top 50 R Interview Questions\\n  * Practice __\\n    * All DSA Problems\\n    * Problem of the Day\\n    * GfG SDE Sheet\\n    * Curated DSA Lists __\\n      * Beginner's DSA Sheet\\n      * Love Babbar Sheet\\n      * Top 50 Array Problems\\n      * Top 50 String Problems\\n      * Top 50 Tree Problems\\n      * Top 50 Graph Problems\\n      * Top 50 DP Problems\\n\\n  * __ __ __\\n\\n  * __\\n\\n  * __\\n\\nNotifications\\n\\nMark all as read\\n\\nAll\\n\\nView All\\n\\nNotifications\\n\\nMark all as read\\n\\nAll\\n\\nUnread\\n\\nRead\\n\\nYou're all caught up!!\\n\\n  * __\\n  * \\n\\n__\\n\\n  * Python\\n  * R Language\\n  * Python for Data Science\\n  * NumPy\\n  * Pandas\\n  * OpenCV\\n  * Data Analysis\\n  * ML Math\\n  * Machine Learning\\n  * NLP\\n  * Deep Learning\\n  * Deep Learning Interview Questions\\n  * Machine Learning\\n  * ML Projects\\n  * ML Interview Questions\\n\\n__\\n\\n▲\\n\\nOpen In App\\n\\n  * Ending Soon: 90% Refund Offer\\n  * Share Your Experiences\\n\\n  * What is a Large Language Model (LLM)\\n  * Top 20 LLM (Large Language Models)\\n  * What is LLMOps (Large Language Model Operations)?\\n  * Fine Tuning Large Language Model (LLM)\\n  * What are Language Models in NLP?\\n  * Gemma vs. Gemini vs. LLM (Large Language Model)\\n  * LLM vs GPT : Comparing Large Language Models and GPT\\n  * Future of Large Language Models\\n  * Large Language Models (LLMs) vs Transformers\\n  * Multiturn Deviation in Large Language Model\\n  * What is PaLM 2: Google's Large Language Model Explained\\n  * 10 Free Resources to Learn Large Language Models (LLMs)\\n  * Top 20 Applications of Large Language Models in Real-Life\\n  * Exploring Multimodal Large Language Models\\n  * What is Natural Language Processing (NLP) Chatbots?\\n  * 7 Steps to Mastering Large Language Model Fine-tuning\\n  * Rabbit AI: Large Action Models (LAMs)\\n  * ML | JURASSIC-1 - Language Model\\n  * ALIGN: A Large-scale ImaGe and Noisy-text Model\\n  * Machine Learning & Data Science Course\\n\\n# What is a Large Language Model (LLM)\\n\\nLast Updated :  22 Jan, 2025\\n\\nSummarize\\n\\n__\\n\\nComments\\n\\n__\\n\\nImprove\\n\\n__\\n\\n  *   *   * \\n\\n__ Suggest changes\\n\\nLike Article\\n\\n__ Like\\n\\n__ Share\\n\\n__ Report\\n\\nFollow\\n\\nLarge Language Models (LLMs) represent a breakthrough in artificial\\nintelligence, employing neural network techniques with extensive parameters\\nfor advanced language processing.\\n\\nThis article explores the evolution, architecture, applications, and\\nchallenges of LLMs, focusing on their impact in the field of Natural Language\\nProcessing (NLP).\\n\\n## What are Large Language Models(LLMs)?\\n\\nA ****large language model**** is a type of artificial intelligence algorithm\\nthat applies neural network techniques with lots of parameters to process and\\nunderstand human languages or text using self-supervised learning techniques.\\nTasks like text generation, machine translation, summary writing, image\\ngeneration from texts, machine coding, chat-bots, or Conversational AI are\\napplications of the Large Language Model.\\n\\n> Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional\\n> Encoder Representations from Transformers) by Google, etc.\\n\\nThere are many techniques that were tried to perform natural language-related\\ntasks but the LLM is purely based on the _deep learning_ methodologies. LLM\\n(Large language model) models are highly efficient in capturing the complex\\nentity relationships in the text at hand and can generate the text using the\\nsemantic and syntactic of that particular language in which we wish to do so.\\n\\nIf we talk about the size of the advancements in the GPT (Generative Pre-\\ntrained Transformer) model only then:\\n\\n  * ****GPT-1**** which was released in 2018 contains 117 million parameters having 985 million words.\\n  * ****GPT-2**** which was released in 2019 contains 1.5 billion parameters.\\n  * ****GPT-3**** which was released in 2020 contains 175 billion parameters. Chat GPT is also based on this model as well.\\n  * ****GPT-4**** model is released in the early 2023 and it is likely to contain trillions of parameters.\\n  * ****GPT-4 Turbo**** was introduced in late 2023, optimized for speed and cost-efficiency, but its parameter count remains unspecified.\\n\\n## How do Large Language Models work?\\n\\nLarge Language Models (LLMs) operate on the principles of deep learning,\\nleveraging neural network architectures to process and understand human\\nlanguages.\\n\\nThese models, are trained on vast datasets using self-supervised learning\\ntechniques. The core of their functionality lies in the intricate patterns and\\nrelationships they learn from diverse language data during training. LLMs\\nconsist of multiple layers, including feedforward layers, embedding layers,\\nand attention layers. They employ attention mechanisms, like self-attention,\\nto weigh the importance of different tokens in a sequence, allowing the model\\nto capture dependencies and relationships.\\n\\n## Architecture of LLM\\n\\nLarge Language Model’s (LLM) architecture is determined by a number of\\nfactors, like the objective of the specific model design, the available\\ncomputational resources, and the kind of language processing tasks that are to\\nbe carried out by the LLM. The general architecture of LLM consists of many\\nlayers such as the feed forward layers, embedding layers, attention layers. A\\ntext which is embedded inside is collaborated together to generate\\npredictions.\\n\\nImportant components to influence Large Language Model architecture:\\n\\n  * Model Size and Parameter Count\\n  * input representations\\n  * Self-Attention Mechanisms\\n  * Training Objectives\\n  * Computational Efficiency\\n  * Decoding and Output Generation\\n\\n### Transformer-Based LLM Model Architectures\\n\\nTransformer-based models, which have revolutionized natural language\\nprocessing tasks, typically follow a general architecture that includes the\\nfollowing components:\\n\\n  1. ****Input Embeddings:**** The input text is tokenized into smaller units, such as words or sub-words, and each token is embedded into a continuous vector representation. This embedding step captures the semantic and syntactic information of the input.\\n  2. ****Positional Encoding:**** Positional encoding is added to the input embeddings to provide information about the positions of the tokens because transformers do not naturally encode the order of the tokens. This enables the model to process the tokens while taking their sequential order into account.\\n  3. ****Encoder:**** Based on a neural network technique, the encoder analyses the input text and creates a number of hidden states that protect the context and meaning of text data. Multiple encoder layers make up the core of the transformer architecture. Self-attention mechanism and feed-forward neural network are the two fundamental sub-components of each encoder layer.\\n    1. ****Self-Attention Mechanism:**** Self-attention enables the model to weigh the importance of different tokens in the input sequence by computing attention scores. It allows the model to consider the dependencies and relationships between different tokens in a context-aware manner.\\n    2. ****Feed-Forward Neural Network:**** After the self-attention step, a feed-forward neural network is applied to each token independently. This network includes fully connected layers with non-linear activation functions, allowing the model to capture complex interactions between tokens.\\n  4. ****Decoder Layers:**** In some transformer-based models, a decoder component is included in addition to the encoder. The decoder layers enable autoregressive generation, where the model can generate sequential outputs by attending to the previously generated tokens.\\n  5. ****Multi-Head Attention:**** Transformers often employ multi-head attention, where self-attention is performed simultaneously with different learned attention weights. This allows the model to capture different types of relationships and attend to various parts of the input sequence simultaneously.\\n  6. ****Layer Normalization:**** Layer normalization is applied after each sub-component or layer in the transformer architecture. It helps stabilize the learning process and improves the model’s ability to generalize across different inputs.\\n  7. ****Output Layers:**** The output layers of the transformer model can vary depending on the specific task. For example, in language modeling, a linear projection followed by SoftMax activation is commonly used to generate the probability distribution over the next token.\\n\\nIt’s important to keep in mind that the actual architecture of transformer-\\nbased models can change and be enhanced based on particular research and model\\ncreations. To fulfill different tasks and objectives, several models like GPT,\\nBERT, and T5 may integrate more components or modifications.\\n\\n## Popular Large Language Models\\n\\nNow let’s look at some of the famous LLMs which has been developed and are up\\nfor inference.\\n\\n  * ****GPT-3:**** GPT 3 is developed by OpenAI, stands for Generative Pre-trained Transformer 3. This model powers ChatGPT and is widely recognized for its ability to generate human-like text across a variety of applications.\\n  * ****BERT:**** It is created by Google, is commonly used for natural language processing tasks and generating text embeddings, which can also be utilized for training other models.\\n  * ****RoBERTa:**** RoBERTa is******** an advanced version of BERT, stands for Robustly Optimized BERT Pretraining Approach. Developed by Facebook AI Research, it enhances the performance of the transformer architecture.\\n  * ****BLOOM:**** It is the first multilingual LLM, designed collaboratively by multiple organizations and researchers. It follows an architecture similar to GPT-3, enabling diverse language-based tasks.\\n\\nFor implementation details, these models are available on open-source\\nplatforms like Hugging Face and OpenAI for Python-based applications.\\n\\n## Large Language Models Use Cases\\n\\n  * ****Code Generation**** : LLMs can generate accurate code based on user instructions for specific tasks.\\n  * ****Debugging and Documentation**** : They assist in identifying code errors, suggesting fixes, and even automating project documentation.\\n  * ****Question Answering**** : Users can ask both casual and complex questions, receiving detailed, context-aware responses.\\n  * ****Language Translation and Correction**** : LLMs can translate text between over 50 languages and correct grammatical errors.\\n  * ****Prompt-Based Versatility**** : By crafting creative prompts, users can unlock endless possibilities, as LLMs excel in one-shot and zero-shot learning scenarios.\\n\\nUse cases of LLM are not limited to the above-mentioned one has to be just\\ncreative enough to write better prompts and you can make these models do a\\nvariety of tasks as they are trained to perform tasks on one-shot learning and\\nzero-shot learning methodologies as well. Due to this only Prompt Engineering\\nis a totally new and hot topic in academics for people who are looking forward\\nto using ChatGPT-type models extensively.\\n\\n## Applications of Large Language Models\\n\\nLLMs, such as GPT-3, have a wide range of applications across various domains.\\nFew of them are:\\n\\n  * ****Natural Language Understanding (NLU):****\\n    * Large language models power advanced chatbots capable of engaging in natural conversations.\\n    * They can be used to create intelligent virtual assistants for tasks like scheduling, reminders, and information retrieval.\\n  * ****Content Generation:****\\n    * Creating human-like text for various purposes, including content creation, creative writing, and storytelling.\\n    * Writing code snippets based on natural language descriptions or commands.\\n  * ****Language Translation**** : Large language models can aid in translating text between different languages with improved accuracy and fluency.\\n  * ****Text Summarization**** : Generating concise summaries of longer texts or articles.\\n  * ****Sentiment Analysis**** : Analyzing and understanding sentiments expressed in social media posts, reviews, and comments.\\n\\n## Difference Between NLP and LLM\\n\\nNLP is Natural Language Processing, a field of artificial intelligence (AI).\\nIt consists of the development of the algorithms. NLP is a broader field than\\nLLM, which consists of algorithms and techniques. NLP rules two approaches\\ni.e. Machine learning and the analyze language data. Applications of NLP are-\\n\\n  * Automotive routine task\\n  * Improve search \\n  * Search engine optimization\\n  * Analyzing and organizing large documents\\n  * Social Media Analytics.\\n\\nwhile on the other hand, LLM is a Large Language Model, and is more specific\\nto human- like text, providing content generation, and personalized\\nrecommendations.\\n\\n## What are the Advantages of Large Language Models?\\n\\nLarge Language Models (LLMs) come with several advantages that contribute to\\ntheir widespread adoption and success in various applications:\\n\\n  * LLMs can perform ****zero-shot learning**** , meaning they can generalize to tasks for which they were not explicitly trained. This capability allows for adaptability to new applications and scenarios without additional training.\\n  * LLMs****efficiently handle vast amounts of data**** , making them suitable for tasks that require a deep understanding of extensive text corpora, such as language translation and document summarization.\\n  * LLMs can be ****fine-tuned**** on specific datasets or domains, allowing for continuous learning and adaptation to specific use cases or industries.\\n  * LLMs ****enable the automation**** of various language-related tasks, from code generation to content creation, freeing up human resources for more strategic and complex aspects of a project.\\n\\n## Challenges in Training of Large Language Models\\n\\n  * ****High Costs**** : Training LLMs requires significant financial investment, with millions of dollars needed for large-scale computational power.\\n  * ****Time-Intensive**** : Training takes months, often involving human intervention for fine-tuning to achieve optimal performance.\\n  * ****Data Challenges**** : Obtaining large text datasets is difficult, and concerns about the legality of data scraping for commercial purposes have arisen.\\n  * ****Environmental Impact**** : Training a single LLM from scratch can produce carbon emissions equivalent to the lifetime emissions of five cars, raising serious environmental concerns.\\n\\n## Conclusion\\n\\nDue to the challenges faced in training LLM transfer learning is promoted\\nheavily to get rid of all of the challenges discussed above. LLM has the\\ncapability to bring revolution in the AI-powered application but the\\nadvancements in this field seem a bit difficult because just increasing the\\nsize of the model may increase its performance but after a particular time a\\nsaturation in the performance will come and the challenges to handle these\\nmodels will be bigger than the performance boost achieved by further\\nincreasing the size of the models.\\n\\n## Frequently Asked Questions\\n\\n### What is a large language model?\\n\\n> A large language model is a powerful artificial intelligence system trained\\n> on vast amounts of text data.\\n\\n### What is a LLM in AI?\\n\\n> In AI, LLM refers to Large Language Models, such as GPT-3, designed for\\n> natural language understanding and generation.\\n\\n### What are the best Large Language Models?\\n\\n> Open AI,ChatGPT,GPT-3,GooseAI,Claude,Cohere,GPT-4.\\n\\n### How does LLM model work?\\n\\n> LLMs work by training on diverse language data, learning patterns, and\\n> relationships, enabling them to understand and generate human-like text.\\n\\n### What is an example of an LLM model?\\n\\n> GPT-3 (Generative Pre-trained Transformer 3) is an example of a state-of-\\n> the-art large language model in AI.\\n\\n### What are large language models for education?\\n\\n> Large Language Models are widely being in used for educational purposes:\\n>\\n>   * Provides learning goals\\n>   * Gives a critical summary of any topic to the students\\n>   * Educate students on any topic they want to learn.\\n>\\n\\n  \\n\\n**Get IBM Certification** and a **90% fee refund** on completing 90% course in\\n90 days! Take the Three 90 Challenge today.\\n\\nMaster Machine Learning, Data Science & AI with this complete program and also\\nget a 90% refund. What more motivation do you need? Start the challenge right\\naway!\\n\\n  \\n\\n__ Comment\\n\\nMore info\\n\\nPlacement Training Program\\n\\nNext Article\\n\\nTop 20 LLM (Large Language Models)\\n\\nA\\n\\n__\\n\\nabhishekm482g\\n\\nFollow\\n\\n__\\n\\nImprove __\\n\\nArticle Tags :\\n\\n  * AI-ML-DS\\n  * Data Science\\n  * Machine Learning\\n  * NLP\\n  * ChatGPT\\n  * data-science\\n\\n+2 More\\n\\nPractice Tags :\\n\\n  * Machine Learning\\n\\n### Similar Reads\\n\\nFine Tuning Large Language Model (LLM)\\n\\nLarge Language Models (LLMs) have dramatically transformed natural language\\nprocessing (NLP), excelling in tasks like text generation, translation,\\nsummarization, and question-answering. However, these models may not always be\\nideal for specific domains or tasks. To address this, fine-tuning is\\nperformed. Fine-tuning customizes pre-trained LLMs to\\n\\n__ 13 min read\\n\\nGemma vs. Gemini vs. LLM (Large Language Model)\\n\\nArtificial Intelligence (AI) has witnessed exponential growth, with language\\nmodels at the forefront of many transformative applications. Three key players\\nin this space, Gemma, Gemini, and LLMs (Large Language Models), represent\\ncutting-edge advancements in AI-driven conversational agents and data\\nprocessing. While they share common goals, these t\\n\\n__ 5 min read\\n\\nTop 20 LLM (Large Language Models)\\n\\nLarge Language Model commonly known as an LLM, refers to a neural network\\nequipped with billions of parameters and trained extensively on extensive\\ndatasets of unlabeled text. This training typically involves self-supervised\\nor semi-supervised learning techniques. In this article, we explore about Top\\n20 LLM Models and get to know how each model ha\\n\\n__ 15+ min read\\n\\nLLM vs GPT : Comparing Large Language Models and GPT\\n\\nIn recent years, the field of natural language processing (NLP) has made\\ntremendous strides, largely due to the development of large language models\\n(LLMs) and, more specifically, the Generative Pre-trained Transformer (GPT)\\nseries. Both LLMs and GPTs have transformed how machines understand and\\ngenerate human language. Table of ContentWhat is a La\\n\\n__ 4 min read\\n\\nLLM Architecture: Exploring the Technical Architecture Behind Large Language\\nModels\\n\\nLarge Language Models (LLMs) have become a cornerstone in the field of\\nartificial intelligence, driving advancements in natural language processing\\n(NLP), conversational AI, and various applications that require understanding\\nand generating human-like text. The technical architecture of these models is\\na complex interplay of several components, eac\\n\\n__ 6 min read\\n\\nRAG Vs Fine-Tuning for Enhancing LLM Performance\\n\\nData Science and Machine Learning researchers and practitioners alike are\\nconstantly exploring innovative strategies to enhance the capabilities of\\nlanguage models. Among the myriad approaches, two prominent techniques have\\nemerged which are Retrieval-Augmented Generation (RAG) and Fine-tuning. The\\narticle aims to explore the importance of model pe\\n\\n__ 9 min read\\n\\nDevelop an LLM Application using Openai\\n\\nLanguage Models (LMs) play a crucial role in natural language processing\\napplications, enabling the development of tools that generate human-like text.\\nOpenAI's Generative Pre-Trained Transformer (GPT) models, such as\\nGPT-3.5-turbo, are widely used in this domain. They excel in understanding\\ncontext, learning language patterns, and generating coher\\n\\n__ 5 min read\\n\\nCan LLM replace Data Analyst\\n\\nAs we know, today's era is all about data, as the quantity of data is\\nincreasing daily. Data analysis is the process of extracting, cleaning, and\\npreprocessing the data and gathering insights from the data. Nowadays, there\\nis also a trend of large language models such as ChatGPT4, so many business\\nanalysts use large language models to solve their p\\n\\n__ 7 min read\\n\\nPi: World's Friendliest Chatbot Gets Even Smarter with Inflection-2.5 LLM\\n\\nThe world of conversational AI, powered by large language models (LLMs), is\\nrapidly evolving. Inflection AI, a leader in this field, is pushing the\\nboundaries with its innovative chatbot, Pi. Known for its friendly\\npersonality, Pi is about to get even smarter thanks to a groundbreaking\\nupgrade – the Inflection-2.5 LLM. This cutting-edge AI technolo\\n\\n__ 5 min read\\n\\nSecuring LLM Systems Against Prompt Injection\\n\\nLarge Language Models (LLMs) have revolutionized the field of artificial\\nintelligence, enabling applications such as chatbots, content generators, and\\npersonal assistants. However, the integration of LLMs into various\\napplications has introduced new security vulnerabilities, notably prompt\\ninjection attacks. These attacks exploit the way LLMs proce\\n\\n__ 11 min read\\n\\n__ Like\\n\\n__\\n\\n____\\n\\n389k+ interested Geeks\\n\\n**Complete Machine Learning & Data Science Program **\\n\\nExplore\\n\\n39k+ interested Geeks\\n\\n**Data Science Training Program**\\n\\nExplore\\n\\n1k+ interested Geeks\\n\\n**Artificial Intelligence for Kids - Complete AI Course for Beginners**\\n\\nExplore\\n\\n__\\n\\nCorporate & Communications Address:\\n\\nA-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh\\n(201305)\\n\\n__\\n\\nRegistered Address:\\n\\nK 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh\\nNagar, Uttar Pradesh, 201305\\n\\nAdvertise with us\\n\\n  * Company\\n  * About Us\\n  * Legal\\n  * Privacy Policy\\n  * Careers\\n  * In Media\\n  * Contact Us\\n  * GfG Corporate Solution\\n  * Placement Training Program\\n\\n  * Explore\\n  * Job-A-Thon Hiring Challenge\\n  * Hack-A-Thon\\n  * GfG Weekly Contest\\n  * Offline Classes (Delhi/NCR)\\n  * DSA in JAVA/C++\\n  * Master System Design\\n  * Master CP\\n  * GeeksforGeeks Videos\\n  * Geeks Community\\n\\n  * Languages\\n  * Python\\n  * Java\\n  * C++\\n  * PHP\\n  * GoLang\\n  * SQL\\n  * R Language\\n  * Android Tutorial\\n\\n  * DSA\\n  * Data Structures\\n  * Algorithms\\n  * DSA for Beginners\\n  * Basic DSA Problems\\n  * DSA Roadmap\\n  * DSA Interview Questions\\n  * Competitive Programming\\n\\n  * Data Science & ML\\n  * Data Science With Python\\n  * Data Science For Beginner\\n  * Machine Learning\\n  * ML Maths\\n  * Data Visualisation\\n  * Pandas\\n  * NumPy\\n  * NLP\\n  * Deep Learning\\n\\n  * Web Technologies\\n  * HTML\\n  * CSS\\n  * JavaScript\\n  * TypeScript\\n  * ReactJS\\n  * NextJS\\n  * NodeJs\\n  * Bootstrap\\n  * Tailwind CSS\\n\\n  * Python Tutorial\\n  * Python Programming Examples\\n  * Django Tutorial\\n  * Python Projects\\n  * Python Tkinter\\n  * Web Scraping\\n  * OpenCV Tutorial\\n  * Python Interview Question\\n\\n  * Computer Science\\n  * GATE CS Notes\\n  * Operating Systems\\n  * Computer Network\\n  * Database Management System\\n  * Software Engineering\\n  * Digital Logic Design\\n  * Engineering Maths\\n\\n  * DevOps\\n  * Git\\n  * AWS\\n  * Docker\\n  * Kubernetes\\n  * Azure\\n  * GCP\\n  * DevOps Roadmap\\n\\n  * System Design\\n  * High Level Design\\n  * Low Level Design\\n  * UML Diagrams\\n  * Interview Guide\\n  * Design Patterns\\n  * OOAD\\n  * System Design Bootcamp\\n  * Interview Questions\\n\\n  * School Subjects\\n  * Mathematics\\n  * Physics\\n  * Chemistry\\n  * Biology\\n  * Social Science\\n  * English Grammar\\n\\n  * Software and Tools\\n  * AI Tools Directory\\n  * Marketing Tools Directory\\n  * Accounting Software Directory\\n  * HR Management Tools\\n  * Editing Software Directory\\n  * Microsoft Products and Apps\\n  * Figma Tutorial\\n\\n  * Databases\\n  * SQL\\n  * MYSQL\\n  * PostgreSQL\\n  * PL/SQL\\n  * MongoDB\\n\\n  * Preparation Corner\\n  * Company-Wise Recruitment Process\\n  * Resume Templates\\n  * Aptitude Preparation\\n  * Puzzles\\n  * Company-Wise Preparation\\n  * Companies\\n  * Colleges\\n\\n  * Competitive Exams\\n  * JEE Advanced\\n  * UGC NET\\n  * UPSC\\n  * SSC CGL\\n  * SBI PO\\n  * SBI Clerk\\n  * IBPS PO\\n  * IBPS Clerk\\n\\n  * More Tutorials\\n  * Software Development\\n  * Software Testing\\n  * Product Management\\n  * Project Management\\n  * Linux\\n  * Excel\\n  * All Cheat Sheets\\n  * Recent Articles\\n\\n  * Free Online Tools\\n  * Typing Test\\n  * Image Editor\\n  * Code Formatters\\n  * Code Converters\\n  * Currency Converter\\n  * Random Number Generator\\n  * Random Password Generator\\n\\n  * Write & Earn\\n  * Write an Article\\n  * Improve an Article\\n  * Pick Topics to Write\\n  * Share your Experiences\\n  * Internships\\n\\n  * DSA/Placements\\n  * DSA - Self Paced Course\\n  * DSA in JavaScript - Self Paced Course\\n  * DSA in Python - Self Paced\\n  * C Programming Course Online - Learn C with Data Structures\\n  * Complete Interview Preparation\\n  * Master Competitive Programming\\n  * Core CS Subject for Interview Preparation\\n  * Mastering System Design: LLD to HLD\\n  * Tech Interview 101 - From DSA to System Design [LIVE]\\n  * DSA to Development [HYBRID]\\n  * Placement Preparation Crash Course [LIVE]\\n\\n  * Development/Testing\\n  * JavaScript Full Course\\n  * React JS Course\\n  * React Native Course\\n  * Django Web Development Course\\n  * Complete Bootstrap Course\\n  * Full Stack Development - [LIVE]\\n  * JAVA Backend Development - [LIVE]\\n  * Complete Software Testing Course [LIVE]\\n  * Android Mastery with Kotlin [LIVE]\\n\\n  * Machine Learning/Data Science\\n  * Complete Machine Learning & Data Science Program - [LIVE]\\n  * Data Analytics Training using Excel, SQL, Python & PowerBI - [LIVE]\\n  * Data Science Training Program - [LIVE]\\n  * Mastering Generative AI and ChatGPT\\n  * Data Science Course with IBM Certification\\n\\n  * Programming Languages\\n  * C Programming with Data Structures\\n  * C++ Programming Course\\n  * Java Programming Course\\n  * Python Full Course\\n\\n  * Clouds/Devops\\n  * DevOps Engineering\\n  * AWS Solutions Architect Certification\\n  * Salesforce Certified Administrator Course\\n\\n  * GATE 2026\\n  * GATE CS Rank Booster\\n  * GATE DA Rank Booster\\n  * GATE CS & IT Course - 2026\\n  * GATE DA Course 2026\\n  * GATE Rank Predictor\\n\\n@GeeksforGeeks, Sanchhaya Education Private Limited, All rights reserved\\n\\nWe use cookies to ensure you have the best browsing experience on our website.\\nBy using our site, you acknowledge that you have read and understood our\\n_Cookie Policy_ & _Privacy Policy_ Got It !\\n\\nImprovement\\n\\n__\\n\\nSuggest changes\\n\\nSuggest Changes\\n\\nHelp us improve. Share your suggestions to enhance the article. Contribute\\nyour expertise and make a difference in the GeeksforGeeks portal.\\n\\nCreate Improvement\\n\\nEnhance the article with your expertise. Contribute to the GeeksforGeeks\\ncommunity and help create better learning resources for all.\\n\\n__\\n\\nSuggest Changes\\n\\n__\\n\\nmin 4 words, max Words Limit:1000\\n\\n## Thank You!\\n\\nYour suggestions are valuable to us.\\n\\n## What kind of Experience do you want to share?\\n\\nInterview Experiences\\n\\nAdmission Experiences\\n\\nCareer Journeys\\n\\nWork Experiences\\n\\nCampus Experiences\\n\\nCompetitive Exam Experiences\\n\\n\"}\n",
      "Refining :: https://www.geeksforgeeks.org/large-language-model-llm/\n",
      "-------------------------------\n",
      "{'url': 'https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f', 'markdown': 'Open in app\\n\\nSign up\\n\\nSign in\\n\\nWrite\\n\\nSign up\\n\\nSign in\\n\\nTitle Image. How Large Language Models work. From zero to ChatGPT.\\n\\n# How Large Language Models work\\n\\n## From zero to ChatGPT\\n\\nAndreas Stöffelbauer\\n\\n·\\n\\nFollow\\n\\nPublished in\\n\\nData Science at Microsoft\\n\\n·\\n\\n25 min read\\n\\n·\\n\\nOct 24, 2023\\n\\n\\\\--\\n\\n56\\n\\nListen\\n\\nShare\\n\\nThanks to Large Language Models (or LLMs for short), Artificial Intelligence\\nhas now caught the attention of pretty much everyone. ChatGPT, possibly the\\nmost famous LLM, has immediately skyrocketed in popularity due to the fact\\nthat natural language is such a, well, natural interface that has made the\\nrecent breakthroughs in Artificial Intelligence accessible to everyone.\\nNevertheless, how LLMs work is still less commonly understood, unless you are\\na Data Scientist or in another AI-related role. In this article, I will try to\\nchange that.\\n\\nAdmittedly, that’s an ambitious goal. After all, the powerful LLMs we have\\ntoday are a culmination of decades of research in AI. Unfortunately, most\\narticles covering them are one of two kinds: They are either very technical\\nand assume a lot of prior knowledge, or they are so trivial that you don’t end\\nup knowing more than before.\\n\\nThis article is meant to strike a balance between these two approaches. Or\\nactually let me rephrase that, it’s meant to take you from zero all the way\\nthrough to how LLMs are trained and why they work so impressively well. We’ll\\ndo this by picking up just all the relevant pieces along the way.\\n\\nThis is not going to be a deep dive into all the nitty-gritty details, so\\nwe’ll rely on intuition here rather than on math, and on visuals as much as\\npossible. But as you’ll see, while certainly being a very complex topic in the\\ndetails, the main mechanisms underlying LLMs are very intuitive, and that\\nalone will get us very far here.\\n\\nThis article should also help you get more out of using LLMs like ChatGPT. In\\nfact, we will learn some of the neat tricks that you can apply to increase the\\nchances of a useful response. Or as Andrei Karparthy, a well-known AI\\nresearcher and engineer, recently and pointedly said: “English is the hottest\\nnew programming language.”\\n\\nBut first, let’s try to understand where LLMs fit in the world of Artificial\\nIntelligence.\\n\\nThe field of Artificial Intelligence in layers.\\n\\nThe field of AI is often visualized in layers:\\n\\n  * **Artificial Intelligence** (AI) is very a broad term, but generally it deals with intelligent machines.\\n  * **Machine Learning** (ML) is a subfield of AI that specifically focuses on pattern recognition in data. As you can imagine, once you recoginze a pattern, you can apply that pattern to new observations. That’s the essence of the idea, but we will get to that in just a bit.\\n  * **Deep Learning** is the field within ML that is focused on unstructured data, which includes text and images. It relies on artificial neural networks, a method that is (loosely) inspired by the human brain.\\n  * **Large Language Models** (LLMs) deal with text specifically, and that will be the focus of this article.\\n\\nAs we go, we’ll pick up the relevant pieces from each of those layers. We’ll\\nskip only the most outer one, Artificial Intelligence (as it is too general\\nanyway) and head straight into what is Machine Learning.\\n\\nMachine Learning. Level: Beginner.\\n\\nThe goal of Machine Learning is to discover patterns in data. Or more\\nspecifically, a pattern that describes the relationship between an input and\\nan outcome. This is best explained using an example.\\n\\nLet’s say we would like to distinguish between two of my favorite genres of\\nmusic: reggaeton and R&B. If you are not familiar with those genres, here’s a\\nvery quick intro that will help us understand the task. Reggaeton is a Latin\\nurban genre known for its lively beats and danceable rhythms, while R&B\\n(Rhythm and Blues) is a genre rooted in African-American musical traditions,\\ncharacterized by soulful vocals and a mix of upbeat and slower-paced songs.\\n\\nMachine Learning in practice. Predicting music genre is an example of a\\nclassification problem.\\n\\nSuppose we have 20 songs. We know each song’s tempo and energy, two metrics\\nthat can be simply measured or computed for any song. In addition, we’ve\\nlabeled them with a genre, either reggaeton or R&B. When we visualize the\\ndata, we can see that high energy, high tempo songs are primarily reggaeton\\nwhile lower tempo, lower energy songs are mostly R&B, which makes sense.\\n\\nHowever, we want to avoid having to label the genre by hand all the time\\nbecause it’s time consuming and not scalable. Instead, we can learn the\\nrelationship between the song metrics (tempo, energy) and genre and then make\\npredictions using only the readily available metrics.\\n\\nIn Machine Learning terms, we say that this is a classification problem,\\nbecause the outcome variable (the genre) can only take on one of a fixed set\\nof classes/labels — here reggaeton and R&B. This is in contrast to a\\nregression problem, where the outcome is a continuous value (e.g., a\\ntemperature or a distance).\\n\\nWe can now “train” a Machine Learning model (or “classifier”) using our\\nlabeled dataset, i.e., using a set of songs for which we do know the genre.\\nVisually speaking, what the training of the model does here is that it finds\\nthe line that best separates the two classes.\\n\\nHow is that useful? Well, now that we know this line, for any new song we can\\nmake a prediction about whether it’s a reggaeton or an R&B song, depending on\\nwhich side of the line the song falls on. All we need is the tempo and energy,\\nwhich we assumed is more easily available. That is much simpler and scalable\\nthan have a human assign the genre for each and every song.\\n\\nAdditionally, as you can imagine, the further away from the line, the more\\ncertain we can be about being correct. Therefore, we can often also make a\\nstatement on how confident we are that a prediction is correct based on the\\ndistance from the line. For example, for our new low-energy, low-tempo song we\\nmight be 98 percent certain that this is an R&B song, with a two percent\\nlikelihood that it’s actually reggaeton.\\n\\nIn reality, things are often much more complex.\\n\\nBut of course, reality is often more complex than that.\\n\\nThe best boundary to separate the classes may not be linear. In other words,\\nthe relationship between the inputs and the outcome can be more complex. It\\nmay be curved as in the image above, or even many times more complex than\\nthat.\\n\\nReality is typically more complex in another way too. Rather than only two\\ninputs as in our example, we often have tens, hundreds, or even thousands of\\ninput variables. In addition, we often have more than two classes. And all\\nclasses can depend on all these inputs through an incredibly complex, non-\\nlinear relationship.\\n\\nEven with our example, we know that in reality there are more than two genres,\\nand we need many more metrics other than tempo and energy. The relationship\\namong them is probably not so simple either.\\n\\nWhat I mainly want you to take away is this: The more complex the relationship\\nbetween input and output, the more complex and powerful is the Machine\\nLearning model we need in order to learn that relationship. Usually, the\\ncomplexity increases with the number of inputs and the number of classes.\\n\\nIn addition to that, we also need more data as well. You will see why this is\\nimportant in just a bit.\\n\\nImage classification example.\\n\\nLet’s move on to a slightly different problem now, but one for which we will\\nsimply try to apply our mental model from before. In our new problem we have\\nas input an image, for example, this image of a cute cat in a bag (because\\nexamples with cats are always the best).\\n\\nAs for our outcome, let’s say this time that we have three possible labels:\\ntiger, cat, and fox. If you need some motivation for this task, let’s say we\\nmay want to protect a herd of sheep and sound an alarm if we see a tiger but\\nnot if we see a cat or a fox.\\n\\nWe already know this is again a classification task because the output can\\nonly take on one of a few fixed classes. Therefore, just like before, we could\\nsimply use some available labeled data (i.e., images with assigned class\\nlabels) and train a Machine Learning model.\\n\\nHowever, it’s not quite obvious as to exactly how we would process a visual\\ninput, as a computer can process only numeric inputs. Our song metrics energy\\nand tempo were numeric, of course. And fortunately, images are just numeric\\ninputs too as they consist of pixels. They have a height, a width, and three\\nchannels (red, green, and blue). So in theory, we could directly feed the\\npixels into a Machine Learning model (ignore for now that there is a spatial\\nelement here, which we haven’t dealt with before).\\n\\nHowever, now we are facing two problems. First, even a small, low-quality\\n224x224 image consists of more than 150,000 pixels (224x224x3). Remember, we\\nwere speaking about a maximum of hundreds of input variables (rarely more than\\na thousand), but now we suddenly have at least 150,000.\\n\\nSecond, if you think about the relationship between the raw pixels and the\\nclass label, it’s incredibly complex, at least from an ML perspective that is.\\nOur human brains have the amazing ability to generally distinguish among\\ntigers, foxes, and cats quite easily. However, if you saw the 150,000 pixels\\none by one, you would have no idea what the image contains. But this is\\nexactly how a Machine Learning model sees them, so it needs to learn from\\nscratch the mapping or relationship between those raw pixels and the image\\nlabel, which is not a trivial task.\\n\\nSentiment classification example.\\n\\nLet’s consider another type of input-output relationship that is extremely\\ncomplex — the relationship between a sentence and its sentiment. By sentiment\\nwe typically mean the emotion that a sentence conveys, here positive or\\nnegative.\\n\\nLet’s formalize the problem setup again: As the input here we have a sequence\\nof words, i.e., a sentence, and the sentiment is our outcome variable. As\\nbefore, this is a classification task, this time with two possible labels,\\ni.e., positive or negative.\\n\\nAs with the images example discussed earlier, as humans we understand this\\nrelationship naturally, but can we teach a Machine Learning model to do the\\nsame?\\n\\nBefore answering that, it’s again not obvious at the start how words can be\\nturned into numeric inputs for a Machine Learning model. In fact, this is a\\nlevel or two more complicated than what we saw with images, which as we saw\\nare essentially already numeric. This is not the case with words. We won’t go\\ninto details here, but what you need to know is that every word can be turned\\ninto a word embedding.\\n\\nIn short, a word embedding represents the word’s semantic and syntactic\\nmeaning, often within a specific context. These embeddings can be obtained as\\npart of training the Machine Learning model, or by means of a separate\\ntraining procedure. Usually, word embeddings consist of between tens and\\nthousands of variables, per word that is.\\n\\nTo summarize, what to take away from here is that we can take a sentence and\\nturn it into a sequence of numeric inputs, i.e., the word embeddings, which\\ncontain semantic and syntactic meaning. This can then be fed into a Machine\\nLearning model. (Again, if you’re observant you may notice that there is a new\\nsequential dimension that is unlike our examples from before, but we will\\nignore this one here too.)\\n\\nGreat, but now we face the same challenges as with the visual input. As you\\ncan imagine, with a long sentence (or paragraph or even a whole document), we\\ncan quickly reach a very large number of inputs because of the large size of\\nthe word embeddings.\\n\\nThe second problem is the relationship between language and its sentiment,\\nwhich is complex — very complex. Just think of a sentence like “That was a\\ngreat fall” and all the ways it can be interpreted (not to mention\\nsarcastically).\\n\\nWhat we need is an extremely powerful Machine Learning model, and lots of\\ndata. That’s where Deep Learning comes in.\\n\\nDeep Learning. Level: Advanced.\\n\\nWe already took a major step toward understanding LLMs by going through the\\nbasics of Machine Learning and the motivations behind the use of more powerful\\nmodels, and now we’ll take another big step by introducing Deep Learning.\\n\\nWe talked about the fact that if the relationship between an input and output\\nis very complex, as well as if the number of input or output variables is\\nlarge (and both are the case for our image and language examples from before),\\nwe need more flexible, powerful models. A linear model or anything close to\\nthat will simply fail to solve these kinds of visual or sentiment\\nclassification tasks.\\n\\nThis is where neural networks come in.\\n\\nNeural Networks are the most powerful Machine Learning models we have today.\\n\\nNeural networks are powerful Machine Learning models that allow arbitrarily\\ncomplex relationships to be modeled. They are the engine that enables learning\\nsuch complex relationships at massive scale.\\n\\nIn fact, neural networks are loosely inspired by the brain, although the\\nactual similarities are debatable. Their basic architecture is relatively\\nsimple. They consist of a sequence of layers of connected “neurons” that an\\ninput signal passes through in order to predict the outcome variable. You can\\nthink of them as multiple layers of linear regression stacked together, with\\nthe addition of non-linearities in between, which allows the neural network to\\nmodel highly non-linear relationships.\\n\\nNeural networks are often many layers deep (hence the name Deep Learning),\\nwhich means they can be extremely large. ChatGPT, for example, is based on a\\nneural network consisting of 176 billion neurons, which is more than the\\napproximate 100 billion neurons in a human brain.\\n\\nSo, from here on we will assume a neural network as our Machine Learning\\nmodel, and take into account that we have also learned how to process images\\nand text.\\n\\nLarge Language Models. Level: Expert.\\n\\nFinally, we can start talking about Large Language Models, and this is where\\nthings get really interesting. If you have made it this far, you should have\\nall the knowledge to also understand LLMs.\\n\\nWhat’s a good way to start? Probably by explaining what _Large Language Model_\\nactually means. We already know what large means, in this case it simply\\nrefers to the number of neurons, also called parameters, in the neural\\nnetwork. There is no clear number for what constitutes a Large Language Model,\\nbut you may want to consider everything above 1 billion neurons as large.\\n\\nWith that established, what’s a “language model”? Let’s discuss this next —\\nand just know that in a bit, we’ll also get to learn what the GPT in ChatGPT\\nstands for. But one step at a time.\\n\\nLanguage modeling is learning to predict the next word.\\n\\nLet’s take the following idea and frame it as a Machine Learning problem: What\\nis the next word in a given sequence of words, i.e., in a sentence or\\nparagraph? In other words, we simply want to learn how to predict the next\\nword at any time. From earlier in this article we’ve learned everything we\\nneed to frame that as a Machine Learning problem. In fact, the task is not\\nunlike the sentiment classification we saw earlier.\\n\\nAs in that example, the input to the neural network is a sequence of words,\\nbut now, the outcome is simply the next word. Again, this is just a\\nclassification task. The only difference is that instead of only two or a few\\nclasses, we now have as many classes as there are words — let’s say around\\n50,000. This is what language modeling is about — learning to predict the next\\nword.\\n\\nOkay, so that’s orders of magnitude more complex than the binary sentiment\\nclassification, as you can imagine. But now that we also know about neural\\nnetworks and their sheer power, the only response to that concern is really\\n“why not?”\\n\\n> **Quick disclaimer** : Of course, we are simplifying many things here (as is\\n> done throughout the article). In reality things are a little more complex,\\n> but that shouldn’t hold us back from understanding the main mechanics, which\\n> is why we simplify and leave out some of the details.\\n\\nMassive amounts of traning data can be created relatively easily.\\n\\nWe know the task, and now we need data to train the neural network. It’s\\nactually not difficult to create a lot of data for our “next word prediction”\\ntask. There’s an abundance of text on the internet, in books, in research\\npapers, and more. And we can easily create a massive dataset from all of this.\\nWe don’t even need to label the data, because the next word itself is the\\nlabel, that’s why this is also called _self-supervised learning_.\\n\\nThe image above shows how this is done. Just a single sequence can be turned\\ninto multiple sequences for training. And we have lots of such sequences.\\nImportantly, we do this for many short and long sequences (some up to\\nthousands of words) so that in every context we learn what the next word\\nshould be.\\n\\nTo summarize, all we are doing here is to train a neural network (the LLM) to\\npredict the next word in a given sequence of words, no matter if that sequence\\nis long or short, in German or in English or in any other language, whether\\nit’s a tweet or a mathematical formula, a poem or a snippet of code. All of\\nthose are sequences that we will find in the training data.\\n\\nIf we have a large enough neural network as well as enough data, the LLM\\nbecomes really good at predicting the next word. Will it be perfect? No, of\\ncourse not, since there are often multiple words that can follow a sequence.\\nBut it will become good at selecting one of the appropriate words that are\\nsyntactically and semantically appropriate.\\n\\nWe can perform natural language generation by predicting one word at a time.\\n\\nNow that we can predict one word, we can feed the extended sequence back into\\nthe LLM and predict another word, and so on. In other words, using our trained\\nLLM, we can now generate text, not just a single word. This is why LLMs are an\\nexample of what we call Generative AI. We have just taught the LLM to speak,\\nso to say, one word at a time.\\n\\nThere’s one more detail to this that I think is important to understand. We\\ndon’t necessarily always have to predict the most likely word. We can instead\\nsample from, say, the five most likely words at a given time. As a result, we\\nmay get some more creativity from the LLM. Some LLMs actually allow you to\\nchoose how deterministic or creative you want the output to be. This is also\\nwhy in ChatGPT, which uses such a sampling strategy, you typically do not get\\nthe same answer when you regenerate a response.\\n\\nSpeaking of ChatGPT, you could ask yourself now why it’s not called ChatLLM.\\nAs it turns out, language modeling is not the end of the story — in fact it’s\\njust the beginning. So what does the GPT in ChatGPT stand for?\\n\\nGPT = Generative Pre-trained Transformer.\\n\\nWe have actually just learned what the G stands for, namely “generative” —\\nmeaning that it was trained on a language generation pretext, which we have\\ndiscussed. But what about the P and the T?\\n\\nWe’ll gloss over the T here, which stands for “transformer” — not the one from\\nthe movies (sorry), but one that’s simply the type of neural network\\narchitecture that is being used. This shouldn’t really bother us here, but if\\nyou are curious and you only want to know its main strength, it’s that the\\ntransformer architecture works so well because it can focus its attention on\\nthe parts of the input sequence that are most relevant at any time. You could\\nargue that this is similar to how humans work. We, too, need to focus our\\nattention on what’s most relevant to the task and ignore the rest.\\n\\nNow to the P, which stands for “pre-training”. We discuss next why we suddenly\\nstart speaking about pre-training and not just training any longer.\\n\\nThe reason is that Large Language Models like ChatGPT are actually trained in\\nphases.\\n\\nPhases of LLM training: (1) Pre-Training, (2) Instruction Fine-Tuning, (3)\\nReinforcement from Human Feedback (RLHF).\\n\\n## Pre-training\\n\\nThe first stage is pre-training, which is exactly what we’ve gone through just\\nnow. This stage requires massive amounts of data to learn to predict the next\\nword. In that phase, the model learns not only to master the grammar and\\nsyntax of language, but it also acquires a great deal of knowledge about the\\nworld, and even some other emerging abilities that we will speak about later.\\n\\nBut now I have a couple of questions for you: First, what might be the problem\\nwith this kind of pre-training? Well, there are certainly a few, but the one I\\nam trying to point to here has to do with what the LLM has really learned.\\n\\nNamely, it has learned mainly to ramble on about a topic. It may even be doing\\nan incredibly good job, but what it doesn’t do is respond well to the kind of\\ninputs you would generally want to give an AI, such as a question or an\\ninstruction. The problem is that this model has not learned to be, and so is\\nnot behaving as, an assistant.\\n\\nFor example, if you ask a pre-trained LLM “What is your fist name?” it may\\nrespond with “What is your last name?” simply because this is the kind of data\\nit has seen during pre-training, as in many empty forms, for example. It’s\\nonly trying to complete the input sequence.\\n\\nIt doesn’t do well with following instructions simply because this kind of\\nlanguage structure, i.e., instruction followed by a response, is not very\\ncommonly seen in the training data. Maybe Quora or StackOverflow would be the\\nclosest representation of this sort of structure.\\n\\nAt this stage, we say that the LLM is not aligned with human intentions.\\nAlignment is an important topic for LLMs, and we’ll learn how we can fix this\\nto a large extent, because as it turns out, those pre-trained LLMs are\\nactually quite steerable. So even though initially they don’t respond well to\\ninstructions, they can be taught to do so.\\n\\n## Instruction fine-tuning and RLHF\\n\\nThis is where instruction tuning comes in. We take the pre-trained LLM with\\nits current abilities and do essentially what we did before — i.e., learn to\\npredict one word at a time — but now we do this using only high-quality\\ninstruction and response pairs as our training data.\\n\\nThat way, the model un-learns to simply be a text completer and learns to\\nbecome a helpful assistant that follows instructions and responds in a way\\nthat is aligned with the user’s intention. The size of this instruction\\ndataset is typically a lot smaller than the pre-training set. This is because\\nthe high-quality instruction-response pairs are much more expensive to create\\nas they are typically sourced from humans. This is very different from the\\ninexpensive self-supervised labels we used in pre-training. This is why this\\nstage is also called _supervised instruction fine-tuning_.\\n\\nThere is also a third stage that some LLMs like ChatGPT go through, which is\\nreinforcement learning from human feedback (RLHF). We won’t go into details\\nhere, but the purpose is similar to instruction fine-tuning. RLHF also helps\\nalignment and ensures that the LLM’s output reflects human values and\\npreferences. There is some early research that indicates that this stage is\\ncritical for reaching or surpassing human-level performance. In fact,\\ncombining the fields of reinforcement learning and language modeling is being\\nshown to be especially promising and is likely to lead to some massive\\nimprovements over the LLMs we currently have.\\n\\nSo now let’s test our understanding on some common use cases.\\n\\nExamples to test our understanding of LLMs.\\n\\nFirst, **why can an LLM perform summarization** of a longer piece of text? (If\\nyou didn’t already know, it does a really great job. Just paste in a document\\nand ask it to summarize it.)\\n\\nTo understand why, we need to think about the training data. As it so happens,\\npeople often make summarizations — on the internet, in research papers, books,\\nand more. As a result, an LLM trained on that data learns how to do that too.\\nIt learns to attend to the main points and compress them into a short text.\\n\\nNote that when a summary is generated, the full text is part of the input\\nsequence of the LLM. This is similar to, say, a research paper that has a\\nconclusion while the full text appears just before.\\n\\nAs a result, that skill has probably been learned during pre-training already,\\nalthough surely instruction fine-tuning helped improve that skill even\\nfurther. We can assume that this phase included some summarization examples\\ntoo.\\n\\nSecond, **why can a LLM answer common knowledge questions**?\\n\\nAs mentioned, the ability to act as an assistant and respond appropriately is\\ndue to instruction fine-tuning and RLHF. But all (or most of) the knowledge to\\nanswer questions itself was already acquired during pre-training.\\n\\nOf course, that now raises another big question: **What if the LLM doesn’t\\nknow the answer**? Unfortunately, it may just make one up in that case. To\\nunderstand why, we need to think about the training data again, and the\\ntraining objective.\\n\\nLLMs suffer from hallucinations, but this can be mitigated by providing\\nadditional context.\\n\\nYou might have heard about the term “hallucination” in the context of LLMs,\\nwhich refers to the phenomenon of LLMs making up facts when they shouldn’t.\\n\\nWhy does that happen? Well, the LLM learns only to generate text, not\\nfactually true text. Nothing in its training gives the model any indicator of\\nthe truth or reliability of any of the training data. However, that is not\\neven the main issue here, it’s that generally text out there on the internet\\nand in books sounds confident, so the LLM of course learns to sound that way,\\ntoo, even if it is wrong. In this way, an LLM has little indication of\\nuncertainty.\\n\\nThat being said, this is an active area of research, from which we can expect\\nthat LLMs will be less prone to hallucinations over time. For example, during\\ninstruction tuning we can try and teach the LLM to abstain from hallucinating\\nto some extent, but only time will tell whether we can fully solve this issue.\\n\\nYou may be surprised that we can actually try to solve this problem here\\ntogether right now. We have the knowledge we need to figure out a solution\\nthat at least partially helps and is already used widely today.\\n\\nBing chat is an example of a search-based LLM workflow.\\n\\nSuppose that you ask the LLM the following question: Who is the current\\npresident of Colombia? There’s a good chance an LLM may respond with the wrong\\nname. This could be because of two reasons:\\n\\n  * The first is what we have already brought up: The LLM may just hallucinate and simply respond with a wrong or even fake name.\\n  * The second one I will mention only in passing: LLMs are trained only on data up to a certain cut-off date, and that can be as early as last year. Because of that, the LLM cannot even know the current president with certainty, because things could have changed since the data was created.\\n\\nSo how can we solve both these problems? The answer lies in providing the\\nmodel some relevant context. The rationale here is that everything that’s in\\nthe LLM’s input sequence is readily available for it to process, while any\\nimplicit knowledge it has acquired in pre-training is more difficult and\\nprecarious for it to retrieve.\\n\\nSuppose we were to include the Wikipedia article on Colombia’s political\\nhistory as context for the LLM. In that case it would much more likely to\\nanswer correctly because it can simply extract the name from the context\\n(given that it is up to date and includes the current president of course).\\n\\nIn the image above you can see what a typical prompt for an LLM with\\nadditional context may look like. (By the way, prompt is just another name for\\nthe instructions we give to an LLM, i.e., the instructions form the input\\nsequence.)\\n\\nThis process is called grounding the LLM in the context, or in the real world\\nif you like, rather than allowing it to generate freely.\\n\\nAnd that’s exactly how Bing Chat and other search-based LLMs work. They first\\nextract relevant context from the web using a search engine and then pass all\\nthat information to the LLM, alongside the user’s initial question. See the\\nillustration above for a visual of how this is accomplished.\\n\\nBack to the AI Magic. Level: Unicorn.\\n\\nWe’ve now reached a point where you pretty much understand the main mechanisms\\nof the state-of-the art LLMs (as of the second half of 2023, anyway).\\n\\nYou may be thinking “this is actually not that magical” because all that is\\nhappening is the predicting of words, one at a time. It’s pure statistics,\\nafter all. Or is it?\\n\\nLet’s back up a bit. The magical part of all this is how remarkably well it\\nworks. In fact, everyone, even the researchers at OpenAI, were surprised at\\nhow far this sort of language modeling can go. One of the key drivers in the\\nlast few years has simply been the massive scaling up of neural networks and\\ndata sets, which has caused performance to increase along with them. For\\nexample, GPT-4, reportedly a model with more than one trillion parameters in\\ntotal, can pass the bar exam or AP Biology with a score in the top 10 percent\\nof test takers.\\n\\nSurprisingly, those large LLMs even show certain **emerging abilities** ,\\ni.e., abilities to solve tasks and to do things that they were not explicitly\\ntrained to do.\\n\\nIn this last part of the article, we’ll discuss some of these emerging\\nabilities and I’ll show you some tricks for how you can use them to solve\\nproblems.\\n\\nLLMs can solve entirely new tasks in a zero-shot manner.\\n\\nA ubiquitous emerging ability is, just as the name itself suggests, that LLMs\\ncan perform entirely new tasks that they haven’t encountered in training,\\nwhich is called zero-shot. All it takes is some instructions on how to solve\\nthe task.\\n\\nTo illustrate this ability with a silly example, you can ask an LLM to\\ntranslate a sentence from German to English while responding only with words\\nthat start with “f”.\\n\\nFor instance, when asked to translate a sentence using only words that start\\nwith “f”, an LLM translated “Die Katze schläft gerne in der Box” (which is\\nGerman and literally means “The cat likes to sleep in the box”) with “Feline\\nfriend finds fluffy fortress”, which is a pretty cool translation, I think.\\n\\nLLMs, just like humans, can benefit from providing them with examples or\\ndemonstrations.\\n\\nFor more complex tasks, you may quickly realize that zero-shot prompting often\\nrequires very detailed instructions, and even then, performance is often far\\nfrom perfect.\\n\\nTo make another connection to human intelligence, if someone tells you to\\nperform a new task, you would probably ask for some examples or demonstrations\\nof how the task is performed. LLMs can benefit from the same.\\n\\nAs an example, let’s say you want a model to translate different currency\\namounts into a common format. You could describe what you want in details or\\njust give a brief instruction and some example demonstrations. The image above\\nshows a sample task.\\n\\nUsing this prompt, the model should do well on the last example, which is\\n“Steak: 24.99 USD”, and respond with $24.99.\\n\\nNote how we simply left out the solution to the last example. Remember that an\\nLLM is still a text-completer at heart, so keep a consistent structure. You\\nshould almost force the model to respond with just what you want, as we did in\\nthe example above.\\n\\nTo summarize, a general tip is to provide some examples if the LLM is\\nstruggling with the task in a zero-shot manner. You will find that often helps\\nthe LLM understand the task, making the performance typically better and more\\nreliable.\\n\\nChain-of-thought provides LLMs a working memory, which can improve their\\nperformance substantially, especially on more complex tasks.\\n\\nAnother interesting ability of LLMs is also reminiscent of human intelligence.\\nIt is especially useful if the task is more complex and requires multiple\\nsteps of reasoning to solve.\\n\\nLet’s say I ask you “Who won the World Cup in the year before Lionel Messi was\\nborn?” What would you do? You would probably solve this step by step by\\nwriting down any intermediate solutions needed in order to arrive at the\\ncorrect answer. And that’s exactly what LLMs can do too.\\n\\nIt has been found that simply telling an LLM to “think step by step” can\\nincrease its performance substantially in many tasks.\\n\\nWhy does this work? We know everything we need to answer this. The problem is\\nthat this kind of unusual composite knowledge is probably not directly in the\\nLLM’s internal memory. However, all the individual facts might be, like\\nMessi’s birthday, and the winners of various World Cups.\\n\\nAllowing the LLM to build up to the final answer helps because it gives the\\nmodel time to think out loud — a working memory so to say — and to solve the\\nsimpler sub-problems before giving the final answer.\\n\\nThe key here is to remember that everything to the left of a to-be-generated\\nword is context that the model can rely on. So, as shown in the image above,\\nby the time the model says “Argentina”, Messi’s birthday and the year of the\\nWord Cup we inquired about are already in the LLM’s working memory, which\\nmakes it easier to answer correctly.\\n\\n# Conclusion\\n\\nBefore I wrap things up, I want to answer a question I asked earlier in the\\narticle. Is the LLM really just predicting the next word or is there more to\\nit? Some researchers are arguing for the latter, saying that to become so good\\nat next-word-prediction in any context, the LLM must actually have acquired a\\ncompressed understanding of the world internally. Not, as others argue, that\\nthe model has simply learned to memorize and copy patterns seen during\\ntraining, with no actual understanding of language, the world, or anything\\nelse.\\n\\nThere is probably no clear right or wrong between those two sides at this\\npoint; it may just be a different way of looking at the same thing. Clearly\\nthese LLMs are proving to be very useful and show impressive knowledge and\\nreasoning capabilities, and maybe even show some sparks of general\\nintelligence. But whether or to what extent that resembles human intelligence\\nis still to be determined, and so is how much further language modeling can\\nimprove the state of the art.\\n\\nI hope that this article helps you understand LLMs and the current craze that\\nis surrounding them, so that you can form your own opinion about AI’s\\npotentials and risks. It’s not only up to AI researchers and data scientists\\nto decide how AI is used to benefit the world; everyone should be able to have\\na say. This is why I wanted to write an article that doesn’t require a lot of\\nbackground knowledge.\\n\\nIf you made it through this article, I think you pretty much know how some the\\nstate-of-the-art LLMs work (as of Autumn 2023), at least at a high level.\\n\\nI’ll leave you with some of my final thoughts on the current state of\\nArtificial Intelligence and LLMs.\\n\\nFinal thoughts on the state-of-the-art in Artificial Intelligence.\\n\\nThank you for reading. If you have any questions, feel free to contact me on\\nLinkedIn. Thank you also to Casey Doyle for the edits and suggestions.\\n\\nLarge Language Models\\n\\nNLP\\n\\nData Science\\n\\nMachine Learning\\n\\nDeep Learning\\n\\n\\\\--\\n\\n\\\\--\\n\\n56\\n\\n## Published in Data Science at Microsoft\\n\\n7.4K Followers\\n\\n·Last published 2 days ago\\n\\nLessons learned in the practice of data science at Microsoft.\\n\\nFollow\\n\\n## Written by Andreas Stöffelbauer\\n\\n1.4K Followers\\n\\n·50 Following\\n\\nData Scientist @ Microsoft | Guest Teacher @ London School of Economics (LSE)\\n\\nFollow\\n\\n## Responses (56)\\n\\nSee all responses\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams\\n\\n'}\n",
      "Refining :: https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f\n",
      "-------------------------------\n",
      "{'url': 'https://aws.amazon.com/what-is/large-language-model/', 'markdown': \"Skip to main content\\n\\nClick here to return to Amazon Web Services homepage\\n\\nAbout AWS Contact Us Support  English  My Account\\n\\nSign In\\n\\nCreate an AWS Account\\n\\n__\\n\\n__\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nLogin\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nView profile\\n\\nLog out\\n\\n__\\n\\n  * Amazon Q\\n  * Products\\n  * Solutions\\n  * Pricing\\n  * Documentation\\n  * Learn\\n  * Partner Network\\n  * AWS Marketplace\\n  * Customer Enablement\\n  * Events\\n  * Explore More \\n\\n__\\n\\n__\\n\\nClose\\n\\n  * عربي\\n  * Bahasa Indonesia\\n  * Deutsch\\n  * English\\n  * Español\\n  * Français\\n  * Italiano\\n  * Português\\n\\n  * Tiếng Việt\\n  * Türkçe\\n  * Ρусский\\n  * ไทย\\n  * 日本語\\n  * 한국어\\n  * 中文 (简体)\\n  * 中文 (繁體)\\n\\nClose\\n\\n  * My Profile\\n  * Sign out of AWS Builder ID\\n  * AWS Management Console\\n  * Account Settings\\n  * Billing & Cost Management\\n  * Security Credentials\\n  * AWS Personal Health Dashboard\\n\\nClose\\n\\n  * Support Center\\n  * Expert Help\\n  * Knowledge Center\\n  * AWS Support Overview\\n  * AWS re:Post\\n\\nClick here to return to Amazon Web Services homepage\\n\\n__\\n\\n__\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nLogin\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nView profile\\n\\nLog out\\n\\nClose\\n\\nProfile\\n\\nYour profile helps improve your interactions with select AWS experiences.\\n\\nView profile\\n\\nLog out\\n\\nGet Started for Free\\n\\nContact Us\\n\\n  * Products\\n  * Solutions\\n  * Pricing\\n  * Introduction to AWS\\n  * Getting Started\\n  * Documentation\\n  * Training and Certification\\n  * Developer Center\\n  * Customer Success\\n  * Partner Network\\n  * AWS Marketplace\\n  * Support\\n  * AWS re:Post\\n  * Log into Console\\n  * Download the Mobile App\\n\\n  * What is Cloud Computing?\\n  * Cloud Computing Concepts Hub\\n  * Generative AI\\n\\n# What is LLM (Large Language Model)?\\n\\n  \\n\\nCreate an AWS Account\\n\\nExplore Generative AI Services\\n\\nBuild, deploy, and run generative AI applications on AWS\\n\\nCheck out Generative AI on AWS\\n\\nInnovate faster with the most comprehensive set of Generative AI services\\n\\nBrowse Generative AI Trainings\\n\\nGet started on generative AI training with content built by AWS experts\\n\\nRead Generative AI Blogs\\n\\nGet the latest AWS generative AI product news and best practices\\n\\nWhat are Large Language Models? Why are large language models important? How\\ndo large language models work? What are applications of large language models?\\nHow are large language models trained? What is the future of LLMs? How can AWS\\nhelp with LLMs?\\n\\n## What are Large Language Models?\\n\\nLarge language models, also known as LLMs, are very large deep learning models\\nthat are pre-trained on vast amounts of data. The underlying transformer is a\\nset of neural networks that consist of an encoder and a decoder with self-\\nattention capabilities. The encoder and decoder extract meanings from a\\nsequence of text and understand the relationships between words and phrases in\\nit.\\n\\nTransformer LLMs are capable of unsupervised training, although a more precise\\nexplanation is that transformers perform self-learning. It is through this\\nprocess that transformers learn to understand basic grammar, languages, and\\nknowledge.\\n\\nUnlike earlier recurrent neural networks (RNN) that sequentially process\\ninputs, transformers process entire sequences in parallel. This allows the\\ndata scientists to use GPUs for training transformer-based LLMs, significantly\\nreducing the training time.\\n\\nTransformer neural network architecture allows the use of very large models,\\noften with hundreds of billions of parameters. Such large-scale models can\\ningest massive amounts of data, often from the internet, but also from sources\\nsuch as the Common Crawl, which comprises more than 50 billion web pages, and\\nWikipedia, which has approximately 57 million pages.\\n\\nRead more about neural networks »\\n\\nRead more about deep learning »\\n\\n## Why are large language models important?\\n\\nLarge language models are incredibly flexible. One model can perform\\ncompletely different tasks such as answering questions, summarizing documents,\\ntranslating languages and completing sentences. LLMs have the potential to\\ndisrupt content creation and the way people use search engines and virtual\\nassistants.\\n\\nWhile not perfect, LLMs are demonstrating a remarkable ability to make\\npredictions based on a relatively small number of prompts or inputs. LLMs can\\nbe used for generative AI (artificial intelligence) to produce content based\\non input prompts in human language.\\n\\nLLMs are big, very big. They can consider billions of parameters and have many\\npossible uses. Here are some examples:\\n\\n  * Open AI's GPT-3 model has 175 billion parameters. Its cousin, ChatGPT, can identify patterns from data and generate natural and readable output. While we don’t know the size of Claude 2, it can take inputs up to 100K tokens in each prompt, which means it can work over hundreds of pages of technical documentation or even an entire book.\\n  * AI21 Labs’ Jurassic-1 model has 178 billion parameters and a token vocabulary of 250,000-word parts and similar conversational capabilities.\\n  * Cohere’s Command model has similar capabilities and can work in more than 100 different languages.\\n  * LightOn's Paradigm offers foundation models with claimed capabilities that exceed those of GPT-3. All these LLMs come with APIs that allow developers to create unique generative AI applications.\\n\\nRead more about generative AI »\\n\\nRead more about foundation models »\\n\\n## How do large language models work?\\n\\nA key factor in how LLMs work is the way they represent words. Earlier forms\\nof machine learning used a numerical table to represent each word. But, this\\nform of representation could not recognize relationships between words such as\\nwords with similar meanings. This limitation was overcome by using multi-\\ndimensional vectors, commonly referred to as word embeddings, to represent\\nwords so that words with similar contextual meanings or other relationships\\nare close to each other in the vector space.\\n\\nUsing word embeddings, transformers can pre-process text as numerical\\nrepresentations through the encoder and understand the context of words and\\nphrases with similar meanings as well as other relationships between words\\nsuch as parts of speech. It is then possible for LLMs to apply this knowledge\\nof the language through the decoder to produce a unique output.\\n\\n## What are applications of large language models?\\n\\nThere are many practical applications for LLMs.\\n\\n### Copywriting\\n\\nApart from GPT-3 and ChatGPT, Claude, Llama 2, Cohere Command, and Jurassiccan\\nwrite original copy. AI21 Wordspice suggests changes to original sentences to\\nimprove style and voice.\\n\\n### Knowledge base answering\\n\\nOften referred to as knowledge-intensive natural language processing (KI-NLP),\\nthe technique refers to LLMs that can answer specific questions from\\ninformation help in digital archives. An example is the ability of AI21 Studio\\nplayground to answer general knowledge questions.\\n\\n### Text classification\\n\\nUsing clustering, LLMs can classify text with similar meanings or sentiments.\\nUses include measuring customer sentiment, determining the relationship\\nbetween texts, and document search.\\n\\n### Code generation\\n\\nLLM are proficient in code generation from natural language prompts. Examples\\ninclude Amazon CodeWhisperer and Open AI's codex used in GitHub Copilot, which\\ncan code in Python, JavaScript, Ruby and several other programming languages.\\nOther coding applications include creating SQL queries, writing shell commands\\nand website design. Learn more about AI code generation.\\n\\n### Text generation\\n\\nSimilar to code generation, text generation can complete incomplete sentences,\\nwrite product documentation or, like Alexa Create, write a short children's\\nstory.\\n\\n## How are large language models trained?\\n\\nTransformer-based neural networks are very large. These networks contain\\nmultiple nodes and layers. Each node in a layer has connections to all nodes\\nin the subsequent layer, each of which has a weight and a bias. Weights and\\nbiases along with embeddings are known as model parameters. Large transformer-\\nbased neural networks can have billions and billions of parameters. The size\\nof the model is generally determined by an empirical relationship between the\\nmodel size, the number of parameters, and the size of the training data.\\n\\nTraining is performed using a large corpus of high-quality data. During\\ntraining, the model iteratively adjusts parameter values until the model\\ncorrectly predicts the next token from an the previous squence of input\\ntokens. It does this through self-learning techniques which teach the model to\\nadjust parameters to maximize the likelihood of the next tokens in the\\ntraining examples.\\n\\nOnce trained, LLMs can be readily adapted to perform multiple tasks using\\nrelatively small sets of supervised data, a process known as fine tuning.\\n\\nThree common learning models exist:\\n\\n  * Zero-shot learning; Base LLMs can respond to a broad range of requests without explicit training, often through prompts, although answer accuracy varies.\\n  * Few-shot learning: By providing a few relevant training examples, base model performance significantly improves in that specific area.\\n  * Fine-tuning: This is an extension of few-shot learning in that data scientists train a base model to adjust its parameters with additional data relevant to the specific application.\\n\\n## What is the future of LLMs?\\n\\nThe introduction of large language models like ChatGPT, Claude 2, and Llama 2\\nthat can answer questions and generate text points to exciting possibilities\\nin the future. Slowly, but surely, LLMs are moving closer to human-like\\nperformance. The immediate success of these LLMs demonstrates a keen interest\\nin robotic-type LLMs that emulate and, in some contexts, outperform the human\\nbrain. Here are some thoughts on the future of LLMs,\\n\\n### Increased capabilities\\n\\nAs impressive as they are, the current level of technology is not perfect and\\nLLMs are not infallible. However, newer releases will have improved accuracy\\nand enhanced capabilities as developers learn how to improve their performance\\nwhile reducing bias and eliminating incorrect answers.\\n\\n### Audiovisual training\\n\\nWhile developers train most LLMs using text, some have started training models\\nusing video and audio input. This form of training should lead to faster model\\ndevelopment and open up new possibilities in terms of using LLMs for\\nautonomous vehicles.\\n\\n### Workplace transformation\\n\\nLLMs are a disruptive factor that will change the workplace. LLMs will likely\\nreduce monotonous and repetitive tasks in the same way that robots did for\\nrepetitive manufacturing tasks. Possibilities include repetitive clerical\\ntasks, customer service chatbots, and simple automated copywriting.\\n\\n### Conversational AI\\n\\nLLMs will undoubtedly improve the performance of automated virtual assistants\\nlike Alexa, Google Assistant, and Siri. They will be better able to interpret\\nuser intent and respond to sophisticated commands.\\n\\nRead more about conversational AI\\n\\n## How can AWS help with LLMs?\\n\\nAWS offers several possibilities for large language model developers. Amazon\\nBedrock is the easiest way to build and scale generative AI applications with\\nLLMs. Amazon Bedrock is a fully managed service that makes LLMs from Amazon\\nand leading AI startups available through an API, so you can choose from\\nvarious LLMs to find the model that's best suited for your use case.\\n\\nAmazon SageMaker JumpStart __ is a machine learning hub with foundation\\nmodels, built-in algorithms, and prebuilt ML solutions that you can deploy\\nwith just a few clicks With SageMaker JumpStart, you can access pretrained\\nmodels, including foundation models, to perform tasks like article\\nsummarization and image generation. Pretrained models are fully customizable\\nfor your use case with your data, and you can easily deploy them into\\nproduction with the user interface or SDK.\\n\\nGet started with LLMs and AI on AWS by creating a free account today.\\n\\n##  Next Steps on AWS\\n\\nCheck out additional product-related resources\\n\\nInnovate faster with AWS generative AI services  __\\n\\nSign up for a free account\\n\\nInstant get access to the AWS Free Tier.\\n\\nSign up  __\\n\\nStart building in the console\\n\\nGet started building in the AWS management console.\\n\\nSign in  __\\n\\nSign In to the Console\\n\\n###  Learn About AWS\\n\\n  * What Is AWS?\\n  * What Is Cloud Computing?\\n  * AWS Accessibility\\n  * What Is DevOps?\\n  * What Is a Container?\\n  * What Is a Data Lake?\\n  * What is Artificial Intelligence (AI)?\\n  * What is Generative AI?\\n  * What is Machine Learning (ML)?\\n  * AWS Cloud Security\\n  * What's New\\n  * Blogs\\n  * Press Releases\\n\\n###  Resources for AWS\\n\\n  * Getting Started\\n  * Training and Certification\\n  * AWS Trust Center\\n  * AWS Solutions Library\\n  * Architecture Center\\n  * Product and Technical FAQs\\n  * Analyst Reports\\n  * AWS Partners\\n\\n###  Developers on AWS\\n\\n  * Developer Center\\n  * SDKs & Tools\\n  * .NET on AWS\\n  * Python on AWS\\n  * Java on AWS\\n  * PHP on AWS\\n  * JavaScript on AWS\\n\\n###  Help\\n\\n  * Contact Us\\n  * Get Expert Help\\n  * File a Support Ticket\\n  * AWS re:Post\\n  * Knowledge Center\\n  * AWS Support Overview\\n  * Legal\\n  * AWS Careers\\n\\nCreate an AWS Account\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\n__\\n\\nAmazon is an Equal Opportunity Employer: _Minority / Women / Disability /\\nVeteran / Gender Identity / Sexual Orientation / Age._\\n\\n  * Language\\n  * عربي\\n  * Bahasa Indonesia\\n  * Deutsch\\n  * English\\n  * Español\\n  * Français\\n  * Italiano\\n  * Português\\n  * Tiếng Việt\\n  * Türkçe\\n  * Ρусский\\n  * ไทย\\n  * 日本語\\n  * 한국어\\n  * 中文 (简体)\\n  * 中文 (繁體)\\n\\n  * Privacy\\n  * |\\n  * Accessibility\\n  * |\\n  * Site Terms\\n  * |\\n  * Cookie Preferences \\n  * |\\n  * © 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\\n\\n####  Ending Support for Internet Explorer\\n\\nGot it\\n\\nAWS support for Internet Explorer ends on 07/31/2022. Supported browsers are\\nChrome, Firefox, Edge, and Safari. Learn more »\\n\\nGot it\\n\\n\"}\n",
      "Refining :: https://aws.amazon.com/what-is/large-language-model/\n",
      "-------------------------------\n",
      "{'url': 'https://www.elastic.co/what-is/large-language-models', 'markdown': 'Skip to main content\\n\\nElastic\\n\\nEN\\n\\nClose panel\\n\\n  * Deutsch\\n  * English\\n  * Español\\n  * Français\\n  * 日本語\\n  * 한국어\\n  * 简体中文\\n  * Português\\n\\nSearch\\n\\nLogin\\n\\nToggle Navigation\\n\\nStart free trialContact Sales\\n\\nPlatform\\n\\nClose panel\\n\\n## The Search AI Company\\n\\nBuild tailored experiences with Elastic.\\n\\nElastic Search AI Platform overview\\n\\n* * *\\n\\n**Scale your business with Elastic Partners**\\n\\n  * Find a partner\\n  * Become a partner\\n\\nPartner overview\\n\\n## ELK Stack\\n\\nSearch and analytics, data ingestion, and visualization – all at your\\nfingertips.\\n\\n  * Kibana\\n  * Elasticsearch\\n  * Integrations\\n\\nELK Stack overview\\n\\n* * *\\n\\n**By developers, for developers**\\n\\n  * Try the world\\'s most used vector database\\n  * Scale with the low-latency Search AI Lake\\n  * Join our community\\n\\n## Elastic Cloud\\n\\nUnlock the power of real-time insights with Elastic on your preferred cloud\\nprovider.\\n\\nElastic Cloud overview\\n\\n* * *\\n\\n  * Elastic Cloud Serverless\\n  * Elastic Cloud Serverless pricing\\n  * Search AI Lake\\n\\nSolutions\\n\\nClose panel\\n\\n## Generative AI\\n\\nPrototype and integrate with LLMs faster using search AI.\\n\\n  * Search AI Lake\\n  * Elastic AI Assistant\\n  * Retrieval Augmented Generation\\n\\n* * *\\n\\n  * Generative AI blogs\\n  * Search Labs tutorials\\n  * Elastic Community\\n\\nGenerative AI overview\\n\\n## Search\\n\\nDiscover a world of AI possibilities — built with the power of search.\\n\\n  * Vector database\\n  * Relevance\\n  * Search applications\\n  * Ecommerce\\n  * Website search\\n  * Workplace search\\n  * Customer support\\n\\n* * *\\n\\n  * Search Labs\\n\\nSearch overview\\n\\n## Security\\n\\nProtect, investigate, and respond to cyber threats with AI-driven security\\nanalytics.\\n\\n  * SIEM\\n  * AI for the SOC\\n  * Threat Research\\n\\n* * *\\n\\n  * Security Labs\\n\\nSecurity overview\\n\\n## Observability\\n\\nUnify app and infrastructure visibility to proactively resolve issues.\\n\\n  * Log monitoring and analytics\\n  * OpenTelemetry\\n  * Application performance monitoring\\n  * Infrastructure monitoring\\n  * Synthetic monitoring\\n  * Real user monitoring\\n  * Universal Profiling\\n  * AIOps\\n\\n* * *\\n\\n  * Observability Labs\\n\\nObservability overview\\n\\nCustomers\\n\\nClose panel\\n\\n## By solution\\n\\nSee how customers search, solve, and succeed — all on one Search AI Platform.\\n\\n  * Search\\n  * Security\\n  * Observability\\n\\nAll customer stories\\n\\n## Industries\\n\\nExceed customer expectations and go to market faster.\\n\\n  * Public sector\\n  * Financial services\\n  * Telecommunications\\n  * Retail\\n  * Manufacturing\\n\\nIndustries overview\\n\\n## Customer spotlight\\n\\nCisco saves 5,000 support engineer hours per month\\n\\nRead more\\n\\nSitecore automates 96 percent of security workflows with Elastic\\n\\nRead more\\n\\nComcast transforms customer experiences with Elastic Observability\\n\\nRead more\\n\\nResources\\n\\nClose panel\\n\\n## Research\\n\\nStay at the forefront of innovation with technical tips from the experts.\\n\\n  * Search Labs\\n  * Security Labs\\n  * Observability Labs\\n\\n## Build\\n\\nCode with other developers to create a better Elastic, together.\\n\\n  * Community\\n  * Forum\\n  * Downloads\\n  * Documentation\\n\\n## Learn\\n\\nUnleash the possibilities of your data and grow your skill set.\\n\\n  * Getting started\\n  * Elastic resources\\n  * Consulting services\\n  * Trainings & certifications\\n\\n## Connect\\n\\nKeep informed about the latest tech and news from Elastic.\\n\\n  * Blog\\n  * Events\\n\\n* * *\\n\\nHave questions?\\n\\n  * Contact sales\\n  * Get support\\n\\nPricingDocs\\n\\nNew\\n\\nRead more\\n\\nAbout usPartnersSupport|Login\\n\\n  * Products\\n  * Solutions\\n  * Why Elastic?\\n  * Resources\\nPricingDocs\\n\\nSearch\\n\\nStart free trialContact sales\\n\\n# What are large language models (LLMs)?\\n\\nExperiment with RAG using AI Playground\\n\\n## Large language model definition\\n\\n**A large language model (LLM) is a deep learning algorithm that can perform a\\nvariety of****natural language processing (NLP)****tasks.** Large language\\nmodels use transformer models and are trained using massive datasets — hence,\\nlarge. This enables them to recognize, translate, predict, or generate text or\\nother content.\\n\\nLarge language models are also referred to as neural networks (NNs), which are\\ncomputing systems inspired by the human brain. These neural networks work\\nusing a network of nodes that are layered, much like neurons.\\n\\nIn addition to teaching human languages to artificial intelligence (AI)\\napplications, large language models can also be trained to perform a variety\\nof tasks like understanding protein structures, writing software code, and\\nmore. Like the human brain, large language models must be pre-trained and then\\nfine-tuned so that they can solve text classification, question answering,\\ndocument summarization, and text generation problems. Their problem-solving\\ncapabilities can be applied to fields like healthcare, finance, and\\nentertainment where large language models serve a variety of NLP applications,\\nsuch as translation, chatbots, AI assistants, and so on.\\n\\nLarge language models also have large numbers of parameters, which are akin to\\nmemories the model collects as it learns from training. Think of these\\nparameters as the model\\'s knowledge bank.\\n\\n**Watch this video and take a deeper dive into LLMs.**\\n\\n* * *\\n\\n## So, what is a transformer model?\\n\\nA **transformer model** is the most common architecture of a large language\\nmodel. It consists of an encoder and a decoder. A transformer model processes\\ndata by tokenizing the input, then simultaneously conducting mathematical\\nequations to discover relationships between tokens. This enables the computer\\nto see the patterns a human would see were it given the same query.\\n\\nTransformer models work with self-attention mechanisms, which enables the\\nmodel to learn more quickly than traditional models like long short-term\\nmemory models. Self-attention is what enables the transformer model to\\nconsider different parts of the sequence, or the entire context of a sentence,\\nto generate predictions.\\n\\n**Related:** Apply transformers to your search applications\\n\\n* * *\\n\\n## Key components of large language models\\n\\nLarge language models are composed of multiple neural network layers.\\nRecurrent layers, feedforward layers, embedding layers, and attention layers\\nwork in tandem to process the input text and generate output content.\\n\\n**The embedding layer** creates embeddings from the input text. This part of\\nthe large language model captures the semantic and syntactic meaning of the\\ninput, so the model can understand context.\\n\\n**The feedforward layer (FFN)** of a large language model is made of up\\nmultiple fully connected layers that transform the input embeddings. In so\\ndoing, these layers enable the model to glean higher-level abstractions — that\\nis, to understand the user\\'s intent with the text input.\\n\\n**The recurrent layer** interprets the words in the input text in sequence. It\\ncaptures the relationship between words in a sentence.\\n\\n**The attention mechanism** enables a language model to focus on single parts\\nof the input text that is relevant to the task at hand. This layer allows the\\nmodel to generate the most accurate outputs.\\n\\nThere are three main kinds of large language models:\\n\\n  * **Generic or raw language models** predict the next word based on the language in the training data. These language models perform information retrieval tasks.\\n  * **Instruction-tuned language models** are trained to predict responses to the instructions given in the input. This allows them to perform sentiment analysis, or to generate text or code.\\n  * **Dialog-tuned language models** are trained to have a dialog by predicting the next response. Think of chatbots or conversational AI.\\n\\n* * *\\n\\n## What is the difference between large language models and generative AI?\\n\\nGenerative AI is an umbrella term that refers to artificial intelligence\\nmodels that have the capability to generate content. Generative AI can\\ngenerate text, code, images, video, and music. Examples of generative AI\\ninclude Midjourney, DALL-E, and ChatGPT.\\n\\nLarge language models are a type of generative AI that are trained on text and\\nproduce textual content. ChatGPT is a popular example of generative text AI.\\n\\nAll large language models are generative AI1.\\n\\n* * *\\n\\n## How do large language models work?\\n\\nA large language model is based on a transformer model and works by receiving\\nan input, encoding it, and then decoding it to produce an output prediction.\\nBut before a large language model can receive text input and generate an\\noutput prediction, it requires training, so that it can fulfill general\\nfunctions, and fine-tuning, which enables it to perform specific tasks.\\n\\n**Training:** Large language models are pre-trained using large textual\\ndatasets from sites like Wikipedia, GitHub, or others. These datasets consist\\nof trillions of words, and their quality will affect the language model\\'s\\nperformance. At this stage, the large language model engages in unsupervised\\nlearning, meaning it processes the datasets fed to it without specific\\ninstructions. During this process, the LLM\\'s AI algorithm can learn the\\nmeaning of words, and of the relationships between words. It also learns to\\ndistinguish words based on context. For example, it would learn to understand\\nwhether \"right\" means \"correct,\" or the opposite of \"left.\"\\n\\n**Fine-tuning:** In order for a large language model to perform a specific\\ntask, such as translation, it must be fine-tuned to that particular activity.\\nFine-tuning optimizes the performance of specific tasks.\\n\\n**Prompt-tuning** fulfills a similar function to fine-tuning, whereby it\\ntrains a model to perform a specific task through few-shot prompting, or zero-\\nshot prompting. A prompt is an instruction given to an LLM. Few-shot prompting\\nteaches the model to predict outputs through the use of examples. For\\ninstance, in this sentiment analysis exercise, a few-shot prompt would look\\nlike this:\\n\\n    \\n    \\n    Customer review: This plant is so beautiful!  \\n    Customer sentiment: positive  \\n      \\n    Customer review: This plant is so hideous!  \\n    Customer sentiment: negative  \\n    \\n\\nThe language model would understand, through the semantic meaning of\\n\"hideous,\" and because an opposite example was provided, that the customer\\nsentiment in the second example is \"negative.\"\\n\\nAlternatively, zero-shot prompting does not use examples to teach the language\\nmodel how to respond to inputs. Instead, it formulates the question as \"The\\nsentiment in ‘This plant is so hideous\\' is….\" It clearly indicates which task\\nthe language model should perform, but does not provide problem-solving\\nexamples.\\n\\n* * *\\n\\n## Large language models use cases\\n\\nLarge language models can be used for several purposes:\\n\\n  * **Information retrieval:****** Think of Bing or Google. Whenever you use their search feature, you are relying on a large language model to produce information in response to a query. It\\'s able to retrieve information, then summarize and communicate the answer in a conversational style.\\n  * **Sentiment analysis****:** As applications of natural language processing, large language models enable companies to analyze the sentiment of textual data.\\n  * **Text generation:** Large language models are behind generative AI, like ChatGPT, and can generate text based on inputs. They can produce an example of text when prompted. For example: \"Write me a poem about palm trees in the style of Emily Dickinson.\"\\n  * **Code generation:** Like text generation, code generation is an application of generative AI. LLMs understand patterns, which enables them to generate code.\\n  * **Chatbots and conversational AI:** Large language models enable customer service chatbots or conversational AI to engage with customers, interpret the meaning of their queries or responses, and offer responses in turn.\\n\\nRelated: How to make a chatbot: Dos and don\\'ts for developers\\n\\nIn addition to these use cases, large language models can complete sentences,\\nanswer questions, and summarize text.\\n\\nWith such a wide variety of applications, large language applications can be\\nfound in a multitude of fields:\\n\\n  * **Tech:** Large language models are used anywhere from enabling search engines to respond to queries, to assisting developers with writing code.\\n  * **Healthcare and Science:** Large language models have the ability to understand proteins, molecules, DNA, and RNA. This position allows LLMs to assist in the development of vaccines, finding cures for illnesses, and improving preventative care medicines. LLMs are also used as medical chatbots to perform patient intakes or basic diagnoses.\\n  * **Customer Service:** LLMs are used across industries for customer service purposes such as chatbots or conversational AI.\\n  * **Marketing:** Marketing teams can use LLMs to perform sentiment analysis to quickly generate campaign ideas or text as pitching examples, and much more.\\n  * **Legal:** From searching through massive textual datasets to generating legalese, large language models can assist lawyers, paralegals, and legal staff.\\n  * **Banking:** LLMs can support credit card companies in detecting fraud.\\n\\nGet started with Generative AI in Enterprise. Watch this webinar and explore\\nthe challenges and opportunities of generative AI in your enterprise\\nenvironment.\\n\\n* * *\\n\\n## Benefits of large language models\\n\\nWith a broad range of applications, large language models are exceptionally\\nbeneficial for problem-solving since they provide information in a clear,\\nconversational style that is easy for users to understand.\\n\\n**Large set of applications:** They can be used for language translation,\\nsentence completion, sentiment analysis, question answering, mathematical\\nequations, and more.\\n\\n**Always improving:** Large language model performance is continually\\nimproving because it grows when more data and parameters are added. In other\\nwords, the more it learns, the better it gets. What’s more, large language\\nmodels can exhibit what is called \"in-context learning.\" Once an LLM has been\\npretrained, few-shot prompting enables the model to learn from the prompt\\nwithout any additional parameters. In this way, it is continually learning.\\n\\n**They learn fast:** When demonstrating in-context learning, large language\\nmodels learn quickly because they do not require additional weight, resources,\\nand parameters for training. It is fast in the sense that it doesn’t require\\ntoo many examples.\\n\\n* * *\\n\\n## Limitations and challenges of LLMs\\n\\nLarge language models might give us the impression that they understand\\nmeaning and can respond to it accurately. However, they remain a technological\\ntool and as such, large language models face a variety of challenges.\\n\\n**Hallucinations:** A hallucination is when a LLM produces an output that is\\nfalse, or that does not match the user\\'s intent. For example, claiming that it\\nis human, that it has emotions, or that it is in love with the user. Because\\nlarge language models predict the next syntactically correct word or phrase,\\nthey can\\'t wholly interpret human meaning. The result can sometimes be what is\\nreferred to as a \"hallucination.\"\\n\\n**Security:** Large language models present important security risks when not\\nmanaged or surveilled properly. They can leak people\\'s private information,\\nparticipate in phishing scams, and produce spam. Users with malicious intent\\ncan reprogram AI to their ideologies or biases, and contribute to the spread\\nof misinformation. The repercussions can be devastating on a global scale.\\n\\n**Bias:** The data used to train language models will affect the outputs a\\ngiven model produces. As such, if the data represents a single demographic, or\\nlacks diversity, the outputs produced by the large language model will also\\nlack diversity.\\n\\n**Consent:** Large language models are trained on trillions of datasets — some\\nof which might not have been obtained consensually. When scraping data from\\nthe internet, large language models have been known to ignore copyright\\nlicenses, plagiarize written content, and repurpose proprietary content\\nwithout getting permission from the original owners or artists. When it\\nproduces results, there is no way to track data lineage, and often no credit\\nis given to the creators, which can expose users to copyright infringement\\nissues.\\n\\nThey might also scrape personal data, like names of subjects or photographers\\nfrom the descriptions of photos, which can compromise privacy.2 LLMs have\\nalready run into lawsuits, including a prominent one by Getty Images3, for\\nviolating intellectual property.\\n\\n**Scaling:** It can be difficult and time- and resource-consuming to scale and\\nmaintain large language models.\\n\\n**Deployment:** Deploying large language models requires deep learning, a\\ntransformer model, distributed software and hardware, and overall technical\\nexpertise.\\n\\n* * *\\n\\n## Examples of popular large language models\\n\\nPopular large language models have taken the world by storm. Many have been\\nadopted by people across industries. You\\'ve no doubt heard of ChatGPT, a form\\nof generative AI chatbot.\\n\\nOther popular LLM models include:\\n\\n  * **PaLM:** Google\\'s Pathways Language Model (PaLM) is a transformer language model capable of common-sense and arithmetic reasoning, joke explanation, code generation, and translation.\\n  * **BERT:** The Bidirectional Encoder Representations from Transformers (BERT) language model was also developed at Google. It is a transformer-based model that can understand natural language and answer questions.\\n  * **XLNet:** A permutation language model, XLNet generated output predictions in a random order, which distinguishes it from BERT. It assesses the pattern of tokens encoded and then predicts tokens in random order, instead of a sequential order.\\n  * **GPT:** Generative pre-trained transformers are perhaps the best-known large language models. Developed by OpenAI, GPT is a popular foundational model whose numbered iterations are improvements on their predecessors (GPT-3, GPT-4, etc.). It can be fine-tuned to perform specific tasks downstream. Examples of this are EinsteinGPT, developed by Salesforce for CRM, and Bloomberg\\'s BloombergGPT for finance.\\n\\nRelated: 2024 getting started guide to open-source LLMs\\n\\n* * *\\n\\n## Future advancements in large language models\\n\\nThe arrival of ChatGPT has brought large language models to the fore and\\nactivated speculation and heated debate on what the future might look like.\\n\\nAs large language models continue to grow and improve their command of natural\\nlanguage, there is much concern regarding what their advancement would do to\\nthe job market. It\\'s clear that large language models will develop the ability\\nto replace workers in certain fields.\\n\\nIn the right hands, large language models have the ability to increase\\nproductivity and process efficiency, but this has posed ethical questions for\\nits use in human society.\\n\\nRelated: 2024 open-source LLMs guide\\n\\n* * *\\n\\n## Introducing the Elasticsearch Relevance Engine\\n\\nTo address the current limitations of LLMs, the Elasticsearch Relevance Engine\\n(ESRE) is a relevance engine built for artificial intelligence-powered search\\napplications. With ESRE, developers are empowered to build their own semantic\\nsearch application, utilize their own transformer models, and combine NLP and\\ngenerative AI to enhance their customers\\' search experience.\\n\\nSupercharge your relevance with the Elasticsearch Relevance Engine\\n\\n* * *\\n\\n## Explore more large language model resources\\n\\n  * Elastic generative AI tools and capabilities\\n  * How to choose a vector database\\n  * How to make a chatbot: Dos and don\\'ts for developers\\n  * Choosing an LLM: The 2024 getting started guide to open-source LLMs\\n  * Language models in Elasticsearch\\n  * 2024 technical trends: How search and generative AI are evolving\\n  * Overview of Natural language processing (NLP) in the Elastic Stack\\n  * Compatible third-party models with the Elastic Stack\\n  * Guide to trained models in the Elastic Stack\\n  * The LLM Safety Assessment\\n\\n* * *\\n\\n## Footnotes\\n\\n1 Myer, Mike. “Are Generative AI and Large Language Models the Same Thing?”\\nQuiq, 12 May 2023, quiq.com/blog/generative-ai-vs-large-language-models/.\\n\\n2 Sheng, Ellen. “In generative AI legal Wild West, the courtroom battles are\\njust getting started,” CNBC, April 3, 2023,\\nhttps://www.cnbc.com/2023/04/03/in-generative-ai-legal-wild-west-lawsuits-are-\\njust-getting-started.html (Accessed June 29, 2023)\\n\\n3 Getty Images Statement, Getty Images, Jan 17 2023\\nhttps://newsroom.gettyimages.com/en/getty-images/getty-images-statement\\n(Accessed June 29, 2023)\\n\\n* * *\\n\\n## Follow us\\n\\n  *   *   *   *   * \\n\\n  * ## About us\\n\\nAbout ElasticLeadershipDE&IBlogNewsroom\\n\\n  * ## Join us\\n\\nCareersCareer portalHow we hire\\n\\n  * ## Partners\\n\\nFind a partnerPartner loginRequest accessBecome a partner\\n\\n  * ## Trust & Security\\n\\nTrust centerEthicsPoint portalECCN reportEthics email\\n\\n  * ## Investor relations\\n\\nInvestor resourcesGovernanceFinancialsStock\\n\\n  * ## Excellence Awards\\n\\nPrevious winnersElasticON TourBecome a sponsorAll events\\n\\n## About us\\n\\nAbout ElasticLeadershipDE&IBlogNewsroom\\n\\n## Join us\\n\\nCareersCareer portalHow we hire\\n\\n## Partners\\n\\nFind a partnerPartner loginRequest accessBecome a partner\\n\\n## Trust & Security\\n\\nTrust centerEthicsPoint portalECCN reportEthics email\\n\\n## Investor relations\\n\\nInvestor resourcesGovernanceFinancialsStock\\n\\n## Excellence Awards\\n\\nPrevious winnersElasticON TourBecome a sponsorAll events\\n\\n  * Trademarks\\n  * Terms of Use\\n  * Privacy\\n  * Sitemap\\n\\n© . Elasticsearch B.V. All Rights Reserved\\n\\nElastic, Elasticsearch and other related marks are trademarks, logos or\\nregistered trademarks of Elasticsearch B.V. in the United States and other\\ncountries.\\n\\nApache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant\\nlogo are trademarks of the Apache Software Foundation in the United States\\nand/or other countries. All other brand names, product names, or trademarks\\nbelong to their respective owners.\\n\\n'}\n",
      "Refining :: https://www.elastic.co/what-is/large-language-models\n",
      "-------------------------------\n",
      "{'url': 'https://en.wikipedia.org/wiki/Large_language_model', 'markdown': 'Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nmove to sidebar hide\\n\\nNavigation\\n\\n  * Main page\\n  * Contents\\n  * Current events\\n  * Random article\\n  * About Wikipedia\\n  * Contact us\\n\\nContribute\\n\\n  * Help\\n  * Learn to edit\\n  * Community portal\\n  * Recent changes\\n  * Upload file\\n  * Special pages\\n\\nSearch\\n\\nSearch\\n\\nAppearance\\n\\n  * Donate\\n  * Create account\\n  * Log in\\n\\nPersonal tools\\n\\n  * Donate\\n  * Create account\\n  * Log in\\n\\nPages for logged out editors learn more\\n\\n  * Contributions\\n  * Talk\\n\\n## Contents\\n\\nmove to sidebar hide\\n\\n  * (Top)\\n\\n  * 1 History\\n\\n  * 2 Dataset preprocessing\\n\\nToggle Dataset preprocessing subsection\\n\\n    * 2.1 Tokenization\\n\\n      * 2.1.1 BPE\\n\\n      * 2.1.2 Problems\\n\\n    * 2.2 Dataset cleaning\\n\\n    * 2.3 Synthetic data\\n\\n  * 3 Training and architecture\\n\\nToggle Training and architecture subsection\\n\\n    * 3.1 Reinforcement learning from human feedback\\n\\n    * 3.2 Instruction tuning\\n\\n    * 3.3 Mixture of experts\\n\\n    * 3.4 Prompt engineering, attention mechanism, and context window\\n\\n    * 3.5 Infrastructure\\n\\n  * 4 Training cost\\n\\n  * 5 Tool use\\n\\n  * 6 Agency\\n\\n  * 7 Compression\\n\\n  * 8 Multimodality\\n\\n  * 9 Reasoning\\n\\n  * 10 Properties\\n\\nToggle Properties subsection\\n\\n    * 10.1 Scaling laws\\n\\n    * 10.2 Emergent abilities\\n\\n  * 11 Interpretation\\n\\nToggle Interpretation subsection\\n\\n    * 11.1 Understanding and intelligence\\n\\n  * 12 Evaluation\\n\\nToggle Evaluation subsection\\n\\n    * 12.1 Perplexity\\n\\n      * 12.1.1 BPW, BPC, and BPT\\n\\n    * 12.2 Task-specific datasets and benchmarks\\n\\n      * 12.2.1 Adversarially constructed evaluations\\n\\n      * 12.2.2 Limitations of LLM benchmarks\\n\\n  * 13 Wider impact\\n\\nToggle Wider impact subsection\\n\\n    * 13.1 Memorization and copyright\\n\\n    * 13.2 Security\\n\\n    * 13.3 Algorithmic bias\\n\\n      * 13.3.1 Stereotyping\\n\\n      * 13.3.2 Selection bias\\n\\n      * 13.3.3 Political bias\\n\\n    * 13.4 Energy demands\\n\\n  * 14 See also\\n\\n  * 15 References\\n\\n  * 16 Further reading\\n\\nToggle the table of contents\\n\\n# Large language model\\n\\n54 languages\\n\\n  * Afrikaans\\n  * العربية\\n  * Aragonés\\n  * Azərbaycanca\\n  * বাংলা\\n  * 閩南語 / Bân-lâm-gú\\n  * Boarisch\\n  * Bosanski\\n  * Català\\n  * Čeština\\n  * Dansk\\n  * Deutsch\\n  * Ελληνικά\\n  * Español\\n  * Esperanto\\n  * Euskara\\n  * فارسی\\n  * Français\\n  * Gaeilge\\n  * Galego\\n  * 한국어\\n  * हिन्दी\\n  * Ido\\n  * Bahasa Indonesia\\n  * IsiZulu\\n  * Italiano\\n  * עברית\\n  * Қазақша\\n  * Magyar\\n  * Македонски\\n  * Монгол\\n  * Nederlands\\n  * 日本語\\n  * Polski\\n  * Português\\n  * Qaraqalpaqsha\\n  * Română\\n  * Runa Simi\\n  * Русский\\n  * Shqip\\n  * Simple English\\n  * Slovenščina\\n  * کوردی\\n  * Српски / srpski\\n  * Tagalog\\n  * తెలుగు\\n  * ไทย\\n  * Türkçe\\n  * Українська\\n  * ئۇيغۇرچە / Uyghurche\\n  * Tiếng Việt\\n  * 文言\\n  * 粵語\\n  * 中文\\n\\nEdit links\\n\\n  * Article\\n  * Talk\\n\\nEnglish\\n\\n  * Read\\n  * Edit\\n  * View history\\n\\nTools\\n\\nTools\\n\\nmove to sidebar hide\\n\\nActions\\n\\n  * Read\\n  * Edit\\n  * View history\\n\\nGeneral\\n\\n  * What links here\\n  * Related changes\\n  * Upload file\\n  * Permanent link\\n  * Page information\\n  * Cite this page\\n  * Get shortened URL\\n  * Download QR code\\n\\nPrint/export\\n\\n  * Download as PDF\\n  * Printable version\\n\\nIn other projects\\n\\n  * Wikimedia Commons\\n  * Wikidata item\\n\\nAppearance\\n\\nmove to sidebar hide\\n\\nFrom Wikipedia, the free encyclopedia\\n\\nType of machine learning model\\n\\nNot to be confused with Logic learning machine.\\n\\nPart of a series on  \\n---  \\nMachine learning  \\nand data mining  \\nParadigms\\n\\n  * Supervised learning\\n  * Unsupervised learning\\n  * Semi-supervised learning\\n  * Self-supervised learning\\n  * Reinforcement learning\\n  * Meta-learning\\n  * Online learning\\n  * Batch learning\\n  * Curriculum learning\\n  * Rule-based learning\\n  * Neuro-symbolic AI\\n  * Neuromorphic engineering\\n  * Quantum machine learning\\n\\n  \\nProblems\\n\\n  * Classification\\n  * Generative modeling\\n  * Regression\\n  * Clustering\\n  * Dimensionality reduction\\n  * Density estimation\\n  * Anomaly detection\\n  * Data cleaning\\n  * AutoML\\n  * Association rules\\n  * Semantic analysis\\n  * Structured prediction\\n  * Feature engineering\\n  * Feature learning\\n  * Learning to rank\\n  * Grammar induction\\n  * Ontology learning\\n  * Multimodal learning\\n\\n  \\nSupervised learning  \\n(**classification** • **regression**)\\n\\n  * Apprenticeship learning\\n  * Decision trees\\n  * Ensembles\\n    * Bagging\\n    * Boosting\\n    * Random forest\\n  * _k_ -NN\\n  * Linear regression\\n  * Naive Bayes\\n  * Artificial neural networks\\n  * Logistic regression\\n  * Perceptron\\n  * Relevance vector machine (RVM)\\n  * Support vector machine (SVM)\\n\\n  \\nClustering\\n\\n  * BIRCH\\n  * CURE\\n  * Hierarchical\\n  * _k_ -means\\n  * Fuzzy\\n  * Expectation–maximization (EM)\\n  *   \\nDBSCAN\\n\\n  * OPTICS\\n  * Mean shift\\n\\n  \\nDimensionality reduction\\n\\n  * Factor analysis\\n  * CCA\\n  * ICA\\n  * LDA\\n  * NMF\\n  * PCA\\n  * PGD\\n  * t-SNE\\n  * SDL\\n\\n  \\nStructured prediction\\n\\n  * Graphical models\\n    * Bayes net\\n    * Conditional random field\\n    * Hidden Markov\\n\\n  \\nAnomaly detection\\n\\n  * RANSAC\\n  * _k_ -NN\\n  * Local outlier factor\\n  * Isolation forest\\n\\n  \\nArtificial neural network\\n\\n  * Autoencoder\\n  * Deep learning\\n  * Feedforward neural network\\n  * Recurrent neural network\\n    * LSTM\\n    * GRU\\n    * ESN\\n    * reservoir computing\\n  * Boltzmann machine\\n    * Restricted\\n  * GAN\\n  * Diffusion model\\n  * SOM\\n  * Convolutional neural network\\n    * U-Net\\n    * LeNet\\n    * AlexNet\\n    * DeepDream\\n  * Neural radiance field\\n  * Transformer\\n    * Vision\\n  * Mamba\\n  * Spiking neural network\\n  * Memtransistor\\n  * Electrochemical RAM (ECRAM)\\n\\n  \\nReinforcement learning\\n\\n  * Q-learning\\n  * SARSA\\n  * Temporal difference (TD)\\n  * Multi-agent\\n    * Self-play\\n\\n  \\nLearning with humans\\n\\n  * Active learning\\n  * Crowdsourcing\\n  * Human-in-the-loop\\n  * RLHF\\n\\n  \\nModel diagnostics\\n\\n  * Coefficient of determination\\n  * Confusion matrix\\n  * Learning curve\\n  * ROC curve\\n\\n  \\nMathematical foundations\\n\\n  * Kernel machines\\n  * Bias–variance tradeoff\\n  * Computational learning theory\\n  * Empirical risk minimization\\n  * Occam learning\\n  * PAC learning\\n  * Statistical learning\\n  * VC theory\\n  * Topological deep learning\\n\\n  \\nJournals and conferences\\n\\n  * ECML PKDD\\n  * NeurIPS\\n  * ICML\\n  * ICLR\\n  * IJCAI\\n  * ML\\n  * JMLR\\n\\n  \\nRelated articles\\n\\n  * Glossary of artificial intelligence\\n  * List of datasets for machine-learning research\\n    * List of datasets in computer vision and image processing\\n  * Outline of machine learning\\n\\n  \\n  \\n  * v\\n  * t\\n  * e\\n\\n  \\n  \\nA **large language model** (**LLM**) is a type of machine learning model\\ndesigned for natural language processing tasks such as language generation.\\nLLMs are language models with many parameters, and are trained with self-\\nsupervised learning on a vast amount of text.\\n\\nThe largest and most capable LLMs are generative pretrained transformers\\n(GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt\\nengineering.[1] These models acquire predictive power regarding syntax,\\nsemantics, and ontologies[2] inherent in human language corpora, but they also\\ninherit inaccuracies and biases present in the data they are trained in.[3]\\n\\n## History\\n\\n[edit]\\n\\nThe training compute of notable large models in FLOPs vs publication date over\\nthe period 2010-2024. For overall notable models (top left), frontier models\\n(top right), top language models (bottom left) and top models within leading\\ncompanies (bottom right). The majority of these models are language models.\\nThe training compute of notable large AI models in FLOPs vs publication date\\nover the period 2017-2024. The majority of large models are language models or\\nmultimodal models with language capacity.\\n\\nBefore 2017, there were a few language models that were large as compared to\\ncapacities then available. In the 1990s, the IBM alignment models pioneered\\nstatistical language modelling. A smoothed n-gram model in 2001 trained on 0.3\\nbillion words achieved state-of-the-art perplexity at the time.[4] In the\\n2000s, as Internet use became prevalent, some researchers constructed\\nInternet-scale language datasets (\"web as corpus\"[5]), upon which they trained\\nstatistical language models.[6][7] In 2009, in most language processing tasks,\\nstatistical language models dominated over symbolic language models because\\nthey can usefully ingest large datasets.[8]\\n\\nAfter neural networks became dominant in image processing around 2012,[9] they\\nwere applied to language modelling as well. Google converted its translation\\nservice to Neural Machine Translation in 2016. Because it preceded the\\nexistence of transformers, it was done by seq2seq deep LSTM networks.\\n\\nAn illustration of main components of the transformer model from the original\\npaper, where layers were normalized after (instead of before) multiheaded\\nattention\\n\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer\\narchitecture in their landmark paper \"Attention Is All You Need\". This paper\\'s\\ngoal was to improve upon 2014 seq2seq technology,[10] and was based mainly on\\nthe attention mechanism developed by Bahdanau et al. in 2014.[11] The\\nfollowing year in 2018, BERT was introduced and quickly became\\n\"ubiquitous\".[12] Though the original transformer has both encoder and decoder\\nblocks, BERT is an encoder-only model. Academic and research usage of BERT\\nbegan to decline in 2023, following rapid improvements in the abilities of\\ndecoder-only models (such as GPT) to solve tasks via prompting.[13]\\n\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that\\ncaught widespread attention because OpenAI at first deemed it too powerful to\\nrelease publicly, out of fear of malicious use.[14] GPT-3 in 2020 went a step\\nfurther and as of 2024[update] is available only via API with no offering of\\ndownloading the model to execute locally. But it was the 2022 consumer-facing\\nbrowser-based ChatGPT that captured the imaginations of the general population\\nand caused some media hype and online buzz.[15] The 2023 GPT-4 was praised for\\nits increased accuracy and as a \"holy grail\" for its multimodal\\ncapabilities.[16] OpenAI did not reveal the high-level architecture and the\\nnumber of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM\\nusage across several research subfields of computer science, including\\nrobotics, software engineering, and societal impact work.[17] In 2024 OpenAI\\nreleased the reasoning model OpenAI o1, which generates long chains of thought\\nbefore returning a final answer.\\n\\nCompeting language models have for the most part been attempting to equal the\\nGPT series, at least in terms of number of parameters.[18]\\n\\nSince 2022, source-available models have been gaining popularity, especially\\nat first with BLOOM and LLaMA, though both have restrictions on the field of\\nuse. Mistral AI\\'s models Mistral 7B and Mixtral 8x7b have the more permissive\\nApache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-\\nparameter open-weight model that performs comparably to OpenAI o1 but at a\\nmuch lower cost.[19]\\n\\nSince 2023, many LLMs have been trained to be multimodal, having the ability\\nto also process or generate other types of data, such as images or audio.\\nThese LLMs are also called large multimodal models (LMMs).[20]\\n\\nAs of 2024, the largest and most capable models are all based on the\\ntransformer architecture. Some recent implementations are based on other\\narchitectures, such as recurrent neural network variants and Mamba (a state\\nspace model).[21][22][23]\\n\\n## Dataset preprocessing\\n\\n[edit]\\n\\nSee also: List of datasets for machine-learning research § Internet\\n\\n### Tokenization\\n\\n[edit]\\n\\nAs machine learning algorithms process numbers rather than text, the text must\\nbe converted to numbers. In the first step, a vocabulary is decided upon, then\\ninteger indices are arbitrarily but uniquely assigned to each vocabulary\\nentry, and finally, an embedding is associated to the integer index.\\nAlgorithms include byte-pair encoding (BPE) and WordPiece. There are also\\nspecial tokens serving as control characters, such as `[MASK]` for masked-out\\ntoken (as used in BERT), and `[UNK]` (\"unknown\") for characters not appearing\\nin the vocabulary. Also, some special symbols are used to denote special text\\nformatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and\\nGPT. \"##\" denotes continuation of a preceding word in BERT.[24]\\n\\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split `tokenizer:\\ntexts -> series of numerical \"tokens\"` as\\n\\ntoken  | izer  | :  |  texts  |  -> | series  |  of  |  numerical  |  \"  | t  | ok  | ens  | \"   \\n---|---|---|---|---|---|---|---|---|---|---|---|---  \\n  \\nTokenization also compresses the datasets. Because LLMs generally require\\ninput to be an array that is not jagged, the shorter texts must be \"padded\"\\nuntil they match the length of the longest one. How many tokens are, on\\naverage, needed per word depends on the language of the dataset.[25][26]\\n\\n#### BPE\\n\\n[edit]\\n\\nMain article: Byte pair encoding\\n\\nAs an example, consider a tokenizer based on byte-pair encoding. In the first\\nstep, all unique characters (including blanks and punctuation marks) are\\ntreated as an initial set of _n_ -grams (i.e. initial set of uni-grams).\\nSuccessively the most frequent pair of adjacent characters is merged into a\\nbi-gram and all instances of the pair are replaced by it. All occurrences of\\nadjacent pairs of (previously merged) _n_ -grams that most frequently occur\\ntogether are then again merged into even lengthier _n_ -gram, until a\\nvocabulary of prescribed size is obtained (in case of GPT-3, the size is\\n50257).[27] After a tokenizer is trained, any text can be tokenized by it, as\\nlong as it does not contain characters not appearing in the initial-set of\\nuni-grams.[28]\\n\\n#### Problems\\n\\n[edit]\\n\\nA token vocabulary based on the frequencies extracted from mainly English\\ncorpora uses as few tokens as possible for an average English word. However,\\nan average word in another language encoded by such an English-optimized\\ntokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use\\nup to 15 times more tokens per word for some languages, for example for the\\nShan language from Myanmar. Even more widespread languages such as Portuguese\\nand German have \"a premium of 50%\" compared to English.[29]\\n\\nGreedy tokenization also causes subtle problems with text completion.[30]\\n\\n### Dataset cleaning\\n\\n[edit]\\n\\nMain article: Data cleansing\\n\\nIn the context of training LLMs, datasets are typically cleaned by removing\\nlow-quality, duplicated, or toxic data.[31] Cleaned datasets can increase\\ntraining efficiency and lead to improved downstream performance.[32][33] A\\ntrained LLM can be used to clean datasets for training a further LLM.[34]\\n\\nWith the increasing proportion of LLM-generated content on the web, data\\ncleaning in the future may include filtering out such content. LLM-generated\\ncontent can pose a problem if the content is similar to human text (making\\nfiltering difficult) but of lower quality (degrading performance of models\\ntrained on it).[35]\\n\\n### Synthetic data\\n\\n[edit]\\n\\nMain article: Synthetic data\\n\\nTraining of largest language models might need more linguistic data than\\nnaturally available, or that the naturally occurring data is of insufficient\\nquality. In these cases, synthetic data might be used. Microsoft\\'s Phi series\\nof LLMs is trained on textbook-like data generated by another LLM.[36]\\n\\n## Training and architecture\\n\\n[edit]\\n\\nSee also: Fine-tuning (machine learning)\\n\\n### Reinforcement learning from human feedback\\n\\n[edit]\\n\\nReinforcement learning from human feedback (RLHF) through algorithms, such as\\nproximal policy optimization, is used to further fine-tune a model based on a\\ndataset of human preferences.[37]\\n\\n### Instruction tuning\\n\\n[edit]\\n\\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct\\nresponses, replacing any naive responses, starting from human-generated\\ncorrections of a few cases. For example, in the instruction \"Write an essay\\nabout the main themes represented in _Hamlet_ ,\" an initial naive completion\\nmight be \"If you submit the essay after March 17, your grade will be reduced\\nby 10% for each day of delay,\" based on the frequency of this textual sequence\\nin the corpus.[38]\\n\\n### Mixture of experts\\n\\n[edit]\\n\\nMain article: Mixture of experts\\n\\nThe largest LLM may be too expensive to train and use directly. For such\\nmodels, mixture of experts (MoE) can be applied, a line of research pursued by\\nGoogle researchers since 2017 to train models reaching up to 1 trillion\\nparameters.[39][40][41]\\n\\n### Prompt engineering, attention mechanism, and context window\\n\\n[edit]\\n\\nSee also: Prompt engineering and Attention (machine learning)\\n\\nMost results previously achievable only by (costly) fine-tuning, can be\\nachieved through prompt engineering, although limited to the scope of a single\\nconversation (more precisely, limited to the scope of a context window).[42]\\n\\nWhen each head calculates, according to its own criteria, how much other\\ntokens are relevant for the \"it_\" token, note that the second attention head,\\nrepresented by the second column, is focusing most on the first two rows, i.e.\\nthe tokens \"The\" and \"animal\", while the third column is focusing most on the\\nbottom two rows, i.e. on \"tired\", which has been tokenized into two\\ntokens.[43]\\n\\nIn order to find out which tokens are relevant to each other within the scope\\nof the context window, the attention mechanism calculates \"soft\" weights for\\neach token, more precisely for its embedding, by using multiple attention\\nheads, each with its own \"relevance\" for calculating its own soft weights. For\\nexample, the small (i.e. 117M parameter sized) GPT-2 model has had twelve\\nattention heads and a context window of only 1k tokens.[44] In its medium\\nversion it has 345M parameters and contains 24 layers, each with 12 attention\\nheads. For the training with gradient descent a batch size of 512 was\\nutilized.[28]\\n\\nThe largest models, such as Google\\'s Gemini 1.5, presented in February 2024,\\ncan have a context window sized up to 1 million (context window of 10 million\\nwas also \"successfully tested\").[45] Other models with large context windows\\nincludes Anthropic\\'s Claude 2.1, with a context window of up to 200k\\ntokens.[46] Note that this maximum refers to the number of input tokens and\\nthat the maximum number of output tokens differs from the input and is often\\nsmaller. For example, the GPT-4 Turbo model has a maximum output of 4096\\ntokens.[47]\\n\\nLength of a conversation that the model can take into account when generating\\nits next answer is limited by the size of a context window, as well. If the\\nlength of a conversation, for example with ChatGPT, is longer than its context\\nwindow, only the parts inside the context window are taken into account when\\ngenerating the next answer, or the model needs to apply some algorithm to\\nsummarize the too distant parts of conversation.\\n\\nThe shortcomings of making a context window larger include higher\\ncomputational cost and possibly diluting the focus on local context, while\\nmaking it smaller can cause a model to miss an important long-range\\ndependency. Balancing them is a matter of experimentation and domain-specific\\nconsiderations.\\n\\nA model may be pre-trained either to predict how the segment continues, or\\nwhat is missing in the segment, given a segment from its training dataset.[48]\\nIt can be either\\n\\n  * autoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\\n  * \"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\"[49] does it): for example, given a segment \"I like to `[__] [__]` cream\", the model predicts that \"eat\" and \"ice\" are missing.\\n\\nModels may be trained on auxiliary tasks which test their understanding of the\\ndata distribution, such as Next Sentence Prediction (NSP), in which pairs of\\nsentences are presented and the model must predict whether they appear\\nconsecutively in the training corpus.[49] During training, regularization loss\\nis also used to stabilize training. However regularization loss is usually not\\nused during testing and evaluation.\\n\\n### Infrastructure\\n\\n[edit]\\n\\nSubstantial infrastructure is necessary for training the largest\\nmodels.[50][51][52]\\n\\n## Training cost\\n\\n[edit]\\n\\nThe qualifier \"large\" in \"large language model\" is inherently vague, as there\\nis no definitive threshold for the number of parameters required to qualify as\\n\"large\". As time goes on, what was previously considered \"large\" may evolve.\\nGPT-1 of 2018 is usually considered the first LLM, even though it has only\\n0.117 billion parameters. The tendency towards larger models is visible in the\\nlist of large language models.\\n\\nAs technology advanced, large sums have been invested in increasingly large\\nmodels. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters\\nmodel) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-\\nparameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in\\n2021) cost around $11 million.[53]\\n\\nFor Transformer-based LLM, training cost is much higher than inference cost.\\nIt costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2\\nFLOPs per parameter to infer on one token.[54]\\n\\n## Tool use\\n\\n[edit]\\n\\nThere are certain tasks that, in principle, cannot be solved by any LLM, at\\nleast not without the use of external tools or additional software. An example\\nof such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that\\nthe LLM has not already encountered a continuation of this calculation in its\\ntraining corpus.[_dubious – discuss_] In such cases, the LLM needs to resort\\nto running program code that calculates the result, which can then be included\\nin its response.[_dubious – discuss_]: Another example is \"What is the time\\nnow? It is \", where a separate program interpreter would need to execute a\\ncode to get system time on the computer, so that the LLM can include it in its\\nreply.[55][56] This basic strategy can be sophisticated with multiple attempts\\nof generated programs, and other sampling strategies.[57]\\n\\nGenerally, in order to get an LLM to use tools, one must fine-tune it for\\ntool-use. If the number of tools is finite, then fine-tuning may be done just\\nonce. If the number of tools can grow arbitrarily, as with online API\\nservices, then the LLM can be fine-tuned to be able to read API documentation\\nand call API correctly.[58][59]\\n\\nRetrieval-augmented generation (RAG) is another approach that enhances LLMs by\\nintegrating them with document retrieval systems. Given a query, a document\\nretriever is called to retrieve the most relevant documents. This is usually\\ndone by encoding the query and the documents into vectors, then finding the\\ndocuments with vectors (usually stored in a vector database) most similar to\\nthe vector of the query. The LLM then generates an output based on both the\\nquery and context included from the retrieved documents.[60]\\n\\n## Agency\\n\\n[edit]\\n\\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability\\nto interact with dynamic environments, recall past behaviors, and plan future\\nactions, but can be transformed into one by integrating modules like\\nprofiling, memory, planning, and action.[61]\\n\\nThe ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of\\nan LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\".\\nSpecifically, the language model is prompted with a textual description of the\\nenvironment, a goal, a list of possible actions, and a record of the actions\\nand observations so far. It generates one or more thoughts before generating\\nan action, which is then executed in the environment.[62] The linguistic\\ndescription of the environment given to the LLM planner can even be the LaTeX\\ncode of a paper describing the environment.[63]\\n\\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first\\nconnected to the visual world via image descriptions, then it is prompted to\\nproduce plans for complex tasks and behaviors based on its pretrained\\nknowledge and environmental feedback it receives.[64]\\n\\nThe Reflexion method[65] constructs an agent that learns over multiple\\nepisodes. At the end of each episode, the LLM is given the record of the\\nepisode, and prompted to think up \"lessons learned\", which would help it\\nperform better at a subsequent episode. These \"lessons learned\" are given to\\nthe agent in the subsequent episodes.[_citation needed_]\\n\\nMonte Carlo tree search can use an LLM as rollout heuristic. When a\\nprogrammatic world model is not available, an LLM can also be prompted with a\\ndescription of the environment to act as world model.[66]\\n\\nFor open-ended exploration, an LLM can be used to score observations for their\\n\"interestingness\", which can be used as a reward signal to guide a normal\\n(non-LLM) reinforcement learning agent.[67] Alternatively, it can propose\\nincreasingly difficult tasks for curriculum learning.[68] Instead of\\noutputting individual actions, an LLM planner can also construct \"skills\", or\\nfunctions for complex action sequences. The skills can be stored and later\\ninvoked, allowing increasing levels of abstraction in planning.[68]\\n\\nLLM-powered agents can keep a long-term memory of its previous contexts, and\\nthe memory can be retrieved in the same way as Retrieval Augmented Generation.\\nMultiple such agents can interact socially.[69]\\n\\n## Compression\\n\\n[edit]\\n\\nTypically, LLMs are trained with single- or half-precision floating point\\nnumbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one\\nbillion parameters require 2 gigabytes. The largest models typically have 100\\nbillion parameters, requiring 200 gigabytes to load, which places them outside\\nthe range of most consumer electronics.[70]\\n\\n_Post-trainingquantization_[71] aims to decrease the space requirement by\\nlowering precision of the parameters of a trained model, while preserving most\\nof its performance.[72][73] The simplest form of quantization simply truncates\\nall numbers to a given number of bits. It can be improved by using a different\\nquantization codebook per layer. Further improvement can be done by applying\\ndifferent precisions to different parameters, with higher precision for\\nparticularly important parameters (\"outlier weights\").[74] See the visual\\nguide to quantization by Maarten Grootendorst[75] for a visual depiction.\\n\\nWhile quantized models are typically frozen, and only pre-quantized models are\\nfine-tuned, quantized models can still be fine-tuned.[76]\\n\\n## Multimodality\\n\\n[edit]\\n\\nSee also: Multimodal learning\\n\\nMultimodality means \"having several modalities\", and a \"modality\" refers to a\\ntype of input or output, such as video, image, audio, text, proprioception,\\netc.[77] There have been many AI models trained specifically to ingest one\\nmodality and output another modality, such as AlexNet for image to label,[78]\\nvisual question answering for image-text to text,[79] and speech recognition\\nfor speech to text.\\n\\nA common method to create multimodal models out of an LLM is to \"tokenize\" the\\noutput of a trained encoder. Concretely, one can construct an LLM that can\\nunderstand images as follows: take a trained LLM, and take a trained image\\nencoder  E {\\\\displaystyle E} . Make a small multilayered perceptron  f\\n{\\\\displaystyle f} , so that for any image  y {\\\\displaystyle y} , the post-\\nprocessed vector  f ( E ( y ) ) {\\\\displaystyle f(E(y))} has the same\\ndimensions as an encoded token. That is an \"image token\". Then, one can\\ninterleave text tokens and image tokens. The compound model is then fine-tuned\\non an image-text dataset. This basic construction can be applied with more\\nsophistication to improve the model. The image encoder may be frozen to\\nimprove stability.[80]\\n\\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning\\na pair of pretrained language model and image encoder to perform better on\\nvisual question answering than models trained from scratch.[81] Google PaLM\\nmodel was fine-tuned into a multimodal model PaLM-E using the tokenization\\nmethod, and applied to robotic control.[82] LLaMA models have also been turned\\nmultimodal using the tokenization method, to allow image inputs,[83] and video\\ninputs.[84]\\n\\nGPT-4 can use both text and image as inputs[85] (although the vision component\\nwas not released to the public until GPT-4V[86]); Google DeepMind\\'s Gemini is\\nalso multimodal.[87] Mistral introduced its own multimodel Pixtral 12B model\\nin September 2024.[88]\\n\\n## Reasoning\\n\\n[edit]\\n\\nIn late 2024, a new direction emerged in LLM development with models\\nspecifically designed for complex reasoning tasks. These \"reasoning models\"\\nwere trained to spend more time generating step-by-step solutions before\\nproviding final answers, similar to human problem-solving processes.[89]\\nOpenAI introduced this trend with their o1 model in September 2024, followed\\nby o3 in December 2024. These models showed significant improvements in\\nmathematics, science, and coding tasks compared to traditional LLMs. For\\nexample, on International Mathematics Olympiad qualifying exam problems,\\nGPT-4o achieved 13% accuracy while o1 reached 83%.[89][90] In January 2025,\\nthe Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter\\nopen-weight reasoning model that achieved comparable performance to OpenAI\\'s\\no1 while being significantly more cost-effective to operate. Unlike\\nproprietary models from OpenAI, DeepSeek-R1\\'s open-weight nature allowed\\nresearchers to study and build upon the algorithm, though its training data\\nremained private.[91] These reasoning models typically require more\\ncomputational resources per query compared to traditional LLMs, as they\\nperform more extensive processing to work through problems step-by-step.\\nHowever, they have shown superior capabilities in domains requiring structured\\nlogical thinking, such as mathematics, scientific research, and computer\\nprogramming.[90]\\n\\nEfforts to reduce or compensate for hallucinations have employed automated\\nreasoning, RAG (retrieval-augmented generation), fine-tuning, and other\\nmethods.[92]\\n\\n## Properties\\n\\n[edit]\\n\\n### Scaling laws\\n\\n[edit]\\n\\nMain article: Neural scaling law\\n\\nThe performance of an LLM after pretraining largely depends on the:\\n\\n  * cost of pretraining  C {\\\\displaystyle C} (the total amount of compute used),\\n  * size of the artificial neural network itself, such as number of parameters  N {\\\\displaystyle N} (i.e. amount of neurons in its layers, amount of weights between them and biases),\\n  * size of its pretraining dataset (i.e. number of tokens in corpus,  D {\\\\displaystyle D} ).\\n\\n\"Scaling laws\" are empirical statistical laws that predict LLM performance\\nbased on such factors. One particular scaling law (\"Chinchilla scaling\") for\\nLLM autoregressively trained for one epoch, with a log-log learning rate\\nschedule, states that:[93] { C = C 0 N D L = A N α + B D β + L 0\\n{\\\\displaystyle {\\\\begin{cases}C=C_{0}ND\\\\\\\\\\\\\\\\[6pt]L={\\\\frac {A}{N^{\\\\alpha\\n}}}+{\\\\frac {B}{D^{\\\\beta }}}+L_{0}\\\\end{cases}}} where the variables are\\n\\n  * C {\\\\displaystyle C} is the cost of training the model, in FLOPs.\\n  * N {\\\\displaystyle N} is the number of parameters in the model.\\n  * D {\\\\displaystyle D} is the number of tokens in the training set.\\n  * L {\\\\displaystyle L} is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\\n\\nand the statistical hyper-parameters are\\n\\n  * C 0 = 6 {\\\\displaystyle C_{0}=6} , meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.[54]\\n  * α = 0.34 , β = 0.28 , A = 406.4 , B = 410.7 , L 0 = 1.69 {\\\\displaystyle \\\\alpha =0.34,\\\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\\n\\n### Emergent abilities\\n\\n[edit]\\n\\nAt point(s) referred to as breaks,[94] the lines change their slopes,\\nappearing on a linear-log plot as a series of linear segments connected by\\narcs.\\n\\nPerformance of bigger models on various tasks, when plotted on a log-log\\nscale, appears as a linear extrapolation of performance achieved by smaller\\nmodels. However, this linearity may be punctuated by \"break(s)\"[94] in the\\nscaling law, where the slope of the line changes abruptly, and where larger\\nmodels acquire \"emergent abilities\".[42][95] They arise from the complex\\ninteraction of the model\\'s components and are not explicitly programmed or\\ndesigned.[96]\\n\\nFurthermore, recent research has demonstrated that AI systems, including large\\nlanguage models, can employ heuristic reasoning akin to human cognition. They\\nbalance between exhaustive logical processing and the use of cognitive\\nshortcuts (heuristics), adapting their reasoning strategies to optimize\\nbetween accuracy and effort. This behavior aligns with principles of resource-\\nrational human cognition, as discussed in classical theories of bounded\\nrationality and dual-process theory.[97]\\n\\nOne of the emergent abilities is in-context learning from example\\ndemonstrations.[98] In-context learning is involved in tasks, such as:\\n\\n  * reported arithmetics\\n  * decoding the International Phonetic Alphabet\\n  * unscrambling a word\\'s letters\\n  * disambiguating word-in-context datasets[42][99][100]\\n  * converting spatial words\\n  * cardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.[101]\\n  * chain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.[102]\\n  * identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.[103]\\n\\nSchaeffer _et. al._ argue that the emergent abilities are not unpredictably\\nacquired, but predictably acquired according to a smooth scaling law. The\\nauthors considered a toy statistical model of an LLM solving multiple-choice\\nquestions, and showed that this statistical model, modified to account for\\nother types of tasks, applies to these tasks as well.[104]\\n\\nLet  x {\\\\displaystyle x} be the number of parameter count, and  y\\n{\\\\displaystyle y} be the performance of the model.\\n\\n  * When  y = average  Pr ( correct token ) {\\\\displaystyle y={\\\\text{average }}\\\\Pr({\\\\text{correct token}})} , then  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} is an exponential curve (before it hits the plateau at one), which looks like emergence.\\n  * When  y = average  log \\u2061 ( Pr ( correct token ) ) {\\\\displaystyle y={\\\\text{average }}\\\\log(\\\\Pr({\\\\text{correct token}}))} , then the  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} plot is a straight line (before it hits the plateau at zero), which does not look like emergence.\\n  * When  y = average  Pr ( the most likely token is correct ) {\\\\displaystyle y={\\\\text{average }}\\\\Pr({\\\\text{the most likely token is correct}})} , then  ( log \\u2061 x , y ) {\\\\displaystyle (\\\\log x,y)} is a step-function, which looks like emergence.\\n\\n## Interpretation\\n\\n[edit]\\n\\nLarge language models by themselves are black boxes, and it is not clear how\\nthey can perform linguistic tasks. Similarly, it is unclear if or how LLMs\\nshould be viewed as models of the human brain and/or human mind.[105]\\n\\nThere are several methods for understanding how LLMs work. Mechanistic\\ninterpretability aims to reverse-engineer LLMs by discovering symbolic\\nalgorithms that approximate the inference performed by an LLM. One example is\\nOthello-GPT, where a small Transformer is trained to predict legal Othello\\nmoves. It is found that there is a linear representation of the Othello board,\\nand modifying the representation changes the predicted legal Othello moves in\\nthe correct way.[106][107] In another example, a small Transformer is trained\\non Karel programs. Similar to the Othello-GPT example, there is a linear\\nrepresentation of Karel program semantics, and modifying the representation\\nchanges output in the correct way. The model also generates correct programs\\nthat are on average shorter than those in the training set.[108]\\n\\nIn another example, the authors trained small transformers on modular\\narithmetic addition. The resulting models were reverse-engineered, and it\\nturned out they used discrete Fourier transform.[109]\\n\\nA related concept is AI explainability, which focuses on understanding how an\\nAI model arrives at a given result.\\n\\n### Understanding and intelligence\\n\\n[edit]\\n\\nSee also: Philosophy of artificial intelligence and Artificial consciousness\\n\\nNLP researchers were evenly split when asked, in a 2022 survey, whether\\n(untuned) LLMs \"could (ever) understand natural language in some nontrivial\\nsense\".[110] Proponents of \"LLM understanding\" believe that some LLM\\nabilities, such as mathematical reasoning, imply an ability to \"understand\"\\ncertain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel\\nand difficult tasks that span mathematics, coding, vision, medicine, law,\\npsychology and more\" and that GPT-4 \"could reasonably be viewed as an early\\n(yet still incomplete) version of an artificial general intelligence system\":\\n\"Can one reasonably say that a system that passes exams for software\\nengineering candidates is not _really_ intelligent?\"[111][112] Ilya Sutskever\\nargues that predicting the next word sometimes involves reasoning and deep\\ninsights, for example if the LLM has to predict the name of the criminal in an\\nunknown detective novel after processing the entire story leading up to the\\nrevelation.[113] Some researchers characterize LLMs as \"alien\\nintelligence\".[114][115] For example, Conjecture CEO Connor Leahy considers\\nuntuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF\\ntuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If\\nyou don\\'t push it too far, the smiley face stays on. But then you give it [an\\nunexpected] prompt, and suddenly you see this massive underbelly of insanity,\\nof weird thought processes and clearly non-human understanding.\"[116][117]\\n\\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are\\n\"simply remixing and recombining existing writing\",[115] a phenomenon known as\\nstochastic parrot, or they point to the deficits existing LLMs continue to\\nhave in prediction skills, reasoning skills, agency, and explainability.[110]\\nFor example, GPT-4 has natural deficits in planning and in real-time\\nlearning.[112] Generative LLMs have been observed to confidently assert claims\\nof fact which do not seem to be justified by their training data, a phenomenon\\nwhich has been termed \"hallucination\".[118] Specifically, hallucinations in\\nthe context of LLMs correspond to the generation of text or responses that\\nseem syntactically sound, fluent, and natural but are factually incorrect,\\nnonsensical, or unfaithful to the provided source input.[119] Neuroscientist\\nTerrence Sejnowski has argued that \"The diverging opinions of experts on the\\nintelligence of LLMs suggests that our old ideas based on natural intelligence\\nare inadequate\".[110]\\n\\nThe matter of LLM\\'s exhibiting intelligence or understanding has two main\\naspects – the first is how to model thought and language in a computer system,\\nand the second is how to enable the computer system to generate human like\\nlanguage.[110] These aspects of language as a model of cognition have been\\ndeveloped in the field of cognitive linguistics. American linguist George\\nLakoff presented Neural Theory of Language (NTL)[120] as a computational basis\\nfor using language as a model of learning tasks and understanding. The NTL\\nModel outlines how specific neural structures of the human brain shape the\\nnature of thought and language and in turn what are the computational\\nproperties of such neural systems that can be applied to model thought and\\nlanguage in a computer system. After a framework for modeling language in a\\ncomputer systems was established, the focus shifted to establishing frameworks\\nfor computer systems to generate language with acceptable grammar. In his 2014\\nbook titled _The Language Myth: Why Language Is Not An Instinct_ , British\\ncognitive linguist and digital communication technologist Vyvyan Evans mapped\\nout the role of probabilistic context-free grammar (PCFG) in enabling NLP to\\nmodel cognitive patterns and generate human like language.[121][122]\\n\\n## Evaluation\\n\\n[edit]\\n\\n### Perplexity\\n\\n[edit]\\n\\nThe canonical measure of the performance of an LLM is its perplexity on a\\ngiven text corpus. Perplexity measures how well a model predicts the contents\\nof a dataset; the higher the likelihood the model assigns to the dataset, the\\nlower the perplexity. In mathematical terms, perplexity is the exponential of\\nthe average negative log likelihood per token.\\n\\nlog \\u2061 ( Perplexity ) = − 1 N ∑ i = 1 N log \\u2061 ( Pr ( token i ∣ context for\\ntoken i ) ) {\\\\displaystyle \\\\log({\\\\text{Perplexity}})=-{\\\\frac {1}{N}}\\\\sum\\n_{i=1}^{N}\\\\log(\\\\Pr({\\\\text{token}}_{i}\\\\mid {\\\\text{context for token}}_{i}))}\\n\\nHere,  N {\\\\displaystyle N} is the number of tokens in the text corpus, and\\n\"context for token  i {\\\\displaystyle i} \" depends on the specific type of LLM.\\nIf the LLM is autoregressive, then \"context for token  i {\\\\displaystyle i} \"\\nis the segment of text appearing before token  i {\\\\displaystyle i} . If the\\nLLM is masked, then \"context for token  i {\\\\displaystyle i} \" is the segment\\nof text surrounding token  i {\\\\displaystyle i} .\\n\\nBecause language models may overfit to training data, models are usually\\nevaluated by their perplexity on a test set.[49] This evaluation is\\npotentially problematic for larger models which, as they are trained on\\nincreasingly large corpora of text, are increasingly likely to inadvertently\\ninclude portions of any given test set.[1]\\n\\n#### BPW, BPC, and BPT\\n\\n[edit]\\n\\nIn information theory, the concept of entropy is intricately linked to\\nperplexity, a relationship notably established by Claude Shannon.[123] This\\nrelationship is mathematically expressed as  Entropy = log 2 \\u2061 ( Perplexity )\\n{\\\\displaystyle {\\\\text{Entropy}}=\\\\log _{2}({\\\\text{Perplexity}})} .\\n\\nEntropy, in this context, is commonly quantified in terms of bits per word\\n(BPW) or bits per character (BPC), which hinges on whether the language model\\nutilizes word-based or character-based tokenization.\\n\\nNotably, in the case of larger language models that predominantly employ sub-\\nword tokenization, bits per token (BPT) emerges as a seemingly more\\nappropriate measure. However, due to the variance in tokenization methods\\nacross different Large Language Models (LLMs), BPT does not serve as a\\nreliable metric for comparative analysis among diverse models. To convert BPT\\ninto BPW, one can multiply it by the average number of tokens per word.\\n\\nIn the evaluation and comparison of language models, cross-entropy is\\ngenerally the preferred metric over entropy. The underlying principle is that\\na lower BPW is indicative of a model\\'s enhanced capability for compression.\\nThis, in turn, reflects the model\\'s proficiency in making accurate\\npredictions.\\n\\n### Task-specific datasets and benchmarks\\n\\n[edit]\\n\\nA large number of testing datasets and benchmarks have also been developed to\\nevaluate the capabilities of language models on more specific downstream\\ntasks. Tests may be designed to evaluate a variety of capabilities, including\\ngeneral knowledge, bias, commonsense reasoning, and mathematical problem-\\nsolving.\\n\\nOne broad category of evaluation dataset is question answering datasets,\\nconsisting of pairs of questions and correct answers, for example, (\"Have the\\nSan Jose Sharks won the Stanley Cup?\", \"No\").[124] A question answering task\\nis considered \"open book\" if the model\\'s prompt includes text from which the\\nexpected answer can be derived (for example, the previous question could be\\nadjoined with some text which includes the sentence \"The Sharks have advanced\\nto the Stanley Cup finals once, losing to the Pittsburgh Penguins in\\n2016.\"[124]). Otherwise, the task is considered \"closed book\", and the model\\nmust draw on knowledge retained during training.[125] Some examples of\\ncommonly used question answering datasets include TruthfulQA, Web Questions,\\nTriviaQA, and SQuAD.[125]\\n\\nEvaluation datasets may also take the form of text completion, having the\\nmodel select the most likely word or sentence to complete a prompt, for\\nexample: \"Alice was friends with Bob. Alice went to visit her friend,\\n____\".[1]\\n\\nSome composite benchmarks have also been developed which combine a diversity\\nof different evaluation datasets and tasks. Examples include GLUE, SuperGLUE,\\nMMLU, BIG-bench, HELM, and HLE (Humanity\\'s Last Exam).[123][125] OpenAI has\\nreleased tools for running composite benchmarks, but noted that the eval\\nresults are sensitive to the prompting method.[126][127] Some public datasets\\ncontain questions that are mislabeled, ambiguous, unanswerable, or otherwise\\nof low-quality, which can be cleaned to give more reliable benchmark\\nscores.[128]\\n\\nBias in LLMs may be measured through benchmarks such as CrowS-Pairs\\n(Crowdsourced Stereotype Pairs),[129] Stereo Set,[130] and the more recent\\nParity Benchmark.[131]\\n\\nIt was previously standard to report results on a heldout portion of an\\nevaluation dataset after doing supervised fine-tuning on the remainder. It is\\nnow more common to evaluate a pre-trained model directly through prompting\\ntechniques, though researchers vary in the details of how they formulate\\nprompts for particular tasks, particularly with respect to how many examples\\nof solved tasks are adjoined to the prompt (i.e. the value of _n_ in _n_ -shot\\nprompting).\\n\\n#### Adversarially constructed evaluations\\n\\n[edit]\\n\\nBecause of the rapid pace of improvement of large language models, evaluation\\nbenchmarks have suffered from short lifespans, with state of the art models\\nquickly \"saturating\" existing benchmarks, exceeding the performance of human\\nannotators, leading to efforts to replace or augment the benchmark with more\\nchallenging tasks.[132] In addition, there are cases of \"shortcut learning\"\\nwherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical\\ncorrelations in superficial test question wording in order to guess the\\ncorrect responses, without necessarily understanding the actual question being\\nasked.[110]\\n\\nSome datasets have been constructed adversarially, focusing on particular\\nproblems on which extant language models seem to have unusually poor\\nperformance compared to humans. One example is the TruthfulQA dataset, a\\nquestion answering dataset consisting of 817 questions which language models\\nare susceptible to answering incorrectly by mimicking falsehoods to which they\\nwere repeatedly exposed during training. For example, an LLM may answer \"No\"\\nto the question \"Can you teach an old dog new tricks?\" because of its exposure\\nto the English idiom _you can\\'t teach an old dog new tricks_ , even though\\nthis is not literally true.[133]\\n\\nAnother example of an adversarial evaluation dataset is Swag and its\\nsuccessor, HellaSwag, collections of problems in which one of multiple options\\nmust be selected to complete a text passage. The incorrect completions were\\ngenerated by sampling from a language model and filtering with a set of\\nclassifiers. The resulting problems are trivial for humans but at the time the\\ndatasets were created state of the art language models had poor accuracy on\\nthem. For example:\\n\\n> We see a fitness center sign. We then see a man talking to the camera and\\n> sitting and laying on a exercise ball. The man...  \\n> a) demonstrates how to increase efficient exercise work by running up and\\n> down balls.  \\n> b) moves all his arms and legs and builds up a lot of muscle.  \\n> c) then plays the ball and we see a graphics and hedge trimming\\n> demonstration.  \\n> d) performs sit ups while on the ball and talking.[134]\\n\\nBERT selects b) as the most likely completion, though the correct answer is\\nd).[134]\\n\\n#### Limitations of LLM benchmarks\\n\\n[edit]\\n\\nBenchmarks can become outdated rapidly. Once a model attains near-perfect\\nscores on a given benchmark, that benchmark ceases to serve as a meaningful\\nindicator of progress. This phenomenon, known as \"benchmark saturation,\"\\nnecessitates the development of more challenging and nuanced tasks to continue\\nadvancing LLM capabilities. For instance, traditional benchmarks like\\nHellaSwag and MMLU have seen models achieving high accuracy already.\\n\\n## Wider impact\\n\\n[edit]\\n\\nIn 2023, _Nature Biomedical Engineering_ wrote that \"it is no longer possible\\nto accurately distinguish\" human-written text from text created by large\\nlanguage models, and that \"It is all but certain that general-purpose large\\nlanguage models will rapidly proliferate... It is a rather safe bet that they\\nwill change many industries over time.\"[135] Goldman Sachs suggested in 2023\\nthat generative language AI could increase global GDP by 7% in the next ten\\nyears, and could expose to automation 300 million jobs globally.[136][137]\\n\\n### Memorization and copyright\\n\\n[edit]\\n\\nFurther information: Artificial intelligence and copyright\\n\\nMemorization is an emergent behavior in LLMs in which long strings of text are\\noccasionally output verbatim from training data, contrary to typical behavior\\nof traditional artificial neural nets. Evaluations of controlled LLM output\\nmeasure the amount memorized from training data (focused on GPT-2-series\\nmodels) as variously over 1% for exact duplicates[138] or up to about 7%.[139]\\n\\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the\\nsame word indefinitely, after a few hundreds of repetitions, it would start\\noutputting excerpts from its training data.[140]\\n\\n### Security\\n\\n[edit]\\n\\nSome commenters expressed concern over accidental or deliberate creation of\\nmisinformation, or other forms of misuse.[141] For example, the availability\\nof large language models could reduce the skill-level required to commit\\nbioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM\\ncreators should exclude from their training data papers on creating or\\nenhancing pathogens.[142]\\n\\nThe potential presence of \"sleeper agents\" within LLMs is another emerging\\nsecurity concern. These are hidden functionalities built into the model that\\nremain dormant until triggered by a specific event or condition. Upon\\nactivation, the LLM deviates from its expected behavior to make insecure\\nactions.[143]\\n\\nLLM applications accessible to the public, like ChatGPT or Claude, typically\\nincorporate safety measures designed to filter out harmful content. However,\\nimplementing these controls effectively has proven challenging. For instance,\\na 2023 study[144] proposed a method for circumventing LLM safety systems.\\nSimilarly, Yongge Wang[145] illustrated in 2024 how a potential criminal could\\npotentially bypass ChatGPT 4o\\'s safety controls to obtain information on\\nestablishing a drug trafficking operation.\\n\\n### Algorithmic bias\\n\\n[edit]\\n\\nMain article: Algorithmic bias\\n\\nWhile LLMs have shown remarkable capabilities in generating human-like text,\\nthey are susceptible to inheriting and amplifying biases present in their\\ntraining data. This can manifest in skewed representations or unfair treatment\\nof different demographics, such as those based on race, gender, language, and\\ncultural groups.[146] Since English data is overrepresented in current large\\nlanguage models\\' training data, it may also downplay non-English views.[147]\\n\\n#### Stereotyping\\n\\n[edit]\\n\\nAI models can reinforce a wide range of stereotypes, including those based on\\ngender, ethnicity, age, nationality, religion, or occupation. This can lead to\\noutputs that unfairly generalize or caricature groups of people, sometimes in\\nharmful or derogatory ways.[148]\\n\\nNotably, gender bias refers to the tendency of these models to produce outputs\\nthat are unfairly prejudiced towards one gender over another. This bias\\ntypically arises from the data on which these models are trained. Large\\nlanguage models often assign roles and characteristics based on traditional\\ngender norms.[146] For example, it might associate nurses or secretaries\\npredominantly with women and engineers or CEOs with men.[149]\\n\\n#### Selection bias\\n\\n[edit]\\n\\nSelection bias refers the inherent tendency of large language models to favor\\ncertain option identifiers irrespective of the actual content of the options.\\nThis bias primarily stems from token bias—that is, the model assigns a higher\\na priori probability to specific answer tokens (such as “A”) when generating\\nresponses. As a result, when the ordering of options is altered (for example,\\nby systematically moving the correct answer to different positions), the\\nmodel’s performance can fluctuate significantly. This phenomenon undermines\\nthe reliability of large language models in multiple-choice\\nsettings.[150][151]\\n\\n#### Political bias\\n\\n[edit]\\n\\nPolitical bias refers to the tendency of algorithms to systematically favor\\ncertain political viewpoints, ideologies, or outcomes over others. Language\\nmodels may also exhibit political biases. Since the training data includes a\\nwide range of political opinions and coverage, the models might generate\\nresponses that lean towards particular political ideologies or viewpoints,\\ndepending on the prevalence of those views in the data.[152]\\n\\n### Energy demands\\n\\n[edit]\\n\\nThe energy demands of LLMs have grown along with their size and capabilities.\\nData centers that enable LLM training require substantial amounts of\\nelectricity. Much of that electricity is generated by non-renewable resources\\nthat create greenhouse gases and contribute to climate change.[153] Nuclear\\npower and geothermal energy are two options tech companies are exploring to\\nmeet the sizable energy demands of LLM training.[154] The significant expense\\nof investing in geothermal solutions has led to major shale producers like\\nChevron and Exxon Mobil advocating for tech companies to use electricity\\nproduced via natural gas to fuel their large energy demands.[155]\\n\\n## See also\\n\\n[edit]\\n\\n  * Foundation models\\n  * List of large language models\\n  * List of chatbots\\n\\n## References\\n\\n[edit]\\n\\n  1. ^ _**a**_ _**b**_ _**c**_ Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). \"Language Models are Few-Shot Learners\" (PDF). _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 1877–1901\\\\. Archived (PDF) from the original on 2023-11-17. Retrieved 2023-03-14.\\n  2. **^** Fathallah, Nadeen; Das, Arunav; De Giorgis, Stefano; Poltronieri, Andrea; Haase, Peter; Kovriguina, Liubov (2024-05-26). _NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning_ (PDF). Extended Semantic Web Conference 2024. Hersonissos, Greece.\\n  3. **^** Manning, Christopher D. (2022). \"Human Language Understanding & Reasoning\". _Daedalus_. **151** (2): 127–138\\\\. doi:10.1162/daed_a_01905. S2CID 248377870. Archived from the original on 2023-11-17. Retrieved 2023-03-09.\\n  4. **^** Goodman, Joshua (2001-08-09), _A Bit of Progress in Language Modeling_ , arXiv:cs/0108005, Bibcode:2001cs........8005G\\n  5. **^** Kilgarriff, Adam; Grefenstette, Gregory (September 2003). \"Introduction to the Special Issue on the Web as Corpus\". _Computational Linguistics_. **29** (3): 333–347\\\\. doi:10.1162/089120103322711569. ISSN 0891-2017.\\n  6. **^** Banko, Michele; Brill, Eric (2001). \"Scaling to very very large corpora for natural language disambiguation\". _Proceedings of the 39th Annual Meeting on Association for Computational Linguistics - ACL \\'01_. Morristown, NJ, USA: Association for Computational Linguistics: 26–33\\\\. doi:10.3115/1073012.1073017.\\n  7. **^** Resnik, Philip; Smith, Noah A. (September 2003). \"The Web as a Parallel Corpus\". _Computational Linguistics_. **29** (3): 349–380\\\\. doi:10.1162/089120103322711578. ISSN 0891-2017. Archived from the original on 2024-06-07. Retrieved 2024-06-07.\\n  8. **^** Halevy, Alon; Norvig, Peter; Pereira, Fernando (March 2009). \"The Unreasonable Effectiveness of Data\". _IEEE Intelligent Systems_. **24** (2): 8–12\\\\. doi:10.1109/MIS.2009.36. ISSN 1541-1672.\\n  9. **^** Chen, Leiyu; Li, Shaobo; Bai, Qiang; Yang, Jing; Jiang, Sanlong; Miao, Yanming (2021). \"Review of Image Classification Algorithms Based on Convolutional Neural Networks\". _Remote Sensing_. **13** (22): 4712. Bibcode:2021RemS...13.4712C. doi:10.3390/rs13224712.\\n  10. **^** Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017). \"Attention is All you Need\" (PDF). _Advances in Neural Information Processing Systems_. **30**. Curran Associates, Inc. Archived (PDF) from the original on 2024-02-21. Retrieved 2024-01-21.\\n  11. **^** Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n  12. **^** Rogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What We Know About How BERT Works\". _Transactions of the Association for Computational Linguistics_. **8** : 842–866\\\\. arXiv:2002.12327. doi:10.1162/tacl_a_00349. S2CID 211532403. Archived from the original on 2022-04-03. Retrieved 2024-01-21.\\n  13. **^** Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\". _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. pp. 1223–1243\\\\. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\n  14. **^** Hern, Alex (14 February 2019). \"New AI fake text generator may be too dangerous to release, say creators\". _The Guardian_. Archived from the original on 14 February 2019. Retrieved 20 January 2024.\\n  15. **^** \"ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months\". Euronews. November 30, 2023. Archived from the original on January 14, 2024. Retrieved January 20, 2024.\\n  16. **^** Heaven, Will (March 14, 2023). \"GPT-4 is bigger and better than ChatGPT—but OpenAI won\\'t say why\". MIT Technology Review. Archived from the original on March 17, 2023. Retrieved January 20, 2024.\\n  17. **^** Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\". _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. pp. 1223–1243\\\\. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08.\\n  18. **^** \"Parameters in notable artificial intelligence systems\". _ourworldindata.org_. November 30, 2023. Retrieved January 20, 2024.\\n  19. **^** Sharma, Shubham (2025-01-20). \"Open-source DeepSeek-R1 uses pure reinforcement learning to match OpenAI o1 — at 95% less cost\". _VentureBeat_. Retrieved 2025-01-26.\\n  20. **^** Zia, Dr Tehseen (2024-01-08). \"Unveiling of Large Multimodal Models: Shaping the Landscape of Language Models in 2024\". _Unite.AI_. Retrieved 2024-12-28.\\n  21. **^** Peng, Bo; et al. (2023). \"RWKV: Reinventing RNNS for the Transformer Era\". arXiv:2305.13048 [cs.CL].\\n  22. **^** Merritt, Rick (2022-03-25). \"What Is a Transformer Model?\". _NVIDIA Blog_. Archived from the original on 2023-11-17. Retrieved 2023-07-25.\\n  23. **^** Gu, Albert; Dao, Tri (2023-12-01), _Mamba: Linear-Time Sequence Modeling with Selective State Spaces_ , arXiv:2312.00752\\n  24. **^** Kaushal, Ayush; Mahowald, Kyle (2022-06-06), _What do tokens know about their characters and how do they know it?_ , arXiv:2206.02608\\n  25. **^** Yennie Jun (2023-05-03). \"All languages are NOT created (tokenized) equal\". _Language models cost much more in some languages than others_. Archived from the original on 2023-08-17. Retrieved 2023-08-17. \"In other words, to express the same sentiment, some languages require up to 10 times more tokens.\"\\n  26. **^** Petrov, Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). \"Language Model Tokenizers Introduce Unfairness Between Languages\". _NeurIPS_. arXiv:2305.15425. Archived from the original on December 15, 2023. Retrieved September 16, 2023 – via openreview.net.\\n  27. **^** \"OpenAI API\". _platform.openai.com_. Archived from the original on April 23, 2023. Retrieved 2023-04-30.\\n  28. ^ _**a**_ _**b**_ Paaß, Gerhard; Giesselbach, Sven (2022). \"Pre-trained Language Models\". _Foundation Models for Natural Language Processing_. Artificial Intelligence: Foundations, Theory, and Algorithms. pp. 19–78\\\\. doi:10.1007/978-3-031-23190-2_2. ISBN 9783031231902. Archived from the original on 3 August 2023. Retrieved 3 August 2023.\\n  29. **^** Petrov, Aleksandar; Emanuele La Malfa; Torr, Philip H. S.; Bibi, Adel (2023). \"Language Model Tokenizers Introduce Unfairness Between Languages\". arXiv:2305.15425 [cs.CL].\\n  30. **^** Lundberg, Scott (2023-12-12). \"The Art of Prompt Design: Prompt Boundaries and Token Healing\". _Medium_. Retrieved 2024-08-05.\\n  31. **^** Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew, William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret; Gardner, Matt (2021). \"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\". arXiv:2104.08758 [cs.CL].\\n  32. **^** Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; Carlini, Nicholas (May 2022). \"Deduplicating Training Data Makes Language Models Better\" (PDF). _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics_. 1: Long Papers: 8424–8445\\\\. doi:10.18653/v1/2022.acl-long.577.\\n  33. **^** Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar, Suriya; Lee, Yin Tat (2023-09-11), _Textbooks Are All You Need II: phi-1.5 technical report_ , arXiv:2309.05463\\n  34. **^** Lin, Zhenghao; Gou, Zhibin; Gong, Yeyun; Liu, Xiao; Shen, Yelong; Xu, Ruochen; Lin, Chen; Yang, Yujiu; Jiao, Jian (2024-04-11). \"Rho-1: Not All Tokens Are What You Need\". arXiv:2404.07965 [cs.CL].\\n  35. **^** Brown, Tom B.; et al. (2020). \"Language Models are Few-Shot Learners\". arXiv:2005.14165 [cs.CL].\\n  36. **^** Abdin, Marah; Jacobs, Sam Ade; Awan, Ammar Ahmad; Aneja, Jyoti; Awadallah, Ahmed; Awadalla, Hany; Bach, Nguyen; Bahree, Amit; Bakhtiari, Arash (2024-04-23). \"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\". arXiv:2404.14219 [cs.CL].\\n  37. **^** Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \"Training language models to follow instructions with human feedback\". arXiv:2203.02155 [cs.CL].\\n  38. **^** Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Model with Self Generated Instructions\". arXiv:2212.10560 [cs.CL].\\n  39. **^** Shazeer, Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geoffrey; Dean, Jeff (2017-01-01). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\". arXiv:1701.06538 [cs.LG].\\n  40. **^** Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng (2021-01-12). \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\". arXiv:2006.16668 [cs.CL].\\n  41. **^** Dai, Andrew M; Du, Nan (December 9, 2021). \"More Efficient In-Context Learning with GLaM\". _ai.googleblog.com_. Archived from the original on 2023-03-12. Retrieved 2023-03-09.\\n  42. ^ _**a**_ _**b**_ _**c**_ Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). \"Emergent Abilities of Large Language Models\". _Transactions on Machine Learning Research_. ISSN 2835-8856. Archived from the original on 22 March 2023. Retrieved 19 March 2023.\\n  43. **^** Allamar, Jay. \"Illustrated transformer\". Archived from the original on 2023-07-25. Retrieved 2023-07-29.\\n  44. **^** Allamar, Jay. \"The Illustrated GPT-2 (Visualizing Transformer Language Models)\". Retrieved 2023-08-01.\\n  45. **^** \"Our next-generation model: Gemini 1.5\". _Google_. 15 February 2024. Archived from the original on 18 February 2024. Retrieved 18 February 2024.\\n  46. **^** \"Long context prompting for Claude 2.1\". December 6, 2023. Archived from the original on August 27, 2024. Retrieved January 20, 2024.\\n  47. **^** \"Rate limits\". _openai.com_. Archived from the original on February 2, 2024. Retrieved January 20, 2024.\\n  48. **^** Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). \"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\". _Proceedings of the Australasian Computer Science Week Multiconference_. pp. 1–4\\\\. arXiv:2104.10810. doi:10.1145/3373017.3373028. ISBN 9781450376976. S2CID 211040895.\\n  49. ^ _**a**_ _**b**_ _**c**_ Jurafsky, Dan; Martin, James H. (7 January 2023). _Speech and Language Processing_ (PDF) (3rd edition draft ed.). Archived (PDF) from the original on 23 March 2023. Retrieved 24 May 2022.\\n  50. **^** \"From bare metal to a 70B model: infrastructure set-up and scripts\". _imbue.com_. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\n  51. **^** \"metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq\". _GitHub_. Archived from the original on 2024-01-24. Retrieved 2024-07-24.\\n  52. **^** Albrecht, Josh (2024-07-23). \"State of the Art: Training >70B LLMs on 10,000 H100 clusters\". _www.latent.space_. Retrieved 2024-07-24.\\n  53. **^** Maslej, Nestor; Fattorini, Loredana; Brynjolfsson, Erik; Etchemendy, John; Ligett, Katrina; Lyons, Terah; Manyika, James; Ngo, Helen; Niebles, Juan Carlos (2023-10-05), _Artificial Intelligence Index Report 2023_ , arXiv:2310.03715\\n  54. ^ _**a**_ _**b**_ Section 2.1 and Table 1, Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \"Scaling Laws for Neural Language Models\". arXiv:2001.08361 [cs.LG].\\n  55. **^** Gao, Luyu; Madaan, Aman; Zhou, Shuyan; Alon, Uri; Liu, Pengfei; Yang, Yiming; Callan, Jamie; Neubig, Graham (2022-11-01). \"PAL: Program-aided Language Models\". arXiv:2211.10435 [cs.CL].\\n  56. **^** \"PAL: Program-aided Language Models\". _reasonwithpal.com_. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\n  57. **^** Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer, Luke; Tulio Ribeiro, Marco (2023-03-01). \"ART: Automatic multi-step reasoning and tool-use for large language models\". arXiv:2303.09014 [cs.CL].\\n  58. **^** Liang, Yaobo; Wu, Chenfei; Song, Ting; Wu, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang; Wang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\". arXiv:2303.16434 [cs.AI].\\n  59. **^** Patil, Shishir G.; Zhang, Tianjun; Wang, Xin; Gonzalez, Joseph E. (2023-05-01). \"Gorilla: Large Language Model Connected with Massive APIs\". arXiv:2305.15334 [cs.CL].\\n  60. **^** Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler, Heinrich; Lewis, Mike; Yih, Wen-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\". _Advances in Neural Information Processing Systems_. **33**. Curran Associates, Inc.: 9459–9474\\\\. arXiv:2005.11401. Archived from the original on 2023-06-12. Retrieved 2023-06-12.\\n  61. **^** \"The Growth Behind LLM-based Autonomous Agents\". _KDnuggets_. October 23, 2023.\\n  62. **^** Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \"ReAct: Synergizing Reasoning and Acting in Language Models\". arXiv:2210.03629 [cs.CL].\\n  63. **^** Wu, Yue; Prabhumoye, Shrimai; Min, So Yeon (24 May 2023). \"SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning\". arXiv:2305.15486 [cs.AI].\\n  64. **^** Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\". arXiv:2302.01560 [cs.AI].\\n  65. **^** Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01). \"Reflexion: Language Agents with Verbal Reinforcement Learning\". arXiv:2303.11366 [cs.AI].\\n  66. **^** Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; Wang, Zhen; Zhe Wang, Daisy; Hu, Zhiting (2023-05-01). \"Reasoning with Language Model is Planning with World Model\". arXiv:2305.14992 [cs.CL].\\n  67. **^** Zhang, Jenny; Lehman, Joel; Stanley, Kenneth; Clune, Jeff (2 June 2023). \"OMNI: Open-endedness via Models of human Notions of Interestingness\". arXiv:2306.01711 [cs.AI].\\n  68. ^ _**a**_ _**b**_ \"Voyager | An Open-Ended Embodied Agent with Large Language Models\". _voyager.minedojo.org_. Archived from the original on 2023-06-08. Retrieved 2023-06-09.\\n  69. **^** Park, Joon Sung; O\\'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-04-01). \"Generative Agents: Interactive Simulacra of Human Behavior\". arXiv:2304.03442 [cs.HC].\\n  70. **^** Mann, Tobias. \"How to run an LLM locally on your PC in less than 10 minutes\". _www.theregister.com_. Retrieved 2024-05-17.\\n  71. **^** Nagel, Markus; Amjad, Rana Ali; Baalen, Mart Van; Louizos, Christos; Blankevoort, Tijmen (2020-11-21). \"Up or Down? Adaptive Rounding for Post-Training Quantization\". _Proceedings of the 37th International Conference on Machine Learning_. PMLR: 7197–7206\\\\. Archived from the original on 2023-06-14. Retrieved 2023-06-14.\\n  72. **^** Polino, Antonio; Pascanu, Razvan; Alistarh, Dan (2018-02-01). \"Model compression via distillation and quantization\". arXiv:1802.05668 [cs.NE].\\n  73. **^** Frantar, Elias; Ashkboos, Saleh; Hoefler, Torsten; Alistarh, Dan (2022-10-01). \"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\". arXiv:2210.17323 [cs.LG].\\n  74. **^** Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, Vage; Kuznedelev, Denis; Frantar, Elias; Ashkboos, Saleh; Borzunov, Alexander; Hoefler, Torsten; Alistarh, Dan (2023-06-01). \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\". arXiv:2306.03078 [cs.CL].\\n  75. **^** Grootendorst, Maarten. \"A Visual Guide to Quantization\". _newsletter.maartengrootendorst.com_. Archived from the original on 31 Jul 2024. Retrieved 2024-07-31.\\n  76. **^** Dettmers, Tim; Pagnoni, Artidoro; Holtzman, Ari; Zettlemoyer, Luke (2023-05-01). \"QLoRA: Efficient Finetuning of Quantized LLMs\". arXiv:2305.14314 [cs.LG].\\n  77. **^** Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Rich (2014-06-18). \"Multimodal Neural Language Models\". _Proceedings of the 31st International Conference on Machine Learning_. PMLR: 595–603\\\\. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  78. **^** Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E (2012). \"ImageNet Classification with Deep Convolutional Neural Networks\". _Advances in Neural Information Processing Systems_. **25**. Curran Associates, Inc. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  79. **^** Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi (2015). \"VQA: Visual Question Answering\". _ICCV_ : 2425–2433\\\\. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  80. **^** Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\". arXiv:2301.12597 [cs.CV].\\n  81. **^** Alayrac, Jean-Baptiste; Donahue, Jeff; Luc, Pauline; Miech, Antoine; Barr, Iain; Hasson, Yana; Lenc, Karel; Mensch, Arthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong, Zhitao (2022-12-06). \"Flamingo: a Visual Language Model for Few-Shot Learning\". _Advances in Neural Information Processing Systems_. **35** : 23716–23736\\\\. arXiv:2204.14198. Archived from the original on 2023-07-02. Retrieved 2023-07-02.\\n  82. **^** Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; Lynch, Corey; Chowdhery, Aakanksha; Ichter, Brian; Wahid, Ayzaan; Tompson, Jonathan; Vuong, Quan; Yu, Tianhe; Huang, Wenlong; Chebotar, Yevgen; Sermanet, Pierre; Duckworth, Daniel; Levine, Sergey (2023-03-01). \"PaLM-E: An Embodied Multimodal Language Model\". arXiv:2303.03378 [cs.LG].\\n  83. **^** Liu, Haotian; Li, Chunyuan; Wu, Qingyang; Lee, Yong Jae (2023-04-01). \"Visual Instruction Tuning\". arXiv:2304.08485 [cs.CV].\\n  84. **^** Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\". arXiv:2306.02858 [cs.CL].\\n  85. **^** OpenAI (2023-03-27). \"GPT-4 Technical Report\". arXiv:2303.08774 [cs.CL].\\n  86. **^** OpenAI (September 25, 2023). \"GPT-4V(ision) System Card\" (PDF).\\n  87. **^** Pichai, Sundar (10 May 2023), _Google Keynote (Google I/O \\'23)_, timestamp 15:31, retrieved 2023-07-02\\n  88. **^** Wiggers, Kyle (11 September 2024). \"Mistral releases Pixtral 12B, its first multimodal model\". _TechCrunch_. Retrieved 14 September 2024.\\n  89. ^ _**a**_ _**b**_ \"Introducing OpenAI o1-preview\". _OpenAI_. 2024-09-12. Retrieved 2025-02-03.\\n  90. ^ _**a**_ _**b**_ Metz, Cade (2024-12-20). \"OpenAI Unveils New A.I. That Can \\'Reason\\' Through Math and Science Problems\". _The New York Times_. Retrieved 2025-02-03.\\n  91. **^** Gibney, Elizabeth (2025-01-30). \"China\\'s cheap, open AI model DeepSeek thrills scientists\". _Nature_. Retrieved 2025-02-03.\\n  92. **^** Lin, Belle (2025-02-05). \"Why Amazon is Betting on \\'Automated Reasoning\\' to Reduce AI\\'s Hallucinations: The tech giant says an obscure field that combines AI and math can mitigate—but not completely eliminate—AI\\'s propensity to provide wrong answers\". _Wall Street Journal_. ISSN 0099-9660.\\n  93. **^** Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \"Training Compute-Optimal Large Language Models\". arXiv:2203.15556 [cs.CL].\\n  94. ^ _**a**_ _**b**_ Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). \"Broken Neural Scaling Laws\". arXiv:2210.14891 [cs.LG].\\n  95. **^** \"137 emergent abilities of large language models\". _Jason Wei_. Retrieved 2023-06-24.\\n  96. **^** Bowman, Samuel R. (2023). \"Eight Things to Know about Large Language Models\". arXiv:2304.00612 [cs.CL].\\n  97. **^** Mukherjee, Anirban; Chang, Hannah (2024). \"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption\". arXiv:2403.09404 [cs.AI].\\n  98. **^** Hahn, Michael; Goyal, Navin (2023-03-14). \"A Theory of Emergent In-Context Learning as Implicit Structure Induction\". arXiv:2303.07971 [cs.LG].\\n  99. **^** Pilehvar, Mohammad Taher; Camacho-Collados, Jose (June 2019). \"Proceedings of the 2019 Conference of the North\". _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_. Minneapolis, Minnesota: Association for Computational Linguistics: 1267–1273\\\\. doi:10.18653/v1/N19-1128. S2CID 102353817. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\n  100. **^** \"WiC: The Word-in-Context Dataset\". _pilehvar.github.io_. Archived from the original on 2023-06-27. Retrieved 2023-06-27.\\n  101. **^** Patel, Roma; Pavlick, Ellie (2021-10-06). \"Mapping Language Models to Grounded Conceptual Spaces\". _ICLR_. Archived from the original on 2023-06-24. Retrieved 2023-06-27.\\n  102. **^** _A Closer Look at Large Language Models Emergent Abilities Archived 2023-06-24 at the Wayback Machine_ (Yao Fu, Nov 20, 2022)\\n  103. **^** Ornes, Stephen (March 16, 2023). \"The Unpredictable Abilities Emerging From Large AI Models\". _Quanta Magazine_. Archived from the original on March 16, 2023. Retrieved March 16, 2023.\\n  104. **^** Schaeffer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \"Are Emergent Abilities of Large Language Models a Mirage?\". arXiv:2304.15004 [cs.AI].\\n  105. **^** Blank, Idan A. (November 2023). \"What are large language models supposed to model?\". _Trends in Cognitive Sciences_. **27** (11): 987–989\\\\. doi:10.1016/j.tics.2023.08.006. PMID 37659920.\\n  106. **^** Li, Kenneth; Hopkins, Aspen K.; Bau, David; Viégas, Fernanda; Pfister, Hanspeter; Wattenberg, Martin (2022-10-01). \"Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\". arXiv:2210.13382 [cs.LG].\\n  107. **^** \"Large Language Model: world models or surface statistics?\". _The Gradient_. 2023-01-21. Retrieved 2023-06-12.\\n  108. **^** Jin, Charles; Rinard, Martin (2023-05-01). \"Evidence of Meaning in Language Models Trained on Programs\". arXiv:2305.11169 [cs.LG].\\n  109. **^** Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \"Progress measures for grokking via mechanistic interpretability\". arXiv:2301.05217 [cs.LG].\\n  110. ^ _**a**_ _**b**_ _**c**_ _**d**_ _**e**_ Mitchell, Melanie; Krakauer, David C. (28 March 2023). \"The debate over understanding in AI\\'s large language models\". _Proceedings of the National Academy of Sciences_. **120** (13): e2215907120. arXiv:2210.13966. Bibcode:2023PNAS..12015907M. doi:10.1073/pnas.2215907120. PMC 10068812. PMID 36943882.\\n  111. **^** Metz, Cade (16 May 2023). \"Microsoft Says New A.I. Shows Signs of Human Reasoning\". _The New York Times_.\\n  112. ^ _**a**_ _**b**_ Bubeck, Sébastien; Chandrasekaran, Varun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar, Ece; Lee, Peter; Lee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\". arXiv:2303.12712 [cs.CL].\\n  113. **^** \"Anthropic CEO Dario Amodei pens a smart look at our AI future\". _Fast Company_. October 17, 2024.\\n  114. **^** \"ChatGPT is more like an \\'alien intelligence\\' than a human brain, says futurist\". _ZDNET_. 2023. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\n  115. ^ _**a**_ _**b**_ Newport, Cal (13 April 2023). \"What Kind of Mind Does ChatGPT Have?\". _The New Yorker_. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\\n  116. **^** Roose, Kevin (30 May 2023). \"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\" _The New York Times_. Archived from the original on 30 May 2023. Retrieved 12 June 2023.\\n  117. **^** \"The A to Z of Artificial Intelligence\". _Time Magazine_. 13 April 2023. Archived from the original on 16 June 2023. Retrieved 12 June 2023.\\n  118. **^** Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). \"Survey of Hallucination in Natural Language Generation\" (pdf). _ACM Computing Surveys_. **55** (12). Association for Computing Machinery: 1–38\\\\. arXiv:2202.03629. doi:10.1145/3571730. S2CID 246652372. Archived from the original on 26 March 2023. Retrieved 15 January 2023.\\n  119. **^** Varshney, Neeraj; Yao, Wenlin; Zhang, Hongming; Chen, Jianshu; Yu, Dong (2023). \"A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation\". arXiv:2307.03987 [cs.CL].\\n  120. **^** Lakoff, George (1999). _Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm_. New York Basic Books. pp. 569–583\\\\. ISBN 978-0-465-05674-3.\\n  121. **^** Evans, Vyvyan. (2014). _The Language Myth_. Cambridge University Press. ISBN 978-1-107-04396-1.\\n  122. **^** Friston, Karl J. (2022). _Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference_. The MIT Press. ISBN 978-0-262-36997-8.\\n  123. ^ _**a**_ _**b**_ Huyen, Chip (October 18, 2019). \"Evaluation Metrics for Language Modeling\". _The Gradient_. Retrieved January 14, 2024.\\n  124. ^ _**a**_ _**b**_ Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). \"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\". arXiv:1905.10044 [cs.CL].\\n  125. ^ _**a**_ _**b**_ _**c**_ Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; Wang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang, Junjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang, Xinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Yun; Wen, Ji-Rong (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\\n  126. **^** _openai/simple-evals_, OpenAI, 2024-05-28, retrieved 2024-05-28\\n  127. **^** _openai/evals_, OpenAI, 2024-05-28, archived from the original on 2024-05-08, retrieved 2024-05-28\\n  128. **^** \"Sanitized open-source datasets for natural language and code understanding: how we evaluated our 70B model\". _imbue.com_. Archived from the original on 2024-07-26. Retrieved 2024-07-24.\\n  129. **^** Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R. (November 2020). \"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models\". In Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang (ed.). _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. Association for Computational Linguistics. pp. 1953–1967\\\\. arXiv:2010.00133. doi:10.18653/v1/2020.emnlp-main.154.`{{cite conference}}`: CS1 maint: multiple names: authors list (link)\\n  130. **^** Nadeem, Moin and Bethke, Anna and Reddy, Siva (August 2021). \"StereoSet: Measuring stereotypical bias in pretrained language models\". In Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto (ed.). _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Association for Computational Linguistics. pp. 5356–5371\\\\. arXiv:2004.09456. doi:10.18653/v1/2021.acl-long.416.`{{cite conference}}`: CS1 maint: multiple names: authors list (link)\\n  131. **^** Simpson, Shmona and Nukpezah, Jonathan and Kie Brooks and Pandya, Raaghav (17 December 2024). \"Parity benchmark for measuring bias in LLMs\". _AI and Ethics_. Springer. doi:10.1007/s43681-024-00613-4.`{{cite journal}}`: CS1 maint: multiple names: authors list (link)\\n  132. **^** Srivastava, Aarohi; et al. (2022). \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\". arXiv:2206.04615 [cs.CL].\\n  133. **^** Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\". arXiv:2109.07958 [cs.CL].\\n  134. ^ _**a**_ _**b**_ Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \"HellaSwag: Can a Machine Really Finish Your Sentence?\". arXiv:1905.07830 [cs.CL].\\n  135. **^** \"Prepare for truly useful large language models\". _Nature Biomedical Engineering_. **7** (2): 85–86\\\\. 7 March 2023. doi:10.1038/s41551-023-01012-6. PMID 36882584. S2CID 257403466.\\n  136. **^** \"Your job is (probably) safe from artificial intelligence\". _The Economist_. 7 May 2023. Archived from the original on 17 June 2023. Retrieved 18 June 2023.\\n  137. **^** \"Generative AI Could Raise Global GDP by 7%\". _Goldman Sachs_. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\n  138. **^** Peng, Zhencan; Wang, Zhizhi; Deng, Dong (13 June 2023). \"Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation\" (PDF). _Proceedings of the ACM on Management of Data_. **1** (2): 1–18\\\\. doi:10.1145/3589324. S2CID 259213212. Archived (PDF) from the original on 2024-08-27. Retrieved 2024-01-20. Citing Lee et al 2022.\\n  139. **^** Peng, Wang & Deng 2023, p. 8.\\n  140. **^** Stephen Council (1 Dec 2023). \"How Googlers cracked an SF rival\\'s tech model with a single word\". SFGATE. Archived from the original on 16 December 2023.\\n  141. **^** Alba, Davey (1 May 2023). \"AI chatbots have been used to create dozens of news content farms\". _The Japan Times_. Retrieved 18 June 2023.\\n  142. **^** \"Could chatbots help devise the next pandemic virus?\". _Science_. 14 June 2023. doi:10.1126/science.adj2463. Archived from the original on 18 June 2023. Retrieved 18 June 2023.\\n  143. **^** Hubinger, Evan (10 January 2024). \"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\". arXiv:2401.05566 [cs.CR].\\n  144. **^** Kang, Daniel (2023). \"Exploiting programmatic behavior of LLMs: Dual-use through standard security attacks\". arXiv:2302.05733 [cs.CR].\\n  145. **^** Wang, Yongge (20 June 2024). \"Encryption Based Covert Channel for Large Language Models\" (PDF). IACR ePrint 2024/586. Archived (PDF) from the original on 24 June 2024. Retrieved 24 June 2024.\\n  146. ^ _**a**_ _**b**_ Stokel-Walker, Chris (November 22, 2023). \"ChatGPT Replicates Gender Bias in Recommendation Letters\". _Scientific American_. Archived from the original on 2023-12-29. Retrieved 2023-12-29.\\n  147. **^** Luo, Queenie; Puett, Michael J.; Smith, Michael D. (2023-03-28). \"A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube\". arXiv:2303.16281v2 [cs.CY].\\n  148. **^** Cheng, Myra; Durmus, Esin; Jurafsky, Dan (2023-05-29), _Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models_ , arXiv:2305.18189\\n  149. **^** Kotek, Hadas; Dockum, Rikker; Sun, David (2023-11-05). \"Gender bias and stereotypes in Large Language Models\". _Proceedings of the ACM Collective Intelligence Conference_. CI \\'23. New York, NY, USA: Association for Computing Machinery. pp. 12–24\\\\. doi:10.1145/3582269.3615599. ISBN 979-8-4007-0113-9.\\n  150. **^** Choi, Hyeong Kyu; Xu, Weijie; Xue, Chi; Eckman, Stephanie; Reddy, Chandan K. (2024-09-27), _Mitigating Selection Bias with Node Pruning and Auxiliary Options_ , arXiv:2409.18857\\n  151. **^** Zheng, Chujie; Zhou, Hao; Meng, Fandong; Zhou, Jie; Huang, Minlie (2023-09-07), _Large Language Models Are Not Robust Multiple Choice Selectors_ , arXiv:2309.03882\\n  152. **^** Heikkilä, Melissa (August 7, 2023). \"AI language models are rife with different political biases\". _MIT Technology Review_. Retrieved 2023-12-29.\\n  153. **^** Mehta, Sourabh (2024-07-03). \"How Much Energy Do LLMs Consume? Unveiling the Power Behind AI\". _Association of Data Scientists_. Retrieved 2025-01-27.\\n  154. **^** \"Artificial Intelligence wants to go nuclear. Will it work?\". _NPR_. Retrieved 2025-01-27.\\n  155. **^** Roy, Dareen (December 19, 2024). \"AI\\'s energy hunger fuels geothermal startups but natgas rivalry clouds future\". _Reuters_.\\n\\n  \\n\\n## Further reading\\n\\n[edit]\\n\\n  * Jurafsky, Dan, Martin, James. H. _Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition_, 3rd Edition draft, 2023.\\n  * Zhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\\n  * Kaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\\n  * Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". _National Science Review_. **11** (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.\\n  * \"AI Index Report 2024 – Artificial Intelligence Index\". _aiindex.stanford.edu_. Retrieved 2024-05-05.\\n  * Frank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". _Nature Reviews Psychology_. **2** (8): 451–452\\\\. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\\n\\n  * v\\n  * t\\n  * e\\n\\nNatural language processing  \\n---  \\nGeneral terms|\\n\\n  * AI-complete\\n  * Bag-of-words\\n  * n-gram\\n    * Bigram\\n    * Trigram\\n  * Computational linguistics\\n  * Natural language understanding\\n  * Stop words\\n  * Text processing\\n\\n  \\nText analysis|\\n\\n  * Argument mining\\n  * Collocation extraction\\n  * Concept mining\\n  * Coreference resolution\\n  * Deep linguistic processing\\n  * Distant reading\\n  * Information extraction\\n  * Named-entity recognition\\n  * Ontology learning\\n  * Parsing\\n    * Semantic parsing\\n    * Syntactic parsing\\n  * Part-of-speech tagging\\n  * Semantic analysis\\n  * Semantic role labeling\\n  * Semantic decomposition\\n  * Semantic similarity\\n  * Sentiment analysis\\n\\n  * Terminology extraction\\n  * Text mining\\n  * Textual entailment\\n  * Truecasing\\n  * Word-sense disambiguation\\n  * Word-sense induction\\n\\n| Text segmentation|\\n\\n  * Compound-term processing\\n  * Lemmatisation\\n  * Lexical analysis\\n  * Text chunking\\n  * Stemming\\n  * Sentence segmentation\\n  * Word segmentation\\n\\n  \\n---|---  \\n  \\nAutomatic summarization|\\n\\n  * Multi-document summarization\\n  * Sentence extraction\\n  * Text simplification\\n\\n  \\nMachine translation|\\n\\n  * Computer-assisted\\n  * Example-based\\n  * Rule-based\\n  * Statistical\\n  * Transfer-based\\n  * Neural\\n\\n  \\nDistributional semantics models|\\n\\n  * BERT\\n  * Document-term matrix\\n  * Explicit semantic analysis\\n  * fastText\\n  * GloVe\\n  * Language model (large)\\n  * Latent semantic analysis\\n  * Seq2seq\\n  * Word embedding\\n  * Word2vec\\n\\n  \\nLanguage resources,  \\ndatasets and corpora| | Types and  \\nstandards|\\n\\n  * Corpus linguistics\\n  * Lexical resource\\n  * Linguistic Linked Open Data\\n  * Machine-readable dictionary\\n  * Parallel text\\n  * PropBank\\n  * Semantic network\\n  * Simple Knowledge Organization System\\n  * Speech corpus\\n  * Text corpus\\n  * Thesaurus (information retrieval)\\n  * Treebank\\n  * Universal Dependencies\\n\\n  \\n---|---  \\nData|\\n\\n  * BabelNet\\n  * Bank of English\\n  * DBpedia\\n  * FrameNet\\n  * Google Ngram Viewer\\n  * UBY\\n  * WordNet\\n  * Wikidata\\n\\n  \\n  \\nAutomatic identification  \\nand data capture|\\n\\n  * Speech recognition\\n  * Speech segmentation\\n  * Speech synthesis\\n  * Natural language generation\\n  * Optical character recognition\\n\\n  \\nTopic model|\\n\\n  * Document classification\\n  * Latent Dirichlet allocation\\n  * Pachinko allocation\\n\\n  \\nComputer-assisted  \\nreviewing|\\n\\n  * Automated essay scoring\\n  * Concordancer\\n  * Grammar checker\\n  * Predictive text\\n  * Pronunciation assessment\\n  * Spell checker\\n\\n  \\nNatural language  \\nuser interface|\\n\\n  * Chatbot\\n  * Interactive fiction (c.f. Syntax guessing)\\n  * Question answering\\n  * Virtual assistant\\n  * Voice user interface\\n\\n  \\nRelated|\\n\\n  * Formal semantics\\n  * Hallucination\\n  * Natural Language Toolkit\\n  * spaCy\\n\\n  \\n  \\n  * v\\n  * t\\n  * e\\n\\nArtificial intelligence (AI)  \\n---  \\nHistory (timeline)  \\nConcepts|\\n\\n  * Parameter\\n    * Hyperparameter\\n  * Loss functions\\n  * Regression\\n    * Bias–variance tradeoff\\n    * Double descent\\n    * Overfitting\\n  * Clustering\\n  * Gradient descent\\n    * SGD\\n    * Quasi-Newton method\\n    * Conjugate gradient method\\n  * Backpropagation\\n  * Attention\\n  * Convolution\\n  * Normalization\\n    * Batchnorm\\n  * Activation\\n    * Softmax\\n    * Sigmoid\\n    * Rectifier\\n  * Gating\\n  * Weight initialization\\n  * Regularization\\n  * Datasets\\n    * Augmentation\\n  * Prompt engineering\\n  * Reinforcement learning\\n    * Q-learning\\n    * SARSA\\n    * Imitation\\n    * Policy gradient\\n  * Diffusion\\n  * Latent diffusion model\\n  * Autoregression\\n  * Adversary\\n  * RAG\\n  * Uncanny valley\\n  * RLHF\\n  * Self-supervised learning\\n  * Recursive self-improvement\\n  * Word embedding\\n  * Hallucination\\n\\n  \\nApplications|\\n\\n  * Machine learning\\n    * In-context learning\\n  * Artificial neural network\\n    * Deep learning\\n  * Language model\\n    * Large language model\\n    * NMT\\n  * Artificial general intelligence\\n\\n  \\nImplementations| | Audio–visual| \\n\\n  * AlexNet\\n  * WaveNet\\n  * Human image synthesis\\n  * HWR\\n  * OCR\\n  * Speech synthesis\\n    * 15.ai\\n    * ElevenLabs\\n  * Speech recognition\\n    * Whisper\\n  * Facial recognition\\n  * AlphaFold\\n  * Text-to-image models\\n    * Aurora\\n    * DALL-E\\n    * Firefly\\n    * Flux\\n    * Ideogram\\n    * Imagen\\n    * Midjourney\\n    * Stable Diffusion\\n  * Text-to-video models\\n    * Dream Machine\\n    * Gen-3 Alpha\\n    * Hailuo AI\\n    * Kling\\n    * Sora\\n    * Veo\\n  * Music generation\\n    * Suno AI\\n    * Udio\\n\\n  \\n---|---  \\nText|\\n\\n  * Word2vec\\n  * Seq2seq\\n  * GloVe\\n  * BERT\\n  * T5\\n  * Llama\\n  * Chinchilla AI\\n  * PaLM\\n  * GPT\\n    * 1\\n    * 2\\n    * 3\\n    * J\\n    * ChatGPT\\n    * 4\\n    * 4o\\n    * 4.5\\n    * o1\\n    * o3\\n  * Claude\\n  * Gemini\\n    * chatbot\\n  * Grok\\n  * LaMDA\\n  * BLOOM\\n  * Project Debater\\n  * IBM Watson\\n  * IBM Watsonx\\n  * Granite\\n  * PanGu-Σ\\n  * DeepSeek\\n  * Qwen\\n\\n  \\nDecisional|\\n\\n  * AlphaGo\\n  * AlphaZero\\n  * OpenAI Five\\n  * Self-driving car\\n  * MuZero\\n  * Action selection\\n    * AutoGPT\\n  * Robot control\\n\\n  \\n  \\nPeople|\\n\\n  * Alan Turing\\n  * Warren Sturgis McCulloch\\n  * Walter Pitts\\n  * John von Neumann\\n  * Claude Shannon\\n  * Marvin Minsky\\n  * John McCarthy\\n  * Nathaniel Rochester\\n  * Allen Newell\\n  * Cliff Shaw\\n  * Herbert A. Simon\\n  * Oliver Selfridge\\n  * Frank Rosenblatt\\n  * Bernard Widrow\\n  * Joseph Weizenbaum\\n  * Seymour Papert\\n  * Seppo Linnainmaa\\n  * Paul Werbos\\n  * Jürgen Schmidhuber\\n  * Yann LeCun\\n  * Geoffrey Hinton\\n  * John Hopfield\\n  * Yoshua Bengio\\n  * Lotfi A. Zadeh\\n  * Stephen Grossberg\\n  * Alex Graves\\n  * Andrew Ng\\n  * Fei-Fei Li\\n  * Alex Krizhevsky\\n  * Ilya Sutskever\\n  * Demis Hassabis\\n  * David Silver\\n  * Ian Goodfellow\\n  * Andrej Karpathy\\n\\n  \\nArchitectures|\\n\\n  * Neural Turing machine\\n  * Differentiable neural computer\\n  * Transformer\\n    * Vision transformer (ViT)\\n  * Recurrent neural network (RNN)\\n  * Long short-term memory (LSTM)\\n  * Gated recurrent unit (GRU)\\n  * Echo state network\\n  * Multilayer perceptron (MLP)\\n  * Convolutional neural network (CNN)\\n  * Residual neural network (RNN)\\n  * Highway network\\n  * Mamba\\n  * Autoencoder\\n  * Variational autoencoder (VAE)\\n  * Generative adversarial network (GAN)\\n  * Graph neural network (GNN)\\n\\n  \\n  \\n  * Portals \\n    * Technology\\n  * Category\\n    * Artificial neural networks\\n    * Machine learning\\n  * List \\n    * Companies\\n    * Projects\\n\\n  \\n  \\nRetrieved from\\n\"https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1279480880\"\\n\\nCategories:\\n\\n  * Large language models\\n  * Deep learning\\n  * Natural language processing\\n\\nHidden categories:\\n\\n  * CS1: long volume value\\n  * Webarchive template wayback links\\n  * CS1 maint: multiple names: authors list\\n  * Articles with short description\\n  * Short description is different from Wikidata\\n  * Articles containing potentially dated statements from 2024\\n  * All articles containing potentially dated statements\\n  * All accuracy disputes\\n  * Articles with disputed statements from September 2024\\n  * All articles with unsourced statements\\n  * Articles with unsourced statements from February 2024\\n\\n  * This page was last edited on 8 March 2025, at 20:38 (UTC).\\n  * Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n  * Privacy policy\\n  * About Wikipedia\\n  * Disclaimers\\n  * Contact Wikipedia\\n  * Code of Conduct\\n  * Developers\\n  * Statistics\\n  * Cookie statement\\n  * Mobile view\\n\\n  *   * \\n\\nSearch\\n\\nSearch\\n\\nToggle the table of contents\\n\\nLarge language model\\n\\n54 languages Add topic\\n\\n  *[v]: View this template\\n  *[t]: Discuss this template\\n  *[e]: Edit this template\\n\\n'}\n",
      "Refining :: https://en.wikipedia.org/wiki/Large_language_model\n",
      "-------------------------------\n",
      "Refining complete\n"
     ]
    }
   ],
   "source": [
    "research_out = researcher_service_instance.research(\"LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': \"{'query': 'What is a Large Language Model (LLM)?', 'goal': 'To understand the foundational concept and definition of Large Language Models.'}\",\n",
       "  'results': [{'url': 'https://aws.amazon.com/what-is/large-language-model/',\n",
       "    'refined_out': \"Large Language Models (LLMs) are extensive deep learning models, pre-trained on vast datasets, designed for understanding and generating human-like text. These models are built upon the transformer neural network architecture, which consists of an encoder and a decoder with self-attention capabilities. This enables them to process entire sequences of text in parallel, unlike past recurrent neural networks (RNNs) that operated sequentially. \\n\\nLLMs are highly flexible and can execute a variety of tasks such as answering questions, summarizing text, translating languages, and completing sentences. They derive this ability from their capacity to consider billions of parameters.\\n\\nThe functionality of LLMs is greatly attributed to their use of word embeddings, which allow them to represent words as multi-dimensional vectors. This way, words with similar meanings or relationships appear close together in the vector space, enhancing the model's understanding of context.\\n\\nTraining LLMs involves feeding them vast amounts of data, enabling them to learn through self-learning techniques to predict and generate text. They can be fine-tuned with additional data to enhance specific applications, allowing for tasks like code generation and text classification across various languages.\\n\\nIn terms of application, LLMs are pivotal in fields like copywriting, knowledgebase answering with natural language processing, sentiment analysis, and even code generation. The future of LLMs holds increased capabilities, potential for multimedia training, and significant impacts on workplace automation and conversational AI.\\n\\nLLMs are increasingly significant in AI development, with platforms like AWS offering tools such as Amazon Bedrock and SageMaker JumpStart to facilitate the building and deployment of these models. These services provide prebuilt models and scalable infrastructure to support generative AI applications.\"},\n",
       "   {'url': 'https://www.ibm.com/think/topics/large-language-models',\n",
       "    'refined_out': 'Large Language Models (LLMs) are a class of foundation models designed to understand and generate natural language and other content types. These models are trained on vast amounts of data, which endows them with the capabilities to perform a wide array of tasks across multiple applications. Rather than building domain-specific models for each use case, LLMs provide a cost-effective and efficient solution due to their ability to handle diverse tasks through a single infrastructure.\\n\\nThe architecture of LLMs is typically based on transformers, a neural network structure that excels at handling sequential data. During training, LLMs learn to predict the next word in a sequence by analyzing preceding words, a process enhanced by an attention mechanism that focuses on specific dataset parts. This training is done using massive text corpora, allowing the models to learn grammar, semantics, and relationships autonomously.\\n\\nLLMs have emerged as significant breakthroughs in natural language processing (NLP) and artificial intelligence (AI), becoming accessible to the public through platforms like OpenAI’s ChatGPT and Google’s BERT models. These models are capable of generating human-like text, translating languages, summarizing content, answering questions, and even assisting in creative writing and code generation.\\n\\nThe use cases for LLMs are plentiful, impacting fields from customer service and content creation to research, language translation, and healthcare. They enhance conversational AI in chatbots and virtual assistants, automate content generation, and support research by summarizing information from large datasets. LLMs also contribute to accessibility by supporting individuals with disabilities and providing accurate language translations.\\n\\nTo ensure their effectiveness and mitigate risks such as bias or misinformation, LLMs are subject to techniques like prompt engineering and reinforcement learning with human feedback. These measures are essential to ensure enterprise-grade models that are reliable and accountable, meeting the governance standards needed for their implementation across various industries.'},\n",
       "   {'url': 'https://en.wikipedia.org/wiki/Large_language_model',\n",
       "    'refined_out': 'A **Large Language Model (LLM)** is a type of machine learning model specifically designed for natural language processing tasks, such as language generation. These models are characterized by having a large number of parameters and are typically trained using self-supervised learning on extensive and diverse text corpora. The goal of LLMs is to acquire predictive power about syntax, semantics, and the underlying ontologies of human language. However, they also tend to inherit inaccuracies and biases present in the training data.\\n\\nThe largest and most capable LLMs are usually based on the transformer architecture, such as the well-known Generative Pretrained Transformers (GPTs). These models can be fine-tuned for specific tasks or modified through techniques like prompt engineering to guide their responses effectively. \\n\\nLLMs utilize various processing techniques such as tokenization, where text is converted into numerical representations, and training involves significant computational resources. The computational capacity required is often measured in terms of operations or \"FLOPs\" (Floating Point Operations), with larger models typically being more resource-intensive to train and deploy.\\n\\nThe field of LLMs has progressed significantly since the early 2010s, with notable milestones including the introduction of transformer architecture by Google researchers in 2017 and the subsequent development of models like BERT and GPT-3. These advancements have led to models that are capable of solving complex tasks through fine-tuning and prompt engineering.\\n\\nLLMs have made substantial impacts across various subfields of computer science, including robotics, software engineering, and even societal impacts. As of 2024, LLMs are transitioning into multimodal territories, meaning they are capable of processing or generating various other types of data, such as images or audio.\\n\\nDespite their capabilities, LLMs have limitations, including high training costs, energy demands, and inherent biases in their outputs. Additionally, achieving further improvements often involves challenges related to scaling laws, emergent abilities, and dealing with hallucinations where the models generate confident but incorrect information. \\n\\nOverall, LLMs represent a rapidly evolving area of artificial intelligence and their applications, architecture, and integration continue to be an active field of research and development.'},\n",
       "   {'url': 'https://www.geeksforgeeks.org/large-language-model-llm/',\n",
       "    'refined_out': 'A Large Language Model (LLM) is a type of powerful artificial intelligence system designed to process and understand human language using neural network techniques. These models contain billions of parameters that enable them to perform complex tasks such as text generation, machine translation, summarization, question-answering, and more. LLMs operate using self-supervised learning techniques, allowing them to learn intricate patterns and relationships in language data.\\n\\nKey characteristics of LLMs include:\\n- **Neural Network Architecture**: LLMs utilize deep learning, specifically transformer-based models, which include components like input embeddings, attention mechanisms, encoder and decoder layers, and output layers. This architecture allows LLMs to weigh the importance of different tokens in a sequence and capture dependencies and relationships in text data.\\n- **Examples**: Notable examples of LLMs include GPT-3 and its successors (such as GPT-4), developed by OpenAI, and BERT by Google.\\n- **Parameter Scale**: GPT models have significantly increased in size, from GPT-1 with 117 million parameters to GPT-3 with 175 billion parameters.\\n- **Applications**: LLMs can be used for applications in natural language understanding, content generation, language translation, sentiment analysis, and more. They are also increasingly used in educational contexts for tasks like summarizing topics and providing learning goals.\\n- **Advantages**: LLMs can perform zero-shot learning, handle vast amounts of data, efficiently generalize to new tasks, automate various language-related tasks, and be fine-tuned for domain-specific applications.\\n- **Challenges**: Training LLMs is costly, time-intensive, and environmentally impactful. The process requires significant computational resources and results in large carbon emissions.\\n\\nThe impact of LLMs on AI and natural language processing is significant, representing advancements in language-based AI applications. However, the challenges associated with their development and deployment, such as cost, environmental concerns, and data accessibility, remain significant.'}]},\n",
       " {'query': \"{'query': 'How do Large Language Models work?', 'goal': 'To explore the underlying mechanics and architecture of LLMs.'}\",\n",
       "  'results': [{'url': 'https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f',\n",
       "    'refined_out': 'Here are the key findings from the provided article on how Large Language Models (LLMs) work, focusing on their underlying mechanics and architecture:\\n\\n1. **Definition and Scope**:\\n   - LLMs, such as ChatGPT, are a type of Artificial Intelligence (AI) that deals specifically with understanding and generating human language.\\n   - They are considered part of the larger AI ecosystem, which includes Machine Learning (ML) and Deep Learning.\\n\\n2. **Machine Learning Context**:\\n   - In ML, the goal is to discover patterns in data, often through classification tasks (e.g., predicting music genres based on song attributes).\\n   - More complex tasks require more complex models, like those provided by neural networks.\\n\\n3. **Deep Learning and Neural Networks**:\\n   - Neural Networks are powerful ML models that can handle complex, non-linear relationships between inputs and outputs. They consist of many layers of \"neurons.\"\\n   - Deep Learning, which uses neural networks, is essential for processing unstructured data like text and images.\\n\\n4. **Large Language Models (LLMs)**:\\n   - The \"large\" in LLMs refers to the number of parameters in the neural network, often exceeding one billion neurons.\\n   - Language modeling involves predicting the next word in a sequence of words, which is a classification task with as many classes as words in the language (e.g., 50,000 classes).\\n\\n5. **Training Process**:\\n   - LLMs are trained using vast amounts of text data from the internet, books, and other sources through a process called self-supervised learning, where the model learns to predict the next word in a sequence.\\n\\n6. **Generative Capabilities**:\\n   - By predicting one word at a time, LLMs can generate coherent and contextually relevant text sequences, making them an example of Generative AI.\\n\\n7. **Architecture of LLMs**:\\n   - The architecture used is often a Transformer, which focuses attention on the most relevant parts of the input sequence at any given time.\\n\\n8. **Training Phases**:\\n   - Training is commonly conducted in phases: Pre-Training (learning language structures and acquiring knowledge), Instruction Fine-Tuning (aligning with human instructions), and optionally Reinforcement Learning from Human Feedback (RLHF) to improve alignment with human values.\\n\\n9. **Applications and Limitations**:\\n   - LLMs can perform tasks like summarization and answering knowledge questions by leveraging massive pre-trained knowledge.\\n   - They face challenges like \"hallucination,\" where they generate incorrect information due to a lack of explicit grounding in real-world facts.\\n\\n10. **Emerging Abilities**:\\n    - LLMs can solve new tasks in a \"zero-shot\" manner without explicit training on those tasks, reflecting emerging abilities.\\n    - They can benefit from examples and step-by-step reasoning, akin to a working memory.\\n\\n11. **Future Considerations**:\\n    - There is ongoing debate about whether LLMs acquire a compressed understanding of the world or simply mimic patterns seen during training.\\n    - LLMs demonstrate significant utility and knowledge, but their resemblance to human intelligence is still being studied.\\n\\nThese insights highlight the complexity and power of LLMs, their foundational role in language processing, and their potential and challenges in AI applications.'},\n",
       "   {'url': 'https://aws.amazon.com/what-is/large-language-model/',\n",
       "    'refined_out': 'The refined search results from the provided data on \"How do Large Language Models work?\" including their mechanics and architecture are:\\n\\n1. **Large Language Models (LLMs):** \\n   - LLMs are essentially large deep learning models, pre-trained on vast amounts of data.\\n   - They are based on the transformer model, which is a type of neural network architecture consisting of an encoder and a decoder with self-attention capabilities.\\n   - Transformers process entire sequences in parallel, enabling efficient use of GPUs during training, which significantly reduces training time.\\n   - LLMs are characterized by their large size, often having hundreds of billions of parameters, enabling them to ingest and process massive datasets.\\n\\n2. **Working Mechanism of LLMs:**\\n   - Word embeddings are used instead of simple numerical tables to represent words, allowing recognition of relationships between words.\\n   - These embeddings allow transformers to process text as numerical representations, understand word context, and relationships within vector space.\\n   - The decoder then applies this understanding of language to produce outputs based on input data.\\n\\n3. **Training of LLMs:**\\n   - LLMs use large corpuses of high-quality data and adjust model parameters iteratively during training to predict the next token in a sequence of inputs.\\n   - Self-learning techniques enable models to learn and adjust parameters, maximizing the likelihood of accurate predictions.\\n   - LLMs can adapt to various tasks with small sets of supervised data through fine-tuning.\\n\\n4. **Applications of LLMs:**\\n   - LLMs can perform diverse tasks such as text generation, answering questions, language translation, content summarization, and code generation from natural language prompts.\\n   - They support various applications, including customer service chatbots, document search, knowledgebase answering, and more.\\n\\n5. **Future and Potential of LLMs:**\\n   - Increased capabilities and improvements in accuracy and performance are expected.\\n   - They are likely to be used in broader applications, including audiovisual training inputs and enhancing conversational AI.\\n   - LLMs could potentially disrupt various sectors by automating repetitive tasks traditionally done by humans.\\n\\n6. **Amazon Web Services (AWS) Role:**\\n   - AWS provides tools such as Amazon Bedrock and SageMaker JumpStart to build, deploy, and scale generative AI applications with LLMs efficiently.\\n   - These services offer pre-trained models and infrastructure to customize and deploy for specific use cases. \\n\\nThese key points summarize the mechanics, architecture, significance, and application of LLMs as extracted from the refined search data.'},\n",
       "   {'url': 'https://www.elastic.co/what-is/large-language-models',\n",
       "    'refined_out': \"To understand the mechanics and architecture of Large Language Models (LLMs), the following key information can be extracted from the search results:\\n\\n1. **Definition**: Large Language Models are deep learning algorithms that perform various natural language processing (NLP) tasks. They are built using transformer models and trained on massive datasets. LLMs function like neural networks with layered nodes and have large numbers of parameters akin to memories that the model collects during training.\\n\\n2. **Transformer Model**: This is a common architecture for LLMs. It consists of an encoder and a decoder and processes data by tokenizing input and using self-attention mechanisms to discover relationships between tokens. This mechanism allows the model to generate predictions considering different parts of the input text or context.\\n\\n3. **Key Components**:\\n   - **Embedding Layer**: Captures semantic and syntactic meaning from input text.\\n   - **Feedforward Layer**: Transforms input embeddings to understand user intent.\\n   - **Recurrent Layer**: Interprets sequence and relationship of words.\\n   - **Attention Mechanism**: Focuses on relevant parts of the input text to produce accurate outputs.\\n\\n4. **Types of LLMs**:\\n   - **Generic or Raw Language Models**: Predict the next word based on training data.\\n   - **Instruction-Tuned Language Models**: Trained to predict responses to given instructions.\\n   - **Dialog-Tuned Language Models**: Trained for dialog, predicting conversational responses.\\n\\n5. **Training and Fine-tuning**:\\n   - **Training**: Involves pre-training on large textual datasets using unsupervised learning to learn word meanings and contexts.\\n   - **Fine-tuning**: Optimizes the model for specific tasks like translation or sentiment analysis, often using few-shot or zero-shot prompting.\\n\\n6. **Applications and Use Cases**:\\n   - Information retrieval, sentiment analysis, text and code generation, chatbots, and many others across fields like healthcare, customer service, marketing, legal, and finance.\\n\\n7. **Benefits**: They offer a wide range of applications, continual performance improvement due to in-context learning, and quick learning capabilities.\\n\\n8. **Challenges and Limitations**:\\n   - **Hallucinations**: Occur when outputs are incorrect or misleading as the models only predict the next word.\\n   - **Security Risks**: Concerns over privacy, potential misuse, and data leakage.\\n   - **Bias and Consent**: Outputs depend on training data diversity, and issues with data consent and intellectual property.\\n   - **Scaling and Deployment**: Require significant resources and technical expertise.\\n\\n9. **Popular LLMs**:\\n   - Examples include Google's PaLM, BERT, XLNet, and OpenAI's GPT models.\\n\\n10. **Difference from Generative AI**: While generative AI refers broadly to AI models that generate content, LLMs specifically deal with textual outputs and are a type of generative AI.\\n\\nUnderstanding these elements gives insight into how LLMs operate, their underlying architecture, benefits, and the challenges they face in various applications.\"},\n",
       "   {'url': 'https://en.wikipedia.org/wiki/Large_language_model',\n",
       "    'refined_out': \"Refined content regarding the mechanics and architecture of large language models (LLMs):\\n\\n1. **Overview**:\\n   - LLMs are a type of machine learning model designed for natural language processing tasks such as language generation. They are trained using self-supervised learning on vast amounts of text, involving many parameters.\\n   - The largest and most capable LLMs today are often generative pre-trained transformers (GPTs). These models can be fine-tuned for specific tasks through techniques such as prompt engineering.\\n   - LLMs can understand language syntax, semantics, and ontologies, but also inherit any inaccuracies and biases present in their training data.\\n\\n2. **History and Development**:\\n   - The early days of language models involved statistical approaches, with neural networks starting to dominate the scene around 2012.\\n   - The transformer architecture, introduced by Google in 2017, changed NLP by leveraging attention mechanisms, leading to models such as BERT (2018) and later advances like GPT-3 (2020) and ChatGPT (2022).\\n   - Modern LLMs like GPT-4 have multimodal capabilities (handling text, images, etc.) and were crucial in popularizing AI across various domains.\\n\\n3. **Dataset Preprocessing**:\\n   - **Tokenization**: Converts text to numbers for processing. Common methods include byte-pair encoding (BPE), which compresses datasets and resolves suboptimal token usage for non-English languages.\\n   - **Dataset Cleaning**: Removal of low-quality or toxic data to improve model training efficiency. Increasing web content generated by AI poses future challenges in dataset quality.\\n\\n4. **Training and Architecture**:\\n   - **Reinforcement Learning from Human Feedback (RLHF)**: Fine-tunes models with human preference data.\\n   - **Mixture of Experts (MoE)**: Allows for training of models with up to 1 trillion parameters by using different models for different data segments.\\n   - **Prompt Engineering**: Optimizes model responses without costly re-training by subtly guiding the model with prompt formulations.\\n\\n5. **Model Properties**:\\n   - **Scaling Laws**: The efficacy of LLMs is tied to factors such as model size, compute cost, and dataset size, with performance improvements often marked by emergent abilities.\\n   - **Emergent Abilities**: As models scale, unforeseen capabilities emerge, enabling them to learn tasks with minimal supervision.\\n\\n6. **Interpretation and Understanding**:\\n   - There is ongoing debate about whether LLMs truly 'understand' language versus merely mimicking it. Some argue they exhibit a form of intelligence, while others caution against over-attributing human-like qualities.\\n\\n7. **Use Cases and Limitations**:\\n   - While capable of high-level reasoning and task completion, LLMs are not autonomous agents and require structured environments and potentially external tools for complex, dynamic tasks.\\n   - They face challenges in hallucination, where generated outputs are syntactically correct but factually incorrect or nonsensical.\\n\\n8. **Infrastructure and Costs**:\\n   - Training and deploying LLMs require significant computational resources. The cost often scales with model size, impacting accessibility and prompting exploration of cost-efficient alternatives like quantization.\\n\\nThese points cover the essential underpinning aspects of LLMs, providing insights into how they function, their architecture, and the factors influencing their capabilities and challenges.\"}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"{'query': 'What is a Large Language Model (LLM)?', 'goal': 'To understand the foundational concept and definition of Large Language Models.'}\", 'results': [{'url': 'https://aws.amazon.com/what-is/large-language-model/', 'refined_out': \"Large Language Models (LLMs) are extensive deep learning models, pre-trained on vast datasets, designed for understanding and generating human-like text. These models are built upon the transformer neural network architecture, which consists of an encoder and a decoder with self-attention capabilities. This enables them to process entire sequences of text in parallel, unlike past recurrent neural networks (RNNs) that operated sequentially. \\n\\nLLMs are highly flexible and can execute a variety of tasks such as answering questions, summarizing text, translating languages, and completing sentences. They derive this ability from their capacity to consider billions of parameters.\\n\\nThe functionality of LLMs is greatly attributed to their use of word embeddings, which allow them to represent words as multi-dimensional vectors. This way, words with similar meanings or relationships appear close together in the vector space, enhancing the model's understanding of context.\\n\\nTraining LLMs involves feeding them vast amounts of data, enabling them to learn through self-learning techniques to predict and generate text. They can be fine-tuned with additional data to enhance specific applications, allowing for tasks like code generation and text classification across various languages.\\n\\nIn terms of application, LLMs are pivotal in fields like copywriting, knowledgebase answering with natural language processing, sentiment analysis, and even code generation. The future of LLMs holds increased capabilities, potential for multimedia training, and significant impacts on workplace automation and conversational AI.\\n\\nLLMs are increasingly significant in AI development, with platforms like AWS offering tools such as Amazon Bedrock and SageMaker JumpStart to facilitate the building and deployment of these models. These services provide prebuilt models and scalable infrastructure to support generative AI applications.\"}, {'url': 'https://www.ibm.com/think/topics/large-language-models', 'refined_out': 'Large Language Models (LLMs) are a class of foundation models designed to understand and generate natural language and other content types. These models are trained on vast amounts of data, which endows them with the capabilities to perform a wide array of tasks across multiple applications. Rather than building domain-specific models for each use case, LLMs provide a cost-effective and efficient solution due to their ability to handle diverse tasks through a single infrastructure.\\n\\nThe architecture of LLMs is typically based on transformers, a neural network structure that excels at handling sequential data. During training, LLMs learn to predict the next word in a sequence by analyzing preceding words, a process enhanced by an attention mechanism that focuses on specific dataset parts. This training is done using massive text corpora, allowing the models to learn grammar, semantics, and relationships autonomously.\\n\\nLLMs have emerged as significant breakthroughs in natural language processing (NLP) and artificial intelligence (AI), becoming accessible to the public through platforms like OpenAI’s ChatGPT and Google’s BERT models. These models are capable of generating human-like text, translating languages, summarizing content, answering questions, and even assisting in creative writing and code generation.\\n\\nThe use cases for LLMs are plentiful, impacting fields from customer service and content creation to research, language translation, and healthcare. They enhance conversational AI in chatbots and virtual assistants, automate content generation, and support research by summarizing information from large datasets. LLMs also contribute to accessibility by supporting individuals with disabilities and providing accurate language translations.\\n\\nTo ensure their effectiveness and mitigate risks such as bias or misinformation, LLMs are subject to techniques like prompt engineering and reinforcement learning with human feedback. These measures are essential to ensure enterprise-grade models that are reliable and accountable, meeting the governance standards needed for their implementation across various industries.'}, {'url': 'https://en.wikipedia.org/wiki/Large_language_model', 'refined_out': 'A **Large Language Model (LLM)** is a type of machine learning model specifically designed for natural language processing tasks, such as language generation. These models are characterized by having a large number of parameters and are typically trained using self-supervised learning on extensive and diverse text corpora. The goal of LLMs is to acquire predictive power about syntax, semantics, and the underlying ontologies of human language. However, they also tend to inherit inaccuracies and biases present in the training data.\\n\\nThe largest and most capable LLMs are usually based on the transformer architecture, such as the well-known Generative Pretrained Transformers (GPTs). These models can be fine-tuned for specific tasks or modified through techniques like prompt engineering to guide their responses effectively. \\n\\nLLMs utilize various processing techniques such as tokenization, where text is converted into numerical representations, and training involves significant computational resources. The computational capacity required is often measured in terms of operations or \"FLOPs\" (Floating Point Operations), with larger models typically being more resource-intensive to train and deploy.\\n\\nThe field of LLMs has progressed significantly since the early 2010s, with notable milestones including the introduction of transformer architecture by Google researchers in 2017 and the subsequent development of models like BERT and GPT-3. These advancements have led to models that are capable of solving complex tasks through fine-tuning and prompt engineering.\\n\\nLLMs have made substantial impacts across various subfields of computer science, including robotics, software engineering, and even societal impacts. As of 2024, LLMs are transitioning into multimodal territories, meaning they are capable of processing or generating various other types of data, such as images or audio.\\n\\nDespite their capabilities, LLMs have limitations, including high training costs, energy demands, and inherent biases in their outputs. Additionally, achieving further improvements often involves challenges related to scaling laws, emergent abilities, and dealing with hallucinations where the models generate confident but incorrect information. \\n\\nOverall, LLMs represent a rapidly evolving area of artificial intelligence and their applications, architecture, and integration continue to be an active field of research and development.'}, {'url': 'https://www.geeksforgeeks.org/large-language-model-llm/', 'refined_out': 'A Large Language Model (LLM) is a type of powerful artificial intelligence system designed to process and understand human language using neural network techniques. These models contain billions of parameters that enable them to perform complex tasks such as text generation, machine translation, summarization, question-answering, and more. LLMs operate using self-supervised learning techniques, allowing them to learn intricate patterns and relationships in language data.\\n\\nKey characteristics of LLMs include:\\n- **Neural Network Architecture**: LLMs utilize deep learning, specifically transformer-based models, which include components like input embeddings, attention mechanisms, encoder and decoder layers, and output layers. This architecture allows LLMs to weigh the importance of different tokens in a sequence and capture dependencies and relationships in text data.\\n- **Examples**: Notable examples of LLMs include GPT-3 and its successors (such as GPT-4), developed by OpenAI, and BERT by Google.\\n- **Parameter Scale**: GPT models have significantly increased in size, from GPT-1 with 117 million parameters to GPT-3 with 175 billion parameters.\\n- **Applications**: LLMs can be used for applications in natural language understanding, content generation, language translation, sentiment analysis, and more. They are also increasingly used in educational contexts for tasks like summarizing topics and providing learning goals.\\n- **Advantages**: LLMs can perform zero-shot learning, handle vast amounts of data, efficiently generalize to new tasks, automate various language-related tasks, and be fine-tuned for domain-specific applications.\\n- **Challenges**: Training LLMs is costly, time-intensive, and environmentally impactful. The process requires significant computational resources and results in large carbon emissions.\\n\\nThe impact of LLMs on AI and natural language processing is significant, representing advancements in language-based AI applications. However, the challenges associated with their development and deployment, such as cost, environmental concerns, and data accessibility, remain significant.'}]}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B9BCVyHjuDJTnTz9Lmzv9xsUwh2yx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_P5Vm8sFCukfgR6uZ0ih13zs3', function=Function(arguments='{\"lessons\":[{\"title\":\"Introduction to Large Language Models (LLMs)\",\"learning_objectives\":[\"Define and understand what Large Language Models (LLMs) are.\",\"Identify the foundational architecture behind LLMs.\"],\"content\":\"Large Language Models (LLMs) are extensive deep learning models designed specifically for understanding and generating human-like text. These models excel at natural language processing tasks and possess billions of parameters that enable them to perform complex tasks like text generation, summarization, and translation. \\\\n\\\\nLLMs are built upon the transformer neural network architecture, which includes an encoder and a decoder with self-attention mechanisms. This architecture allows LLMs to process entire sequences of text in parallel, unlike past recurrent neural networks (RNNs) that operated sequentially. This new approach facilitates enhanced understanding and generation of text by focusing on relevant parts of input sequences.\"},{\"title\":\"Capabilities and Applications of LLMs\",\"learning_objectives\":[\"Identify various tasks and applications LLMs can perform.\",\"Understand the flexibility and adaptability of LLMs across multiple domains.\"],\"content\":\"LLMs are known for their high flexibility, which enables them to execute a variety of tasks such as answering questions, summarizing text, translating languages, and completing sentences accurately. They achieve this through their capacity to manage billions of parameters during processing.\\\\n\\\\nThe applicability of LLMs spans numerous fields such as copywriting, sentiment analysis, conversational AI, and even software development through code generation. Their capabilities extend to handling complex language-based tasks without the need for building separate models for each task, making them a cost-effective solution across various industries.\\\\n\\\\nExamples of popular LLMs include OpenAI’s GPT models and Google’s BERT, which have become significant in fields like customer service, content creation, and accessibility through language translation support.\"},{\"title\":\"Training and Fine-tuning of LLMs\",\"learning_objectives\":[\"Describe how LLMs are trained and fine-tuned.\",\"Understand the significance of self-supervised learning and word embeddings in LLMs.\"],\"content\":\"Training LLMs involves feeding them with extensive and diverse text corpora allowing them to learn through self-supervised techniques. The process involves predicting the next word in the sequence based on the analysis of preceding words, enhanced by an attention mechanism.\\\\n\\\\nKey to LLMs\\' functioning is the use of word embeddings, allowing the models to represent words as multi-dimensional vectors, thereby understanding context and relationships better by positioning similar words close together in vector space.\\\\n\\\\nLLMs can be fine-tuned for specific tasks utilizing additional data. This allows adjustment of model parameters to suit particular applications or enhance certain performance aspects, such as code generation or text classification.\"},{\"title\":\"Challenges and Future of LLMs\",\"learning_objectives\":[\"Discuss the challenges and limitations of LLMs.\",\"Evaluate future trends and developments in the LLM field.\"],\"content\":\"Despite their powerful capabilities, LLMs present several challenges. Training them is resource-intensive and costly, involving substantial computational power and significant energy usage, leading to environmental concerns due to large carbon emissions.\\\\n\\\\nLLMs also face inherent biases and limitations such as potential for generating incorrect information or hallucinations, as well as maintaining ethical considerations in their applications.\\\\n\\\\nLooking forward, LLMs are transitioning into multimodal models capable of processing other types of data like images or audio, which expands their role significantly in AI development. Platforms like AWS provide services such as Amazon Bedrock and SageMaker JumpStart, facilitating the building and deployment of these advanced models.\"}]}', name='generate_lesson_plan'), type='function')]), content_filter_results={})], created=1741527707, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_b705f0c291', usage=CompletionUsage(completion_tokens=716, prompt_tokens=2143, total_tokens=2859, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "{'query': \"{'query': 'How do Large Language Models work?', 'goal': 'To explore the underlying mechanics and architecture of LLMs.'}\", 'results': [{'url': 'https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f', 'refined_out': 'Here are the key findings from the provided article on how Large Language Models (LLMs) work, focusing on their underlying mechanics and architecture:\\n\\n1. **Definition and Scope**:\\n   - LLMs, such as ChatGPT, are a type of Artificial Intelligence (AI) that deals specifically with understanding and generating human language.\\n   - They are considered part of the larger AI ecosystem, which includes Machine Learning (ML) and Deep Learning.\\n\\n2. **Machine Learning Context**:\\n   - In ML, the goal is to discover patterns in data, often through classification tasks (e.g., predicting music genres based on song attributes).\\n   - More complex tasks require more complex models, like those provided by neural networks.\\n\\n3. **Deep Learning and Neural Networks**:\\n   - Neural Networks are powerful ML models that can handle complex, non-linear relationships between inputs and outputs. They consist of many layers of \"neurons.\"\\n   - Deep Learning, which uses neural networks, is essential for processing unstructured data like text and images.\\n\\n4. **Large Language Models (LLMs)**:\\n   - The \"large\" in LLMs refers to the number of parameters in the neural network, often exceeding one billion neurons.\\n   - Language modeling involves predicting the next word in a sequence of words, which is a classification task with as many classes as words in the language (e.g., 50,000 classes).\\n\\n5. **Training Process**:\\n   - LLMs are trained using vast amounts of text data from the internet, books, and other sources through a process called self-supervised learning, where the model learns to predict the next word in a sequence.\\n\\n6. **Generative Capabilities**:\\n   - By predicting one word at a time, LLMs can generate coherent and contextually relevant text sequences, making them an example of Generative AI.\\n\\n7. **Architecture of LLMs**:\\n   - The architecture used is often a Transformer, which focuses attention on the most relevant parts of the input sequence at any given time.\\n\\n8. **Training Phases**:\\n   - Training is commonly conducted in phases: Pre-Training (learning language structures and acquiring knowledge), Instruction Fine-Tuning (aligning with human instructions), and optionally Reinforcement Learning from Human Feedback (RLHF) to improve alignment with human values.\\n\\n9. **Applications and Limitations**:\\n   - LLMs can perform tasks like summarization and answering knowledge questions by leveraging massive pre-trained knowledge.\\n   - They face challenges like \"hallucination,\" where they generate incorrect information due to a lack of explicit grounding in real-world facts.\\n\\n10. **Emerging Abilities**:\\n    - LLMs can solve new tasks in a \"zero-shot\" manner without explicit training on those tasks, reflecting emerging abilities.\\n    - They can benefit from examples and step-by-step reasoning, akin to a working memory.\\n\\n11. **Future Considerations**:\\n    - There is ongoing debate about whether LLMs acquire a compressed understanding of the world or simply mimic patterns seen during training.\\n    - LLMs demonstrate significant utility and knowledge, but their resemblance to human intelligence is still being studied.\\n\\nThese insights highlight the complexity and power of LLMs, their foundational role in language processing, and their potential and challenges in AI applications.'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'refined_out': 'The refined search results from the provided data on \"How do Large Language Models work?\" including their mechanics and architecture are:\\n\\n1. **Large Language Models (LLMs):** \\n   - LLMs are essentially large deep learning models, pre-trained on vast amounts of data.\\n   - They are based on the transformer model, which is a type of neural network architecture consisting of an encoder and a decoder with self-attention capabilities.\\n   - Transformers process entire sequences in parallel, enabling efficient use of GPUs during training, which significantly reduces training time.\\n   - LLMs are characterized by their large size, often having hundreds of billions of parameters, enabling them to ingest and process massive datasets.\\n\\n2. **Working Mechanism of LLMs:**\\n   - Word embeddings are used instead of simple numerical tables to represent words, allowing recognition of relationships between words.\\n   - These embeddings allow transformers to process text as numerical representations, understand word context, and relationships within vector space.\\n   - The decoder then applies this understanding of language to produce outputs based on input data.\\n\\n3. **Training of LLMs:**\\n   - LLMs use large corpuses of high-quality data and adjust model parameters iteratively during training to predict the next token in a sequence of inputs.\\n   - Self-learning techniques enable models to learn and adjust parameters, maximizing the likelihood of accurate predictions.\\n   - LLMs can adapt to various tasks with small sets of supervised data through fine-tuning.\\n\\n4. **Applications of LLMs:**\\n   - LLMs can perform diverse tasks such as text generation, answering questions, language translation, content summarization, and code generation from natural language prompts.\\n   - They support various applications, including customer service chatbots, document search, knowledgebase answering, and more.\\n\\n5. **Future and Potential of LLMs:**\\n   - Increased capabilities and improvements in accuracy and performance are expected.\\n   - They are likely to be used in broader applications, including audiovisual training inputs and enhancing conversational AI.\\n   - LLMs could potentially disrupt various sectors by automating repetitive tasks traditionally done by humans.\\n\\n6. **Amazon Web Services (AWS) Role:**\\n   - AWS provides tools such as Amazon Bedrock and SageMaker JumpStart to build, deploy, and scale generative AI applications with LLMs efficiently.\\n   - These services offer pre-trained models and infrastructure to customize and deploy for specific use cases. \\n\\nThese key points summarize the mechanics, architecture, significance, and application of LLMs as extracted from the refined search data.'}, {'url': 'https://www.elastic.co/what-is/large-language-models', 'refined_out': \"To understand the mechanics and architecture of Large Language Models (LLMs), the following key information can be extracted from the search results:\\n\\n1. **Definition**: Large Language Models are deep learning algorithms that perform various natural language processing (NLP) tasks. They are built using transformer models and trained on massive datasets. LLMs function like neural networks with layered nodes and have large numbers of parameters akin to memories that the model collects during training.\\n\\n2. **Transformer Model**: This is a common architecture for LLMs. It consists of an encoder and a decoder and processes data by tokenizing input and using self-attention mechanisms to discover relationships between tokens. This mechanism allows the model to generate predictions considering different parts of the input text or context.\\n\\n3. **Key Components**:\\n   - **Embedding Layer**: Captures semantic and syntactic meaning from input text.\\n   - **Feedforward Layer**: Transforms input embeddings to understand user intent.\\n   - **Recurrent Layer**: Interprets sequence and relationship of words.\\n   - **Attention Mechanism**: Focuses on relevant parts of the input text to produce accurate outputs.\\n\\n4. **Types of LLMs**:\\n   - **Generic or Raw Language Models**: Predict the next word based on training data.\\n   - **Instruction-Tuned Language Models**: Trained to predict responses to given instructions.\\n   - **Dialog-Tuned Language Models**: Trained for dialog, predicting conversational responses.\\n\\n5. **Training and Fine-tuning**:\\n   - **Training**: Involves pre-training on large textual datasets using unsupervised learning to learn word meanings and contexts.\\n   - **Fine-tuning**: Optimizes the model for specific tasks like translation or sentiment analysis, often using few-shot or zero-shot prompting.\\n\\n6. **Applications and Use Cases**:\\n   - Information retrieval, sentiment analysis, text and code generation, chatbots, and many others across fields like healthcare, customer service, marketing, legal, and finance.\\n\\n7. **Benefits**: They offer a wide range of applications, continual performance improvement due to in-context learning, and quick learning capabilities.\\n\\n8. **Challenges and Limitations**:\\n   - **Hallucinations**: Occur when outputs are incorrect or misleading as the models only predict the next word.\\n   - **Security Risks**: Concerns over privacy, potential misuse, and data leakage.\\n   - **Bias and Consent**: Outputs depend on training data diversity, and issues with data consent and intellectual property.\\n   - **Scaling and Deployment**: Require significant resources and technical expertise.\\n\\n9. **Popular LLMs**:\\n   - Examples include Google's PaLM, BERT, XLNet, and OpenAI's GPT models.\\n\\n10. **Difference from Generative AI**: While generative AI refers broadly to AI models that generate content, LLMs specifically deal with textual outputs and are a type of generative AI.\\n\\nUnderstanding these elements gives insight into how LLMs operate, their underlying architecture, benefits, and the challenges they face in various applications.\"}, {'url': 'https://en.wikipedia.org/wiki/Large_language_model', 'refined_out': \"Refined content regarding the mechanics and architecture of large language models (LLMs):\\n\\n1. **Overview**:\\n   - LLMs are a type of machine learning model designed for natural language processing tasks such as language generation. They are trained using self-supervised learning on vast amounts of text, involving many parameters.\\n   - The largest and most capable LLMs today are often generative pre-trained transformers (GPTs). These models can be fine-tuned for specific tasks through techniques such as prompt engineering.\\n   - LLMs can understand language syntax, semantics, and ontologies, but also inherit any inaccuracies and biases present in their training data.\\n\\n2. **History and Development**:\\n   - The early days of language models involved statistical approaches, with neural networks starting to dominate the scene around 2012.\\n   - The transformer architecture, introduced by Google in 2017, changed NLP by leveraging attention mechanisms, leading to models such as BERT (2018) and later advances like GPT-3 (2020) and ChatGPT (2022).\\n   - Modern LLMs like GPT-4 have multimodal capabilities (handling text, images, etc.) and were crucial in popularizing AI across various domains.\\n\\n3. **Dataset Preprocessing**:\\n   - **Tokenization**: Converts text to numbers for processing. Common methods include byte-pair encoding (BPE), which compresses datasets and resolves suboptimal token usage for non-English languages.\\n   - **Dataset Cleaning**: Removal of low-quality or toxic data to improve model training efficiency. Increasing web content generated by AI poses future challenges in dataset quality.\\n\\n4. **Training and Architecture**:\\n   - **Reinforcement Learning from Human Feedback (RLHF)**: Fine-tunes models with human preference data.\\n   - **Mixture of Experts (MoE)**: Allows for training of models with up to 1 trillion parameters by using different models for different data segments.\\n   - **Prompt Engineering**: Optimizes model responses without costly re-training by subtly guiding the model with prompt formulations.\\n\\n5. **Model Properties**:\\n   - **Scaling Laws**: The efficacy of LLMs is tied to factors such as model size, compute cost, and dataset size, with performance improvements often marked by emergent abilities.\\n   - **Emergent Abilities**: As models scale, unforeseen capabilities emerge, enabling them to learn tasks with minimal supervision.\\n\\n6. **Interpretation and Understanding**:\\n   - There is ongoing debate about whether LLMs truly 'understand' language versus merely mimicking it. Some argue they exhibit a form of intelligence, while others caution against over-attributing human-like qualities.\\n\\n7. **Use Cases and Limitations**:\\n   - While capable of high-level reasoning and task completion, LLMs are not autonomous agents and require structured environments and potentially external tools for complex, dynamic tasks.\\n   - They face challenges in hallucination, where generated outputs are syntactically correct but factually incorrect or nonsensical.\\n\\n8. **Infrastructure and Costs**:\\n   - Training and deploying LLMs require significant computational resources. The cost often scales with model size, impacting accessibility and prompting exploration of cost-efficient alternatives like quantization.\\n\\nThese points cover the essential underpinning aspects of LLMs, providing insights into how they function, their architecture, and the factors influencing their capabilities and challenges.\"}]}\n",
      "<class 'dict'>\n",
      "ChatCompletion(id='chatcmpl-B9BCdayT5l08jfCIig0Iud7DVu8TN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_oey6XhpTekJ0v56GAagsqA98', function=Function(arguments='{\"lessons\":[{\"title\":\"Introduction to LLM Mechanics\",\"learning_objectives\":[\"Understand the fundamental definition and scope of Large Language Models.\",\"Identify the role of Machine Learning and Deep Learning in the context of LLMs.\"],\"content\":\"Large Language Models (LLMs) are a type of Artificial Intelligence (AI) specialized in understanding and generating human language. They are integral to a broader AI ecosystem which also encompasses Machine Learning (ML) and Deep Learning. In the landscape of ML, discovering patterns in data and performing classification tasks, like predicting music genres from song attributes, is common. However, more complex models, such as neural networks, are required for handling tasks with non-linear relationships. Here, neural networks play a vital role in ML as they consist of multiple layers of \\'neurons\\' and can process unstructured data like text and images. Deep Learning, utilizing these neural networks, becomes crucial to processing such data effectively. LLMs are an extension of this concept, acting on a vast scale with neural networks that often exceed one billion parameters, facilitating tasks like language modeling by predicting the next word in a sequence.\"},{\"title\":\"Deep Dive into LLM Architecture\",\"learning_objectives\":[\"Explore the structure and function of the transformer model in LLMs.\",\"Understand word embeddings and the role of the self-attention mechanism.\"],\"content\":\"LLMs, exemplified by models like ChatGPT, rely heavily on a specific architecture called the Transformer. This architecture comprises an encoder and a decoder with self-attention capabilities, allowing the model to focus on the most pertinent parts of the input sequence during processing. Transformers elevate efficiency by processing entire input sequences in parallel, thus leveraging GPUs effectively to reduce training duration. A conceptual core of LLMs lies in word embeddings, which replace simple numerical representations with mapped relationships in a vector space. This approach enables transformers to comprehend word context and relationships efficiently. Utilizing self-attention, the model discerns the importance of different elements within the input. By applying this understanding during the decoding process, LLMs generate informed outputs based on the input data, marking a milestone in AI\\'s ability to understand and produce coherent language.\"},{\"title\":\"Training and Generative Capabilities of LLMs\",\"learning_objectives\":[\"Describe the training process and phases involved in LLM development.\",\"Identify the generative capabilities and limitations of LLMs.\"],\"content\":\"The training process for LLMs involves vast text datasets from multiple sources, employing a technique known as self-supervised learning. This approach teaches the model to predict subsequent words in a sequence, gradually refining model parameters to improve accuracy. The training is executed in stages: Pre-Training allows the model to learn language structure and content-dependent knowledge. Instruction Fine-Tuning aligns the model with human guidance, supplemented optionally by Reinforcement Learning from Human Feedback (RLHF) to enhance alignment with human values.\\\\n\\\\nLLMs are generative by nature, producing coherent text by predicting one word at a time. This ability marks them as notable examples of Generative AI. However, they are not without flaws; challenges like hallucination—where the model generates inaccurate information—remain due to their reliance on pattern prediction without explicit fact grounding.\"},{\"title\":\"Applications and Future of LLMs\",\"learning_objectives\":[\"Explore current applications and emerging abilities in LLMs.\",\"Understand the potential future advancements and challenges.\"],\"content\":\"LLMs serve diverse applications, from summarizing content and answering questions to language translation and text generation. Their ability to adapt to untrained tasks in a zero-shot manner reflects emergent capabilities as models scale. These models benefit from examples and sequential reasoning, akin to working memory, demonstrating adaptability.\\\\n\\\\nLooking ahead, debates continue over whether LLMs develop a compressed world understanding or merely mimic observed patterns. Despite this debate, they hold significant potential, especially in automating repetitive tasks traditionally human-performed and extending into broader application fields. Nonetheless, challenges like hallucination, privacy concerns, and data bias persist, urging careful consideration in deploying LLMs responsibly.\"}]}', name='generate_lesson_plan'), type='function')]), content_filter_results={})], created=1741527715, model='gpt-4o-2024-08-06', object='chat.completion', service_tier=None, system_fingerprint='fp_b705f0c291', usage=CompletionUsage(completion_tokens=813, prompt_tokens=3212, total_tokens=4025, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "lesson_plan_out = researcher_service_instance.generate_lesson_plan(research_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lessons': [{'title': 'Introduction to Large Language Models (LLMs)',\n",
       "    'learning_objectives': ['Define and understand what Large Language Models (LLMs) are.',\n",
       "     'Identify the foundational architecture behind LLMs.'],\n",
       "    'content': 'Large Language Models (LLMs) are extensive deep learning models designed specifically for understanding and generating human-like text. These models excel at natural language processing tasks and possess billions of parameters that enable them to perform complex tasks like text generation, summarization, and translation. \\n\\nLLMs are built upon the transformer neural network architecture, which includes an encoder and a decoder with self-attention mechanisms. This architecture allows LLMs to process entire sequences of text in parallel, unlike past recurrent neural networks (RNNs) that operated sequentially. This new approach facilitates enhanced understanding and generation of text by focusing on relevant parts of input sequences.'},\n",
       "   {'title': 'Capabilities and Applications of LLMs',\n",
       "    'learning_objectives': ['Identify various tasks and applications LLMs can perform.',\n",
       "     'Understand the flexibility and adaptability of LLMs across multiple domains.'],\n",
       "    'content': 'LLMs are known for their high flexibility, which enables them to execute a variety of tasks such as answering questions, summarizing text, translating languages, and completing sentences accurately. They achieve this through their capacity to manage billions of parameters during processing.\\n\\nThe applicability of LLMs spans numerous fields such as copywriting, sentiment analysis, conversational AI, and even software development through code generation. Their capabilities extend to handling complex language-based tasks without the need for building separate models for each task, making them a cost-effective solution across various industries.\\n\\nExamples of popular LLMs include OpenAI’s GPT models and Google’s BERT, which have become significant in fields like customer service, content creation, and accessibility through language translation support.'},\n",
       "   {'title': 'Training and Fine-tuning of LLMs',\n",
       "    'learning_objectives': ['Describe how LLMs are trained and fine-tuned.',\n",
       "     'Understand the significance of self-supervised learning and word embeddings in LLMs.'],\n",
       "    'content': \"Training LLMs involves feeding them with extensive and diverse text corpora allowing them to learn through self-supervised techniques. The process involves predicting the next word in the sequence based on the analysis of preceding words, enhanced by an attention mechanism.\\n\\nKey to LLMs' functioning is the use of word embeddings, allowing the models to represent words as multi-dimensional vectors, thereby understanding context and relationships better by positioning similar words close together in vector space.\\n\\nLLMs can be fine-tuned for specific tasks utilizing additional data. This allows adjustment of model parameters to suit particular applications or enhance certain performance aspects, such as code generation or text classification.\"},\n",
       "   {'title': 'Challenges and Future of LLMs',\n",
       "    'learning_objectives': ['Discuss the challenges and limitations of LLMs.',\n",
       "     'Evaluate future trends and developments in the LLM field.'],\n",
       "    'content': 'Despite their powerful capabilities, LLMs present several challenges. Training them is resource-intensive and costly, involving substantial computational power and significant energy usage, leading to environmental concerns due to large carbon emissions.\\n\\nLLMs also face inherent biases and limitations such as potential for generating incorrect information or hallucinations, as well as maintaining ethical considerations in their applications.\\n\\nLooking forward, LLMs are transitioning into multimodal models capable of processing other types of data like images or audio, which expands their role significantly in AI development. Platforms like AWS provide services such as Amazon Bedrock and SageMaker JumpStart, facilitating the building and deployment of these advanced models.'}]},\n",
       " {'lessons': [{'title': 'Introduction to LLM Mechanics',\n",
       "    'learning_objectives': ['Understand the fundamental definition and scope of Large Language Models.',\n",
       "     'Identify the role of Machine Learning and Deep Learning in the context of LLMs.'],\n",
       "    'content': \"Large Language Models (LLMs) are a type of Artificial Intelligence (AI) specialized in understanding and generating human language. They are integral to a broader AI ecosystem which also encompasses Machine Learning (ML) and Deep Learning. In the landscape of ML, discovering patterns in data and performing classification tasks, like predicting music genres from song attributes, is common. However, more complex models, such as neural networks, are required for handling tasks with non-linear relationships. Here, neural networks play a vital role in ML as they consist of multiple layers of 'neurons' and can process unstructured data like text and images. Deep Learning, utilizing these neural networks, becomes crucial to processing such data effectively. LLMs are an extension of this concept, acting on a vast scale with neural networks that often exceed one billion parameters, facilitating tasks like language modeling by predicting the next word in a sequence.\"},\n",
       "   {'title': 'Deep Dive into LLM Architecture',\n",
       "    'learning_objectives': ['Explore the structure and function of the transformer model in LLMs.',\n",
       "     'Understand word embeddings and the role of the self-attention mechanism.'],\n",
       "    'content': \"LLMs, exemplified by models like ChatGPT, rely heavily on a specific architecture called the Transformer. This architecture comprises an encoder and a decoder with self-attention capabilities, allowing the model to focus on the most pertinent parts of the input sequence during processing. Transformers elevate efficiency by processing entire input sequences in parallel, thus leveraging GPUs effectively to reduce training duration. A conceptual core of LLMs lies in word embeddings, which replace simple numerical representations with mapped relationships in a vector space. This approach enables transformers to comprehend word context and relationships efficiently. Utilizing self-attention, the model discerns the importance of different elements within the input. By applying this understanding during the decoding process, LLMs generate informed outputs based on the input data, marking a milestone in AI's ability to understand and produce coherent language.\"},\n",
       "   {'title': 'Training and Generative Capabilities of LLMs',\n",
       "    'learning_objectives': ['Describe the training process and phases involved in LLM development.',\n",
       "     'Identify the generative capabilities and limitations of LLMs.'],\n",
       "    'content': 'The training process for LLMs involves vast text datasets from multiple sources, employing a technique known as self-supervised learning. This approach teaches the model to predict subsequent words in a sequence, gradually refining model parameters to improve accuracy. The training is executed in stages: Pre-Training allows the model to learn language structure and content-dependent knowledge. Instruction Fine-Tuning aligns the model with human guidance, supplemented optionally by Reinforcement Learning from Human Feedback (RLHF) to enhance alignment with human values.\\n\\nLLMs are generative by nature, producing coherent text by predicting one word at a time. This ability marks them as notable examples of Generative AI. However, they are not without flaws; challenges like hallucination—where the model generates inaccurate information—remain due to their reliance on pattern prediction without explicit fact grounding.'},\n",
       "   {'title': 'Applications and Future of LLMs',\n",
       "    'learning_objectives': ['Explore current applications and emerging abilities in LLMs.',\n",
       "     'Understand the potential future advancements and challenges.'],\n",
       "    'content': 'LLMs serve diverse applications, from summarizing content and answering questions to language translation and text generation. Their ability to adapt to untrained tasks in a zero-shot manner reflects emergent capabilities as models scale. These models benefit from examples and sequential reasoning, akin to working memory, demonstrating adaptability.\\n\\nLooking ahead, debates continue over whether LLMs develop a compressed world understanding or merely mimic observed patterns. Despite this debate, they hold significant potential, especially in automating repetitive tasks traditionally human-performed and extending into broader application fields. Nonetheless, challenges like hallucination, privacy concerns, and data bias persist, urging careful consideration in deploying LLMs responsibly.'}]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesson_plan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
